<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Keefe Murphy">
<meta name="author" content="Sonsoles López-Pernas">
<meta name="author" content="Mohammed Saqr">
<meta name="keywords" content="agglomerative hierarchical clustering, average silhouette width, dissimilarity-based clustering, K-Means, K-Medoids, leaning analytics">

<title>Learning analytics methods and tutorials - 8&nbsp; Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/ch09-model-based-clustering/ch9-model.html" rel="next">
<link href="../../chapters/ch07-prediction/ch7-pred.html" rel="prev">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y4VBV3J9WD"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y4VBV3J9WD', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Learning analytics methods and tutorials - 8&nbsp; Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R">
<meta name="twitter:description" content="Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data.">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="[8]{.chapter-number}&nbsp; [Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R]{.chapter-title}">
<meta name="citation_abstract" content="Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data. Interest lies in partitioning heterogeneous data into homogeneous groups, whereby cases within a group are more similar to each other than cases assigned to other groups, without foreknowledge of the group labels. Clustering is also an important component of several exploratory methods, analytical techniques, and modelling approaches and therefore has been practiced for decades in education research. In this context, finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students ---and their learning processes--- and tailor their supports to different needs. This chapter introduces the theory underpinning dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, $K$-Means, $K$-Medoids, and agglomerative hierarchical clustering. The $K$-Means clustering algorithm is described including the outline of the arguments of the relevant R functions and the main limitations and practical concerns to be aware of in order to obtain the best performance. We also discuss the related $K$-Medoids algorithm and its own associated concerns and function arguments. We later introduce agglomerative hierarchical clustering and the related R functions while outlining various choices available to practitioners and their implications. Methods for choosing the optimal number of clusters are provided, especially criteria that can guide the choice of clustering solution among multiple competing methodologies ---with a particular focus on evaluating solutions obtained using different dissimilarity measures--- and not only the choice of the number of clusters $K$ for a given method. All of these issues are demonstrated in detail with a tutorial in R using a real-life educational data set.">
<meta name="citation_keywords" content="agglomerative hierarchical clustering, average silhouette width, dissimilarity-based clustering, K-Means, K-Medoids, leaning analytics">
<meta name="citation_author" content="Keefe Murphy">
<meta name="citation_author" content="Sonsoles López-Pernas">
<meta name="citation_author" content="Mohammed Saqr">
<meta name="citation_fulltext_html_url" content="https://lamethods.github.io/ch8-clus.html">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=K-means&amp;amp;amp;lt;sup&amp;gt;++&amp;lt;/sup&amp;gt;: The advantages of careful seeding;,citation_author=David Arthur;,citation_author=Sergei Vassilvitskii;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=SODA ’07: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms;,citation_conference=Society for Industrial; Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Efficient agglomerative hierarchical clustering;,citation_author=Athman Bouguettaya;,citation_author=Qi Yu;,citation_author=Xumin Liu;,citation_author=Xiangmin Zhou;,citation_author=Andy Song;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=5;,citation_volume=42;,citation_journal_title=Expert Systems with Applications;">
<meta name="citation_reference" content="citation_title=Model-Based Clustering and Classification for Data Science: With Applications in R;,citation_author=Charles Bouveyron;,citation_author=Gilles Celeux;,citation_author=T. Brendan Murphy;,citation_author=Adrian E. Raftery;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=50;,citation_series_title=Cambridge series in statistical and probabilistic mathematics;">
<meta name="citation_reference" content="citation_title=cluster: cluster analysis basics and extensions;,citation_author=Martin Maechler;,citation_author=Peter Rousseeuw;,citation_author=Anja Struyf;,citation_author=Mia Hubert;,citation_author=Kurt Hornik;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=\url{https://CRAN.R-project.org/package=cluster};">
<meta name="citation_reference" content="citation_title=Kernel K-Means: Spectral clustering and normalized cuts;,citation_author=I. S. Dhillon;,citation_author=Y. Guan;,citation_author=B. Kulis;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_inbook_title=KDD ’04: Proceedings of the Tenth ACM SIGKDD International Conference of Knowledge Discovery and Data Mining, Seattle, WA, U.S.A.;">
<meta name="citation_reference" content="citation_title=Measures of the amount of ecologic association between species;,citation_author=L. R Dice;,citation_publication_date=1945;,citation_cover_date=1945;,citation_year=1945;,citation_issue=3;,citation_volume=26;,citation_journal_title=Ecology;">
<meta name="citation_reference" content="citation_title=Application of DBSCAN clustering algorithm in evaluating students’ learning status;,citation_author=H. Du;,citation_author=S. Chen;,citation_author=H. Niu;,citation_author=Y. Li;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Proceedings of the 17th International Conference on Computational Intelligence and Security, November 19–22, 2021;,citation_series_title=CIS 2021;">
<meta name="citation_reference" content="citation_title=Fuzzy clustering;,citation_author=P. D’Urso;,citation_editor=C. Hennig;,citation_editor=M. Meila;,citation_editor=F. Murtagh;,citation_editor=R. Rocci;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_inbook_title=Handbook of Cluster Analysis;,citation_series_title=Handbooks of modern statistical methods;">
<meta name="citation_reference" content="citation_title=A density-based algorithm for discovering clusters in large spatial databases with noise;,citation_author=Martin Ester;,citation_author=Hans-Peter Kriegel;,citation_author=Jörg Sander;,citation_author=Xiaowei Xu;,citation_editor=Evangelos Simoudis;,citation_editor=undefined Han;,citation_editor=Usama M. Fayyad;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=Proceedings of the Second International Conference on Knowledge Discovery and Data Mining;,citation_conference=AAAI Press;,citation_series_title=KDD’96;">
<meta name="citation_reference" content="citation_title=Cluster Analysis;,citation_author=B. S. Everitt;,citation_author=S. Landau;,citation_author=M. Leese;,citation_author=D. Stahl;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=848;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=fclust: An R package for fuzzy clustering;,citation_author=M. B. Ferraro;,citation_author=P. Giordani;,citation_author=A. Serafini;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=11;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=Cluster analysis of multivariate data: Efficiency vs interpretability of classifications;,citation_author=E. W. Forgy;,citation_publication_date=1965;,citation_cover_date=1965;,citation_year=1965;,citation_volume=21;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=Efficient hierarchical clustering of large high dimensional datasets;,citation_author=Sean Gilpin;,citation_author=Buyue Qian;,citation_author=Ian Davidson;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 22nd ACM International Conference on Information &amp;amp;amp; Knowledge Management;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’13;">
<meta name="citation_reference" content="citation_title=A general coefficient of similarity and some of its properties;,citation_author=J. C. Gower;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_issue=4;,citation_volume=27;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=dbscan: Fast density-based clustering with R;,citation_author=M. Hahsler;,citation_author=M. Piekenbrock;,citation_author=D. Doran;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=1;,citation_volume=91;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=Error detecting and error correcting codes;,citation_author=R. W. Hamming;,citation_publication_date=1950;,citation_cover_date=1950;,citation_year=1950;,citation_issue=2;,citation_volume=29;,citation_journal_title=The Bell System Technical Journal;">
<meta name="citation_reference" content="citation_title=A survey on feature selection approaches for clustering;,citation_author=Emrah Hancer;,citation_author=Bing Xue;,citation_author=Mengjie Zhang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=53;,citation_journal_title=Artificial Intelligence Review;">
<meta name="citation_reference" content="citation_title=Algorithm AS 136: a K-Means clustering algorithm;,citation_author=J. A. Hartigan;,citation_author=M. A. Wong;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_volume=28;,citation_journal_title=Journal of the Royal Statistical Society: Series C (Applied Statistics);">
<meta name="citation_reference" content="citation_title=What are the true clusters?;,citation_author=C. Hennig;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_volume=64;,citation_journal_title=Pattern Recognition Letters;">
<meta name="citation_reference" content="citation_title=Clustering strategy and method selection;,citation_author=C. Hennig;,citation_editor=C. Hennig;,citation_editor=M. Meila;,citation_editor=F. Murtagh;,citation_editor=R. Rocci;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_inbook_title=Handbook of Cluster Analysis;,citation_series_title=Handbooks of modern statistical methods;">
<meta name="citation_reference" content="citation_title=Clustering large data sets with mixed numeric and categorical values;,citation_author=Z. Huang;,citation_editor=H. Lu;,citation_editor=H. Motoda;,citation_editor=H. Luu;,citation_publication_date=1997;,citation_cover_date=1997;,citation_year=1997;,citation_inbook_title=KDD: Techniques and Applications;">
<meta name="citation_reference" content="citation_title=Extensions to the k-means algorithm for clustering large data sets with categorical values;,citation_author=Z. Huang;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=3;,citation_volume=2;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Comparing partitions;,citation_author=L. Hubert;,citation_author=P. Arabie;,citation_publication_date=1985;,citation_cover_date=1985;,citation_year=1985;,citation_issue=1;,citation_issn=1432-1343;,citation_volume=2;,citation_journal_title=Journal of Classification;">
<meta name="citation_reference" content="citation_title=Distribution de la flore alpine dans le bassin des Dranses et dans quelqus régions voisines;,citation_author=P. Jaccard;,citation_publication_date=1901;,citation_cover_date=1901;,citation_year=1901;,citation_volume=37;,citation_journal_title=Bulletin de la Société Vaudoise des Sciences Naturelles;">
<meta name="citation_reference" content="citation_title=Partitioning around medoids (program PAM);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Agglomerative nesting (program AGNES);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Divisive analysis (program DIANA);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Fuzzy analysis (program FANNY);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Least squares quantization in PCM;,citation_author=S. P. Lloyd;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;,citation_issue=2;,citation_volume=28;,citation_journal_title=IEEE Transactions on Information Theory;">
<meta name="citation_reference" content="citation_title=A broad collection of datasets for educational research training and application;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Javier Conde;,citation_author=Laura Del-Río-Carazo;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Some methods for classification and analysis of multivariate observations;,citation_author=J. B. MacQueen;,citation_editor=L. M. Le Cam;,citation_editor=J. Neyman;,citation_publication_date=1967;,citation_cover_date=1967;,citation_year=1967;,citation_volume=1;,citation_conference_title=Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability;,citation_conference=University of California Press;">
<meta name="citation_reference" content="citation_title=Review of the development of multidimensional scaling methods;,citation_author=A. Mead;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=1;,citation_volume=41;,citation_journal_title=Journal of the Royal Statistical Society: Series D (The Statistician);">
<meta name="citation_reference" content="citation_title=Gaussian parsimonious clustering models with covariates and a noise component;,citation_author=Keefe Murphy;,citation_author=Thomas Brendan Murphy;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=14;,citation_journal_title=Advances in Data Analysis and Classification;">
<meta name="citation_reference" content="citation_title=Ward’s hierarchical agglomerative clustering method: Which algorithms implement Ward’s criterion?;,citation_author=F. Murtagh;,citation_author=P. Legendre;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_volume=31;,citation_journal_title=Journal of Classification;">
<meta name="citation_reference" content="citation_title=On spectral clustering: Analysis and an algorithm;,citation_author=A. Y. Ng;,citation_author=M. I. Jordan;,citation_author=Y. Weiss;,citation_editor=T. Dietterich;,citation_editor=S. Becker;,citation_editor=Z. Ghahramani;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_volume=14;,citation_conference_title=Advances in Neural Information Processing Systems;,citation_conference=MIT Press;">
<meta name="citation_reference" content="citation_title=Displaying a clustering with CLUSPLOT;,citation_author=G. Pison;,citation_author=A. Struyf;,citation_author=P. J. Rousseeuq;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_volume=30;,citation_journal_title=Computational Statistics and Data Analysis;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Objective criteria for the evaluation of clustering methods;,citation_author=William M. Rand;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_issue=336;,citation_volume=66;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=rio: a Swiss-army knife for data file ;,citation_author=Chung-hong Chan;,citation_author=Thomas J. Leeper;,citation_author=Jason Becker;,citation_author=David Schoch;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=\url{https://cran.r-project.org/package=rio};">
<meta name="citation_reference" content="citation_title=Silhouettes: A graphical aid to the interpretation and validation of cluster analysis;,citation_author=P. J. Rousseeuw;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_volume=20;,citation_journal_title=Computational and Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Sequence analysis: Basic principles, technique, and tutorial;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Satu Helske;,citation_author=Marion Durand;,citation_author=Keefe Murphy;,citation_author=Matthias Studer;,citation_author=Gilbert Ritschard;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Social betwork analysis: A primer, a guide and a tutorial in R;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Miguel Conde;,citation_author=undefined Hernández-García;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Fast and eager K-Medoids clustering: \mathcal{O}(K) runtime improvement of the PAM, CLARA, and CLARANS algorithms;,citation_author=E. Schubert;,citation_author=P. J. Rousseeuw;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=101;,citation_journal_title=Information Systems;">
<meta name="citation_reference" content="citation_title=Estimating the dimension of a model;,citation_author=Gideon E Schwarz;,citation_publication_date=1978;,citation_cover_date=1978;,citation_year=1978;,citation_issue=2;,citation_volume=6;,citation_journal_title=The Annals of Statistics;,citation_publisher=The Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=An introduction and R tutorial to model-based clustering in education via latent profile analysis;,citation_author=Luca Scrucca;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Keefe Murphy;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models;,citation_author=Luca Scrucca;,citation_author=Michael Fop;,citation_author=T. Brendan Murphy;,citation_author=Adrian E. Raftery;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=8;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons;,citation_author=T. Sørensen;,citation_publication_date=1948;,citation_cover_date=1948;,citation_year=1948;,citation_issue=4;,citation_volume=5;,citation_journal_title=Kongelige Danske Videnskabernes Selskab;">
<meta name="citation_reference" content="citation_title=Welcome to the tidyverse;,citation_author=Hadley Wickham;,citation_author=Mara Averick;,citation_author=Jennifer Bryan;,citation_author=Winston Chang;,citation_author=Lucy D’Agostino McGowan;,citation_author=Romain François;,citation_author=Garrett Grolemund;,citation_author=Alex Hayes;,citation_author=Lionel Henry;,citation_author=Jim Hester;,citation_author=Max Kuhn;,citation_author=Thomas Lin Pedersen;,citation_author=Evan Miller;,citation_author=Stephan Milton Bache;,citation_author=Kirill Müller;,citation_author=Jeroen Ooms;,citation_author=David Robinson;,citation_author=Dana Paige Seidel;,citation_author=Vitalie Spinu;,citation_author=Kohske Takahashi;,citation_author=Davis Vaughan;,citation_author=Claus Wilke;,citation_author=Kara Woo;,citation_author=Hiroaki Yutani;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=43;,citation_volume=4;,citation_journal_title=Journal of Open Source Software;">
<meta name="citation_reference" content="citation_title=Hierarchical grouping to optimize an objective function;,citation_author=J. H. Ward, Jr.;,citation_publication_date=1963;,citation_cover_date=1963;,citation_year=1963;,citation_issue=301;,citation_volume=58;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=A framework for feature selection in clustering;,citation_author=Daniela M. Witten;,citation_author=Robert Tibshirani;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=490;,citation_volume=105;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=Motivational profiles of adult basic education students;,citation_author=Hal W. Beder;,citation_author=Thomas Valentine;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_issue=2;,citation_volume=40;,citation_journal_title=Adult Education Quarterly;">
<meta name="citation_reference" content="citation_title=Clustering of students admission data using K-means, hierarchical, and DBSCAN algorithms;,citation_author=Erwin Lanceta Cahapin;,citation_author=Beverly Ambagan Malabag;,citation_author=Jr Santiago;,citation_author=Jocelyn L. Reyes;,citation_author=Gemma S. Legaspi;,citation_author=Karl Louise Adrales;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=12;,citation_journal_title=Bulletin of Electrical Engineering and Informatics;">
<meta name="citation_reference" content="citation_title=Motivation, self‐confidence, and group cohesion in the foreign language classroom;,citation_author=Richard Clément;,citation_author=Zoltán Dörnyei;,citation_author=Kimberly A. Noels;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=3;,citation_volume=44;,citation_journal_title=Language Learning;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=Advancing the science and practice of precision education to enhance student outcomes;,citation_author=Clayton R. Cook;,citation_author=Stephen P. Kilgus;,citation_author=Matthew K. Burns;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=December 2017;,citation_volume=66;,citation_journal_title=Journal of School Psychology;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Clustering algorithms applied in educational data mining;,citation_author=Ashish Dutt;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=2;,citation_volume=5;,citation_journal_title=International Journal of Information and Electronics Engineering;">
<meta name="citation_reference" content="citation_title=Dissecting learning tactics in MOOC using ordered network analysis;,citation_author=Yizhou Fan;,citation_author=Yuanru Tan;,citation_author=Mladen Raković;,citation_author=Yeyu Wang;,citation_author=Zhiqiang Cai;,citation_author=David Williamson Shaffer;,citation_author=Dragan Gašević;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=1;,citation_volume=39;,citation_journal_title=Journal of Computer Assisted Learning;">
<meta name="citation_reference" content="citation_title=A cluster analysis on students’ perceived motivational climate. Implications on psycho-social variables;,citation_author=Javier Fernandez-Rio;,citation_author=Antonio Méndez-Giménez;,citation_author=Jose A. Cecchini Estrada;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_volume=17;,citation_journal_title=The Spanish Journal of Psychology;">
<meta name="citation_reference" content="citation_title=Informative tools for characterizing individual differences in learning: Latent class, latent profile, and latent transition analysis;,citation_author=Marian Hickendorff;,citation_author=Peter A. Edelsbrunner;,citation_author=Jake McMullen;,citation_author=Michael Schneider;,citation_author=Kelly Trezise;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=66;,citation_journal_title=Learning and Individual Differences;">
<meta name="citation_reference" content="citation_title=Variable-centered, person-centered, and person-specific approaches: Where theory meets the method;,citation_author=Matt C. Howard;,citation_author=Michael E. Hoffman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=21;,citation_journal_title=Organizational Research Methods;">
<meta name="citation_reference" content="citation_title=Learning analytics to unveil learning strategies in a flipped classroom;,citation_author=Jelena Jovanović;,citation_author=Dragan Gašević;,citation_author=Shane Dawson;,citation_author=Abelardo Pardo;,citation_author=Negin Mirriahi;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=33;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Does slow and steady win the race?: Clustering patterns of students’ behaviors in an interactive online mathematics game;,citation_author=Ji-Eun Lee;,citation_author=Jenny Yun-Chen Chan;,citation_author=Anthony Botelho;,citation_author=Erin Ottmar;,citation_publication_date=2022-10;,citation_cover_date=2022-10;,citation_year=2022;,citation_issue=5;,citation_volume=70;,citation_journal_title=Educational Technology Research and Development;">
<meta name="citation_reference" content="citation_title=Bringing synchrony and clarity to complex multi-channel data: A learning analytics study in programming education;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=9;,citation_journal_title=IEEE Access;">
<meta name="citation_reference" content="citation_title=A learning analytics perspective on educational escape rooms;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Aldo Gordillo;,citation_author=Enrique Barra;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=Interactive Learning Environments;">
<meta name="citation_reference" content="citation_title=Putting it all together: Combining learning analytics methods and data sources to understand students’ approaches to learning programming;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Olga Viberg;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=9;,citation_volume=13;,citation_journal_title=Sustainability;">
<meta name="citation_reference" content="citation_title=Clustering and sequential pattern mining of online collaborative learning data;,citation_author=Dilhan Perera;,citation_author=Judy Kay;,citation_author=Irena Koprinska;,citation_author=Kalina Yacef;,citation_author=Osmar R. Zaïane;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=6;,citation_volume=21;,citation_journal_title=IEEE Transactions on Knowledge and Data Engineering;">
<meta name="citation_reference" content="citation_title=Clusteranalysen bei psychologisch-pädagogischen Fragestellungen;,citation_author=B. Rennen-Allhoff;,citation_author=P. Allhoff;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=4;,citation_volume=30;,citation_journal_title=Psychologie in Erziehung und Unterricht;">
<meta name="citation_reference" content="citation_title=Incredible utility: The lost causes and causal debris of psychological science;,citation_author=John E. Richters;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=6;,citation_volume=43;,citation_journal_title=Basic and Applied Social Psychology;">
<meta name="citation_reference" content="citation_title=Using multimodal data to find patterns in student presentations;,citation_author=Felipe Vieira Roque;,citation_author=Cristian Cechinel;,citation_author=Erick Merino;,citation_author=Rodolfo Villarroel;,citation_author=Robson Lemos;,citation_author=Roberto Munoz;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=2018 XIII Latin American Conference on Learning Technologies (LACLO);">
<meta name="citation_reference" content="citation_title=The immersive virtual reality experience: A typology of users revealed through multiple correspondence analysis combined with cluster analysis technique;,citation_author=Pedro J. Rosa;,citation_author=Diogo Morais;,citation_author=Pedro Gamito;,citation_author=Jorge Oliveira;,citation_author=Tomaz Saraiva;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=3;,citation_volume=19;,citation_journal_title=Cyberpsychology, Behavior and Social Networking;">
<meta name="citation_reference" content="citation_title=The longitudinal trajectories of online engagement over a full program;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=175;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=Modelling diffusion in computer-supported collaborative learning: A large scale learning analytics study;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=4;,citation_volume=16;,citation_journal_title=International Journal of Computer-Supported Collaborative Learning;">
<meta name="citation_reference" content="citation_title=The temporal dynamics of online problem-based learning: Why and when sequence matters;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=1;,citation_volume=18;,citation_journal_title=International Journal of Computer-Supported Collaborative Learning;">
<meta name="citation_reference" content="citation_title=Intense, turbulent, or wallowing in the mire: A longitudinal study of cross-course online tactics, strategies, and trajectories;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Jelena Jovanović;,citation_author=Dragan Gašević;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=57;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Teachers’ learning profiles in learning programming: The big picture!;,citation_author=Mohammed Saqr;,citation_author=Ville Tuominen;,citation_author=Teemu Valtonen;,citation_author=Erkko Sointu;,citation_author=Sanna Väisänen;,citation_author=Laura Hirsto;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=7;,citation_journal_title=Frontiers in Education;">
<meta name="citation_reference" content="citation_title=What matters in AI-supported learning: A study of human-AI interactions in language learning using cluster analysis and epistemic network analysis;,citation_author=Xinghua Wang;,citation_author=Qian Liu;,citation_author=Hui Pang;,citation_author=Seng Chee Tan;,citation_author=Jun Lei;,citation_author=Matthew P Wallace;,citation_author=Linlin Li;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=194;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=How CSCL roles emerge, persist, transition, and evolve over time: A four-year longitudinal study;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issn=0360-1315;,citation_volume=189;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=Learner participation profiles in an asynchronous online collaboration context;,citation_author=Min Kyu Kim;,citation_author=Tuba Ketenci;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issn=1096-7516;,citation_volume=41;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Using diffusion network analytics to examine and support knowledge construction in CSCL settings;,citation_author=Mohammed Saqr;,citation_author=Olga Viberg;,citation_editor=C. Alario-Hoyos;,citation_editor=M. J. Rodríguez-Triana;,citation_editor=M. Scheffel;,citation_editor=I. Arnedillo-Sánchez;,citation_editor=S. M. Dennerlein;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=12315;,citation_conference_title=Addressing Global Challenges and Quality Education: Proceedings of the 15th European Conference on Technology Enhanced Learning, EC-TEL 2020, September 14–18, 2020;,citation_conference=Springer;,citation_series_title=Lecture notes in computer science;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Learning analytics methods and tutorials</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lamethods/labook-code/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contributors.html" class="sidebar-item-text sidebar-link">Contributors</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch01-intro/intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Getting started</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch02-data/ch2-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch03-intro-r/ch3-intor.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Intro to R</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch04-data-cleaning/ch4-datacleaning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data cleaning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch05-basic-stats/ch5-stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Basic statistics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch06-data-visualization/ch6-viz.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data visualization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Machine Learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch07-prediction/ch7-pred.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Predictive modeling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch08-clustering/ch8-clus.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch09-model-based-clustering/ch9-model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model-based clustering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Temporal methods</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch10-sequence-analysis/ch10-seq.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch11-vasstra/ch11-vasstra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">VaSSTra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch12-markov/ch12-markov.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Markov models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch13-multichannel/ch13-multi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Multi-channel sequences</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch14-process-mining/ch14-process.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Process mining</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Network analysis</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch15-sna/ch15-sna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Social Network Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch16-community/ch16-comm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Community detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch17-temporal-networks/ch17-tna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Temporal Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch18-ena-ona/ch18-ena.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Epistemic Network Analysis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Psychometrics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch19-psychological-networks/ch19-psych.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Psychological networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch20-factor-analysis/ch20-factor.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Factor analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch21-sem/ch21-sem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Structured Equation Modeling</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch22-conclusion/ch22-conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Conclusion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#sec-review" id="toc-sec-review" class="nav-link" data-scroll-target="#sec-review"><span class="toc-section-number">2</span>  Clustering in education: review of the literature</a></li>
  <li><a href="#sec-methods" id="toc-sec-methods" class="nav-link" data-scroll-target="#sec-methods"><span class="toc-section-number">3</span>  Clustering methodology</a>
  <ul class="collapse">
  <li><a href="#sec-kmtheory" id="toc-sec-kmtheory" class="nav-link" data-scroll-target="#sec-kmtheory"><span class="toc-section-number">3.1</span>  <span class="math inline">\(K\)</span>-Means</a></li>
  <li><a href="#sec-hclust" id="toc-sec-hclust" class="nav-link" data-scroll-target="#sec-hclust"><span class="toc-section-number">3.2</span>  Agglomerative hierarchical clustering</a></li>
  <li><a href="#sec-chooseK" id="toc-sec-chooseK" class="nav-link" data-scroll-target="#sec-chooseK"><span class="toc-section-number">3.3</span>  Choosing the number of clusters</a></li>
  </ul></li>
  <li><a href="#sec-tutR" id="toc-sec-tutR" class="nav-link" data-scroll-target="#sec-tutR"><span class="toc-section-number">4</span>  Tutorial with R</a>
  <ul class="collapse">
  <li><a href="#sec-data" id="toc-sec-data" class="nav-link" data-scroll-target="#sec-data"><span class="toc-section-number">4.1</span>  The data set</a></li>
  <li><a href="#sec-apps" id="toc-sec-apps" class="nav-link" data-scroll-target="#sec-apps"><span class="toc-section-number">4.2</span>  Clustering applications</a></li>
  </ul></li>
  <li><a href="#sec-disc" id="toc-sec-disc" class="nav-link" data-scroll-target="#sec-disc"><span class="toc-section-number">5</span>  Discussion &amp; further readings</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<p><small>© 2023 The authors</small></p>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Keefe Murphy </p>
             <p>Sonsoles López-Pernas </p>
             <p>Mohammed Saqr </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data. Interest lies in partitioning heterogeneous data into homogeneous groups, whereby cases within a group are more similar to each other than cases assigned to other groups, without foreknowledge of the group labels. Clustering is also an important component of several exploratory methods, analytical techniques, and modelling approaches and therefore has been practiced for decades in education research. In this context, finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students —and their learning processes— and tailor their supports to different needs. This chapter introduces the theory underpinning dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-Medoids, and agglomerative hierarchical clustering. The <span class="math inline">\(K\)</span>-Means clustering algorithm is described including the outline of the arguments of the relevant R functions and the main limitations and practical concerns to be aware of in order to obtain the best performance. We also discuss the related <span class="math inline">\(K\)</span>-Medoids algorithm and its own associated concerns and function arguments. We later introduce agglomerative hierarchical clustering and the related R functions while outlining various choices available to practitioners and their implications. Methods for choosing the optimal number of clusters are provided, especially criteria that can guide the choice of clustering solution among multiple competing methodologies —with a particular focus on evaluating solutions obtained using different dissimilarity measures— and not only the choice of the number of clusters <span class="math inline">\(K\)</span> for a given method. All of these issues are demonstrated in detail with a tutorial in R using a real-life educational data set.
  </div>
</div>

</header>

<section id="sec-intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-intro"><span class="header-section-number">1</span> Introduction</h2>
<p>Cluster analysis is a term used to describe a broad range of techniques which have the goal of uncovering groups of observations in a data set. The typical aim of cluster analysis is to categorise observations into groups in such a way that observations within the same group are in some sense more similar to each other, while being relatively dissimilar to those in other groups <span class="citation" data-cites="Everitt2011">[<a href="#ref-Everitt2011" role="doc-biblioref">1</a>]</span>. In other words, clustering methods uncover group structure in heterogeneous populations by identifying more homogeneous groupings of observations which may represent distinct, meaningful subpopulations. Using machine learning terminology, cluster analysis corresponds to unsupervised learning, whereby groups are identified by relying solely on the intrinsic characteristics (typically dissimilarities), without guidance from unavailable ground truth group labels. Indeed, foreknowledge of the fixed number of groups in a data set is characteristic of supervised learning and the distinct field of classification analysis.</p>
<p>It is important to note that there is no universally applicable definition of what constitutes a cluster <span class="citation" data-cites="Hennig2015 Hennig2016">[<a href="#ref-Hennig2015" role="doc-biblioref">2</a>, <a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>. Indeed, in the absence of external information in the form of existing “true” group labels, different clustering methods can reveal different things about the same data. There are many different ways to cluster the same data set, and different methods may yield solutions with different assignments, or even differ in the number of groups they identify. Consequently, we present several cluster analysis algorithms in this chapter; namely, <span class="math inline">\(K\)</span>-Means <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span> in <a href="#sec-kmtheory"><span class="quarto-unresolved-ref">sec-kmtheory</span></a>, a generalisation thereof, <span class="math inline">\(K\)</span>-Medoids <span class="citation" data-cites="Kaufman1990-pam">[<a href="#ref-Kaufman1990-pam" role="doc-biblioref">5</a>]</span>, in <a href="#sec-kmedoids"><span class="quarto-unresolved-ref">sec-kmedoids</span></a>, and agglomerative hierarchical clustering <span class="citation" data-cites="Kaufman1990-agnes">[<a href="#ref-Kaufman1990-agnes" role="doc-biblioref">6</a>]</span> in <a href="#sec-hclust"><span class="quarto-unresolved-ref">sec-hclust</span></a>. We apply each method in a comparative study in our tutorial using R <span class="citation" data-cites="R2023">[<a href="#ref-R2023" role="doc-biblioref">7</a>]</span>, with applications to a data set about the participants from discussion forum of a massive open online course (MOOC) for teachers.</p>
<p>The clustering methods we review in this chapter are designed to find mutually exclusive, non-overlapping groups in a data set, i.e., they recover a hard partition whereby each observation belongs to one group only. This is in contrast to soft clustering approaches under which each observation is assigned a probability of belonging to each group. One such example of soft clustering is the model-based clustering paradigm, which is discussed in the later Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>. While model-based clustering methods, as the name suggests, are underpinned by the assumption of generative probabilistic models <span class="citation" data-cites="Bouveyron2019">[<a href="#ref-Bouveyron2019" role="doc-biblioref">9</a>]</span>, the more traditional dissimilarity-based methods on which this chapter is focused are purely algorithmic in nature and rely on heuristic criteria regarding the pairwise dissimilarities between objects.</p>
<p>Heuristic clustering algorithms can be further divided into two categories; partitional clustering (e.g., <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids) and hierarchical (of which we focus on the agglomerative variant). Broadly speaking, partitional clustering methods start with an initial grouping of observations and iteratively update the clustering until the “best” clustering is found, according to some notion of what defines the “best” clustering. Hierarchical clustering methods, on the other hand, is more sequential in nature; a tree-like structure of nested clusterings is built up via successive mergings of similar observations, according to a defined similarity metric. In our theoretical expositions and our applications in the R tutorial, we provide some guidelines both on how to choose certain method-specific settings to yield the best performance and how to determine the optimal clustering among a set of competing methods.</p>
<p>The remainder of this chapter proceeds as follows. In <a href="#sec-review"><span class="quarto-unresolved-ref">sec-review</span></a>, we review relevant literature in which dissimilarity-based clustering methods have been applied in the realm of educational research. In <a href="#sec-methods"><span class="quarto-unresolved-ref">sec-methods</span></a>, we describe the theoretical underpinnings of each method in turn and discuss relevant practical guidelines which should be followed to secure satisfactory performance from each method throughout. Within <a href="#sec-tutR"><span class="quarto-unresolved-ref">sec-tutR</span></a>, we introduce the data set of our case study in <a href="#sec-data"><span class="quarto-unresolved-ref">sec-data</span></a> and give an overview of some required pre-processing steps in <a href="#sec-process"><span class="quarto-unresolved-ref">sec-process</span></a>, and then present a tutorial using R for each clustering method presented in this chapter in <a href="#sec-apps"><span class="quarto-unresolved-ref">sec-apps</span></a>, with a specific focus on identifying the optimal clustering solution in <a href="#sec-sil"><span class="quarto-unresolved-ref">sec-sil</span></a>. Finally, we conclude with a discussion and some recommendations for related further readings in <a href="#sec-disc"><span class="quarto-unresolved-ref">sec-disc</span></a>, with a particular focus on some limitations of dissimilarity-based clustering which are addressed by other frameworks in the broader field of cluster analysis.</p>
</section>
<section id="sec-review" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-review"><span class="header-section-number">2</span> Clustering in education: review of the literature</h2>
<p>In education, clustering is among the oldest and most common analysis methods, predating the field of learning analytics and educational data mining by several decades. Such early adoption of clustering is due to the immense utility of cluster analysis in helping researchers to find patterns within data, which is a major pursuit of education research <span class="citation" data-cites="Rennenallhoff1983">[<a href="#ref-Rennenallhoff1983" role="doc-biblioref">10</a>]</span>. Interest was fueled by the increasing attention to heterogeneity and individual differences among students, their learning processes, and their approaches to learning <span class="citation" data-cites="Hickendorff2018 Saqr2021a">[<a href="#ref-Hickendorff2018" role="doc-biblioref">11</a>, <a href="#ref-Saqr2021a" role="doc-biblioref">12</a>]</span>. Finding such patterns or differences among students allows teachers and researchers to improve their understanding of the diversity of students and tailor their support to different needs <span class="citation" data-cites="Cook2018">[<a href="#ref-Cook2018" role="doc-biblioref">13</a>]</span>. Finding subgroups within cohorts of students is a hallmark of so-called “person-centered methods”, to which clustering belongs <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>.</p>
<p>A person-centered approach stands in contrast to the variable-centered methods which consider that most students belong to a coherent homogeneous group with little or negligible differences <span class="citation" data-cites="Howard2018">[<a href="#ref-Howard2018" role="doc-biblioref">14</a>]</span>. Variable-centered methods assume that there is a common average pattern that represents all students, that the studied phenomenon has a common causal mechanism, and that the phenomenon evolves in the same way and results in similar outcomes amongst all students. These assumptions are largely understood to be unrealistic, “demonstrably false, and invalidated by a substantial body of uncontested scientific evidence” <span class="citation" data-cites="Richters2021">[<a href="#ref-Richters2021" role="doc-biblioref">15</a>]</span>. In fact, several analytical problems necessitate clustering, e.g., where opposing patterns exist <span class="citation" data-cites="Saqr2023a">[<a href="#ref-Saqr2023a" role="doc-biblioref">16</a>]</span>. For instance, in examining attitudes towards an educational intervention using variable-centered methods, we get an average that is simply the sum of negative and positive attitudes. If the majority of students have a positive attitude towards the proposed intervention —combined with a minority against— the final result will imply that students favour such intervention. The conclusions of this approach are not only wrong but also dangerous as it risks generalising solutions to groups to whom it may cause harm.</p>
<p>Therefore, clustering has become an essential method in all subfields of education (e.g., education psychology, education technology, and learning analytics) having operationalised almost all quantitative data types and been integrated with most of the existing methods <span class="citation" data-cites="Rennenallhoff1983 Dutt2015">[<a href="#ref-Rennenallhoff1983" role="doc-biblioref">10</a>, <a href="#ref-Dutt2015" role="doc-biblioref">17</a>]</span>. For instance, clustering became an essential step in sequence analysis to discover subsequences of data that can be understood as distinct approaches or strategies of students’ behavior <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>. Similar applications can be found in social network analysis data to identify collaborative roles, or in multimodal data analysis to identify moments of interest (e.g., synchrony). Similarly, clustering has been used with survey data to find attitudinal patterns or learning approaches to mention a few <span class="citation" data-cites="Dutt2015">[<a href="#ref-Dutt2015" role="doc-biblioref">17</a>]</span>. Furthermore, identifying patterns within students’ data is a prime educational interest in its own right and therefore, has been used extensively as a standalone analysis technique to identify subgroups of students who share similar interests, attitudes, behaviors, or background.</p>
<p>Clustering has been used in education psychology for decades to find patterns within self-reported questionnaire data. Early examples include the work of Beder and Valentine <span class="citation" data-cites="Beder1990">[<a href="#ref-Beder1990" role="doc-biblioref">19</a>]</span> who used responses of a motivational questionnaire to discover subgroups of students with different motivational attitudes. Similarly, Clément et al. <span class="citation" data-cites="Clement1994">[<a href="#ref-Clement1994" role="doc-biblioref">20</a>]</span> used the responses of a questionnaire that assessed anxiety and motivation towards learning English as a second language to find clusters which differentiated students according to their attitudes and motivation. Other examples include the work by Fernandez-Rio et al. <span class="citation" data-cites="Fernandez-Rio2014">[<a href="#ref-Fernandez-Rio2014" role="doc-biblioref">21</a>]</span> who used hierarchical clustering and <span class="math inline">\(K\)</span>-Means to identify distinct student profiles according to their perception of the class climate. With the digitalisation of educational institutions, authors also sought to identify students profiles using admission data and learning records. For instance, Cahapin et al. <span class="citation" data-cites="Cahapin2023">[<a href="#ref-Cahapin2023" role="doc-biblioref">22</a>]</span> used several algorithms such as <span class="math inline">\(K\)</span>-Means and agglomerative hierarchical clustering to identify patterns in students’ admission data.</p>
<p>The rise of online education opened many opportunities to investigate students’ behavior in learning management systems based on the trace log data that students leave behind them when working on online activities. An example is the work by Saqr et al. <span class="citation" data-cites="Saqr2022a">[<a href="#ref-Saqr2022a" role="doc-biblioref">23</a>]</span>, who used <span class="math inline">\(K\)</span>-Means to cluster in-service teachers’ approaches to learning in a MOOC according to their frequency of clicks on the available learning activities. Recently, research has placed a focus on the temporality of students’ activities, rather than the mere count. For this purpose, clustering has been integrated within sequence analysis to find subsequences which represent meaningful patterns of students behavior. For instance, Jovanovic et al. <span class="citation" data-cites="Jovanovic2017">[<a href="#ref-Jovanovic2017" role="doc-biblioref">24</a>]</span> used hierarchical clustering to identify distinct learning sequential patterns based on students’ online activities in a learning management system within a flipped classroom, and found a significant association between the use of certain learning sequences and learning outcomes. Using a similar approach, López-Pernas et al. <span class="citation" data-cites="Lopez-Pernas2021a">[<a href="#ref-Lopez-Pernas2021a" role="doc-biblioref">25</a>]</span> used hierarchical clustering to identify distinctive learning sequences in students’ use of the learning management system and an automated assessment tool for programming assignments. They also found an association between students’ activity patterns and their performance in the course final exam. Several other examples exist for using clustering to find patterns within sequence data <span class="citation" data-cites="Lopez-Pernas2021b Fan2022 Saqr2023a">[<a href="#ref-Saqr2023a" role="doc-biblioref">16</a>, <a href="#ref-Lopez-Pernas2021b" role="doc-biblioref">26</a>, <a href="#ref-Fan2022" role="doc-biblioref">27</a>]</span>.</p>
<p>A growing application of clustering can be seen in the study of computer-supported collaborative learning. Saqr and López-Pernas <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span> used cluster analysis to discover students with similar emergent roles based on their forum interaction patterns. Using the <span class="math inline">\(K\)</span>-Means algorithm and students’ centrality measures in the collaboration network, they identified three roles: influencers, mediators, and isolates. Perera et al. <span class="citation" data-cites="Perera2009">[<a href="#ref-Perera2009" role="doc-biblioref">29</a>]</span> used <span class="math inline">\(K\)</span>-Means to find distinct groups of similar teams and similar individual participating students according to their contributions in an online learning environment for software engineering education. They found that several clusters which shared some distinct contribution patterns were associated with more positive outcomes. Saqr and López-Pernas <span class="citation" data-cites="Saqr2023b">[<a href="#ref-Saqr2023b" role="doc-biblioref">30</a>]</span> analysed the temporal unfolding of students’ contributions to group discussions. They used hierarchical clustering to identify patterns of distinct students’ sequences of interactions that have a similar start and found a relationship between such patterns and student achievement.</p>
<p>An interesting application of clustering in education concerns the use of multimodal data. For instance, Vieira et al. <span class="citation" data-cites="Roque2018">[<a href="#ref-Roque2018" role="doc-biblioref">31</a>]</span> used <span class="math inline">\(K\)</span>-Means clustering to find patterns in students’ presentation styles according to their voice, position, and posture data. Other innovative uses involve students’ use of educational games <span class="citation" data-cites="Lee2022 Lopez-Pernas2022">[<a href="#ref-Lee2022" role="doc-biblioref">32</a>, <a href="#ref-Lopez-Pernas2022" role="doc-biblioref">33</a>]</span>, virtual reality <span class="citation" data-cites="Rosa2016">[<a href="#ref-Rosa2016" role="doc-biblioref">34</a>]</span>, or artificial intelligence <span class="citation" data-cites="Wang2023">[<a href="#ref-Wang2023" role="doc-biblioref">35</a>]</span>. Though this chapter illustrates traditional dissimilarity-based clustering algorithms with applications using R to data on the centrality measures of the participants of a MOOC, along with related demographic characteristics, readers are also encouraged to read Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span> and Chapter 10 <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span> in which further tutorials are presented and additional literature is reviewed, specifically in the contexts of clustering using the model-based clustering paradigm and clustering longitudinal sequence data, respectively. We now turn to an explication of the cluster analysis methodologies used in this chapter’s tutorial.</p>
</section>
<section id="sec-methods" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-methods"><span class="header-section-number">3</span> Clustering methodology</h2>
<p>In this section, we describe some of the theory underpinning the clustering methods used in the later R tutorial in <a href="#sec-tutR"><span class="quarto-unresolved-ref">sec-tutR</span></a>. We focus on some of the most widely-known heuristic dissimilarity-based clustering algorithms; namely, <span class="math inline">\(K\)</span>-means, <span class="math inline">\(K\)</span>-medoids, and agglomerative hierarchical clustering. In <a href="#sec-kmtheory"><span class="quarto-unresolved-ref">sec-kmtheory</span></a>, we introduce <span class="math inline">\(K\)</span>-Means clustering by describing the algorithm, outline the arguments to the relevant R function <code>kmeans()</code>, and discuss some of the main limitations and practical concerns researchers should be aware of in order to obtain the best performance when running <span class="math inline">\(K\)</span>-Means. We also discuss the related <span class="math inline">\(K\)</span>-Medoids algorithm and the associated function <code>pam()</code> in the <code>cluster</code> library <span class="citation" data-cites="cluster2022">[<a href="#ref-cluster2022" role="doc-biblioref">36</a>]</span> in R, and situate this method in the context of an extension to <span class="math inline">\(K\)</span>-Means designed to overcome its reliance on squared Euclidean distances. In <a href="#sec-hclust"><span class="quarto-unresolved-ref">sec-hclust</span></a>, we introduce agglomerative hierarchical clustering and the related R function <code>hclust()</code>, while outlining various choices available to practitioners and their implications. Though method-specific strategies of choosing the optimal number of clusters <span class="math inline">\(K\)</span> are provided throughout <a href="#sec-kmtheory"><span class="quarto-unresolved-ref">sec-kmtheory</span></a> and <a href="#sec-hclust"><span class="quarto-unresolved-ref">sec-hclust</span></a>, we offer a more detailed treatment of this issue in <a href="#sec-chooseK"><span class="quarto-unresolved-ref">sec-chooseK</span></a>, particularly with regard to criteria that can guide the choice of clustering solution among multiple competing methodologies, and not only the choice of the number of clusters <span class="math inline">\(K\)</span> for a given method.</p>
<section id="sec-kmtheory" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="sec-kmtheory"><span class="header-section-number">3.1</span> <span class="math inline">\(K\)</span>-Means</h3>
<p><span class="math inline">\(K\)</span>-means is a widely-used clustering algorithm in learning analytics and indeed data analysis and machine learning more broadly. It is an unsupervised technique that seeks to divide a typically multivariate data set into some pre-specified number of clusters, based on the similarities between observations. More specifically, <span class="math inline">\(K\)</span>-means aims to partition <span class="math inline">\(n\)</span> objects <span class="math inline">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span>, each having measurements on <span class="math inline">\(j=1,\ldots,d\)</span> strictly continuous covariates, into a set of <span class="math inline">\(K\)</span> groups <span class="math inline">\(\mathcal{C}=\{C_1,\ldots,C_K\}\)</span>, where <span class="math inline">\(C_k\)</span> is the set of <span class="math inline">\(n_k\)</span> objects in cluster <span class="math inline">\(k\)</span> and the number of groups <span class="math inline">\(K \le n\)</span> is pre-specified by the practitioner and remains fixed. <span class="math inline">\(K\)</span>-means constructs these partitions in such a way that the squared Euclidean distance between the row vector for observations in a given cluster and the centroid (i.e., mean vector) of the given cluster are smaller than the distances to the centroids of the remaining clusters. In other words, <span class="math inline">\(K\)</span>-means aims to learn both the cluster centroids and the cluster assignments by minimising the within-cluster sums-of-squares (i.e., variances). Equivalently, this amounts to maximising the between-cluster sums-of-squares, thereby capturing heterogeneity in a data set by partitioning the observations into homogeneous groups. What follows is a brief technical description of the <span class="math inline">\(K\)</span>-Means algorithm in <a href="#sec-kmalgo"><span class="quarto-unresolved-ref">sec-kmalgo</span></a>, after which we describe some limitations of <span class="math inline">\(K\)</span>-Means and discuss practical concerns to be aware of in order to optimise the performance of the method in <a href="#sec-limitations"><span class="quarto-unresolved-ref">sec-limitations</span></a>. In particular, we present the widely-used <span class="math inline">\(K\)</span>-Medoids extension in <a href="#sec-kmedoids"><span class="quarto-unresolved-ref">sec-kmedoids</span></a>.</p>
<section id="sec-kmalgo" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="sec-kmalgo"><span class="header-section-number">3.1.1</span> <span class="math inline">\(K\)</span>-Means algorithm</h4>
<p>The origins of <span class="math inline">\(K\)</span>-Means are not so straightforward to summarise. The name was initially used by James MacQueen in 1967 <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span>. However, the standard algorithm was first proposed by Stuart Lloyd in a Bell Labs technical report in 1957, which was later published as a journal article in 1982 <span class="citation" data-cites="Lloyd1982">[<a href="#ref-Lloyd1982" role="doc-biblioref">37</a>]</span>. In order to understand the ideas involved, we must first define some relevant notation. Let <span class="math inline">\(\mu_k^{(j)}\)</span> denote the centroid value for the <span class="math inline">\(j\)</span>-th variable in cluster <span class="math inline">\(C_k\)</span> by <span class="math display">\[\mu_k^{(j)} = \frac{1}{n_k}\sum_{\mathbf{x}_i \in C_k} x_{ij}\]</span> and the complete centroid vector for cluster <span class="math inline">\(C_k\)</span> by <span class="math display">\[\boldsymbol{\mu}_k = \left(\mu_k^{(1)}, \ldots, \mu_k^{(p)}\right)^\top.\]</span> These centroids therefore correspond to the arithmetic mean vector of the observations in cluster <span class="math inline">\(C_k\)</span>. Finding both these centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> and the clustering partition <span class="math inline">\(\mathcal{C}\)</span> is computationally challenging and typically proceeds by iteratively alternating between allocating observations to clusters and then updating the centroid vectors. Formally, the objective is to minimise the total within-cluster sum-of-squares <span class="math display">\[\begin{equation}
\sum_{i=1}^n\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2,
\label{eq:kmeans_objective}
\end{equation}\]</span> where <span class="math inline">\(\lVert\mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2=\sum_{j=1}^p\left(x_{ij}-\mu_k^{(j)}\right)^2\)</span> denotes the squared Euclidean distance to the centroid <span class="math inline">\(\boldsymbol{\mu}_k\)</span> — such that <span class="math inline">\(\lVert\cdot\rVert_2\)</span> denotes the <span class="math inline">\(\ell^2\)</span> norm— and <span class="math inline">\(\mathbf{z}_i=(z_{i1},\ldots,z_{iK})^\top\)</span> is a latent variable such that <span class="math inline">\(z_{ik}\)</span> denotes the cluster membership of observation <span class="math inline">\(i\)</span>; <span class="math inline">\(z_{ik}=1\)</span> if observation <span class="math inline">\(i\)</span> belongs to cluster <span class="math inline">\(C_k\)</span> and <span class="math inline">\(z_{ik}=0\)</span> otherwise. This latent variable construction implies <span class="math inline">\(\sum_{i=1}^n z_{ik}=n_k\)</span> and <span class="math inline">\(\sum_{k=1}^Kz_{ik}=1\)</span>, such that each observation belongs wholly to one cluster only. As the total variation in the data, which remains fixed, can be written as a sum of the total within-cluster sum-of-squares (TWCSS) from Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span> and the between-cluster sum-of-squares as follows <span class="math display">\[\sum_{i=1}^n \left(\mathbf{x}_i - \bar{\mathbf{x}}\right)^2=\sum_{i=1}^n\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2 + \sum_{k=1}^Kn_k\lVert\boldsymbol{\mu}_k - \bar{\mathbf{x}}\rVert_2^2,\]</span> where <span class="math inline">\(\bar{\mathbf{x}}\)</span> denotes the overall sample mean vector, minimising the TWCSS endeavours to ensure that observations in the same cluster are maximally similar to observations in the same cluster and maximally dissimilar to those in other clusters.</p>
<p>Using the notation just introduced, a generic <span class="math inline">\(K\)</span>-means algorithm would proceed as follows:</p>
<ol type="1">
<li><p><em>Initialise</em>: Select the number of desired clusters <span class="math inline">\(K\)</span> and define <span class="math inline">\(K\)</span> initial centroid vectors <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span>.</p></li>
<li><p><em>Allocate</em>: Find the optimal <span class="math inline">\(z_{ik}\)</span> values that minimise the objective, holding the <span class="math inline">\(\mu_k\)</span> values fixed.</p></li>
</ol>
<p>Calculate the squared Euclidean distance between each observation and each centroid vector and allocate each object to the cluster corresponding to the initial centroid to which it is closest in terms of squared Euclidean distance. Looking at the objective function in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span> closely and examining the contribution of observation <span class="math inline">\(i\)</span>, we need to choose the value of <span class="math inline">\(\mathbf{z}_i\)</span> which minimises the expression <span class="math inline">\(\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2\)</span>. This is achieved by setting <span class="math inline">\(z_{ik}=1\)</span> for the value of <span class="math inline">\(k\)</span> that has smallest <span class="math inline">\(\lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2\)</span> and setting <span class="math inline">\(z_{ik^\prime}=0\:\forall\:k^\prime\ne k\)</span> everywhere else.</p>
<ol start="3" type="1">
<li><em>Update</em>: Find the optimal <span class="math inline">\(\boldsymbol{\mu}_k\)</span> values that minimise the objective, holding the <span class="math inline">\(z_{ik}\)</span> values fixed.</li>
</ol>
<p>If we re-write the objective function in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span> as <span class="math display">\[\sum_{i=1}^n\sum_{k=1}^K\sum_{j=1}^p z_{ik} \left(x_{ij}-\mu_k^{(j)}\right)^2,\]</span> we can use the fact that <span class="math display">\[\frac{\partial}{\partial \mu_k^{(j)}}\left(x_{ij}-\mu_k^{(j)}\right)^2=-2\left(x_{ij}-\mu_k^{(j)}\right)\]</span> to obtain <span class="math display">\[\frac{\partial}{\partial \mu_k^{(j)}}\sum_{i=1}^n\sum_{k=1}^K\sum_{j=1}^p z_{ik} \left(x_{ij}-\mu_k^{(j)}\right)^2=-2\sum_{i=1}^nz_{ik}\left(x_{ij}-\mu_k^{(j)}\right).\]</span> Solving this expression for <span class="math inline">\(\mu_k^{(j)}\)</span> yields <span class="math display">\[\mu_k^{(j)}=\frac{\sum_{i=1}^nz_{ik}x_{ij}}{\sum_{i=1}^nz_{ik}}=\frac{1}{n_k}\sum_{\mathbf{x}_i \in C_k} x_{ij}.\]</span></p>
<ol start="4" type="1">
<li><em>Iterate</em>: One full iteration of the algorithm consists of an allocation step (Step 2) and an update step (Step 3). Steps 2 and 3 are repeated until no objects can be moved between clusters, at which point the algorithm has converged to at least a local minimum of Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span>.</li>
</ol>
<p>Upon convergence, we obtain not only the estimated partition <span class="math inline">\(\mathcal{C}=\{C_1,\ldots,C_K\}\)</span>, indicating the cluster membership of each observation, but also the estimated cluster centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> which act as cluster prototypes, efficiently summarising the characteristics of each cluster. The algorithm just described is just one standard variant of <span class="math inline">\(K\)</span>-Means. there have been several algorithms proposed for the same objective which derive their names from the author who proposed them; namely MacQueen <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span>, Lloyd <span class="citation" data-cites="Lloyd1982">[<a href="#ref-Lloyd1982" role="doc-biblioref">37</a>]</span>, Forgy <span class="citation" data-cites="Forgy1965">[<a href="#ref-Forgy1965" role="doc-biblioref">38</a>]</span>, and Hartigan and Wong <span class="citation" data-cites="Hartigan1979">[<a href="#ref-Hartigan1979" role="doc-biblioref">39</a>]</span>. They differ in some subtle ways, particularly with regard to how initial centroids in Step 1 are chosen and whether Steps 3 and 4 are applied to all <span class="math inline">\(n\)</span> observations simultaneously or whether allocations and centroids are updated for each observation one-by-one (i.e., some variants iterate over Step 2 and Step 3 in a loop over <span class="math inline">\(i=1,\ldots,n\)</span>). Without going into further details, we note that the default option for <code>kmeans()</code> in R uses the option <code>algorithm="Hartigan-Wong"</code> and this is what we will henceforth adopt throughout.</p>
</section>
<section id="sec-limitations" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="sec-limitations"><span class="header-section-number">3.1.2</span> <span class="math inline">\(K\)</span>-Means limitations and practical concerns</h4>
<p>Though <span class="math inline">\(K\)</span>-Means is a useful tool in many application contexts due to its conceptual and computational simplicity —so ubiquitous, in fact, that the <code>kmeans()</code> function in R is available without loading any additional libraries— it suffers from numerous limitations and some care is required in order to obtain reasonable results. We now discuss some of the main limitations in turn, but note that each is addressed explicitly throughout the <span class="math inline">\(K\)</span>-Means application portion of the R tutorial in <a href="#sec-kmapp"><span class="quarto-unresolved-ref">sec-kmapp</span></a>. The concerns relate to choosing <span class="math inline">\(K\)</span> (<a href="#sec-elbow"><span class="quarto-unresolved-ref">sec-elbow</span></a>), choosing initial centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> (<a href="#sec-kmpp"><span class="quarto-unresolved-ref">sec-kmpp</span></a>), and relaxing the reliance on squared Euclidean distances with the more general <span class="math inline">\(K\)</span>-Medoids method (<a href="#sec-kmedoids"><span class="quarto-unresolved-ref">sec-kmedoids</span></a>).</p>
<section id="sec-elbow" class="level5" data-number="3.1.2.1">
<h5 data-number="3.1.2.1" class="anchored" data-anchor-id="sec-elbow"><span class="header-section-number">3.1.2.1</span> Fixed <span class="math inline">\(K\)</span> and the elbow method</h5>
<p>The first major drawback is that the number of clusters <span class="math inline">\(K\)</span> must be pre-specified. This is a key input parameter: if <span class="math inline">\(K\)</span> is too low, dissimilar observations will be wrongly grouped together; if <span class="math inline">\(K\)</span> is too large, observations will be partitioned into many small, similar clusters which may not be meaningfully different. Choosing the optimal <span class="math inline">\(K\)</span> necessitates running the algorithm at various fixed values of <span class="math inline">\(K\)</span> and finding the single value of <span class="math inline">\(K\)</span> which best balances interpretability, parsimony, and fit quality. Fit quality is measured by the TWCSS, i.e., the objective function in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span>. Increasing <span class="math inline">\(K\)</span> indefinitely will cause the TWCSS to decrease indefinitely, but this is not what we want. Instead, we seek a <span class="math inline">\(K\)</span> value beyond which the decrease in TWCSS is minimal, in order to yield a parsimonious solution with a reasonable number of clusters to interpret, without overfitting the data or merely subdividing the actual groups. Thus, a commonly used heuristic graphical method for determining the optimal <span class="math inline">\(K\)</span> value is to plot a range of <span class="math inline">\(K\)</span> values against the corresponding obtained TWCSS values and look for an “elbow” or kink in the resulting curve. Such a plot will guide the choice of <span class="math inline">\(K\)</span> in the <span class="math inline">\(K\)</span>-Means portion of R tutorial which follows in <a href="#sec-kmapp"><span class="quarto-unresolved-ref">sec-kmapp</span></a>.</p>
</section>
<section id="sec-kmpp" class="level5" data-number="3.1.2.2">
<h5 data-number="3.1.2.2" class="anchored" data-anchor-id="sec-kmpp"><span class="header-section-number">3.1.2.2</span> Initialisation and <span class="math inline">\(K\)</span>-Means</h5>
<p>An especially pertinent limitation which must be highlighted is that <span class="math inline">\(K\)</span>-Means is liable to converge to sub-optimal local minima, i.e., it is not guaranteed to converge to the global minimum of the objective function in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span>. Many practitioners have observed that the performance of <span class="math inline">\(K\)</span>-Means is particularly sensitive to a good choice of initial cluster centroids in Step 1. Indeed, as different initial settings can lead to different clusterings of the same data, good starting values are vital to the success of the <span class="math inline">\(K\)</span>-Means algorithm. Typically, <span class="math inline">\(K\)</span> random vectors are used to define the initial <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> centroids. One means of mitigating (but not completely remedying) the problem is to run <span class="math inline">\(K\)</span>-Means with a suitably large number of random starting values and choose the solution associated with the set of initial centroids which minimise the TWCSS criterion.</p>
<p>In order to contextualise this issue, it is prudent to first describe some of the main arguments to the <code>kmeans()</code> R function. The following list is a non-exhaustive list of the available arguments to <code>kmeans()</code>:</p>
<ul>
<li><p><code>x</code>: a numeric matrix of data or a <code>data.frame</code> with all numeric columns.</p></li>
<li><p><code>centers</code>: either the number of clusters <span class="math inline">\(K\)</span> or a set of <span class="math inline">\(K\)</span> initial (distinct) cluster centroids.</p></li>
<li><p><code>nstart</code>: if <code>centers</code> is specified as a number, this represents the number of random sets of <span class="math inline">\(K\)</span> initial centroids with which to run the algorithm.</p></li>
<li><p><code>iter.max</code>: the maximum number of allocation/update cycles allowed per set of initial centroids.</p></li>
</ul>
<p>The arguments <code>nstart</code> and <code>iter.max</code> have default values of <code>1</code> and <code>10</code>, respectively. Thus, a user running <code>kmeans()</code> with <code>centers</code> specified as a number will, by default, only use one random set of initial centroids, to which the results are liable to be highly sensitive, and will have the algorithm terminate after just ten iterations, regardless of whether convergence was achieved. It would seem be an improvement, therefore, to increase the values of <code>nstart</code> and <code>iter.max</code> from these defaults. Fortunately, the function automatically returns the single optimal solution according to the random initialisation which yields the lowest TWCSS when <code>nstart</code> exceeds <code>1</code>.</p>
<p>Though this will generally lead to better results, this approach can be computationally onerous if the number of observations <span class="math inline">\(n\)</span>, number of features <span class="math inline">\(d\)</span>, or size of the range of fixed <span class="math inline">\(K\)</span> values under consideration is large. An alternative strategy, which greatly reduces the computational burden and the sensitivity of the final solution to the initial choices of <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k\)</span> is to choose a suitable set of informed starting values in a data-driven fashion. To this end, the so-called <span class="math inline">\(K\)</span>-Means algorithm was proposed <span class="citation" data-cites="Arthur2007">[<a href="#ref-Arthur2007" role="doc-biblioref">40</a>]</span> in order to improve the performance of <span class="math inline">\(K\)</span>-Means by replacing Step 1 with an iterative distance-weighting scheme to select the initial cluster centroids. Though there is still randomness inherent in <span class="math inline">\(K\)</span>-Means, this initialisation technique ensures that the initial centroids are well spread out across the data space, which increases the likelihood of converging to the true global minimum. The <span class="math inline">\(K\)</span>-Means algorithm works as follows:</p>
<ol type="A">
<li><p>Choose an initial centroid uniformly at random from the rows of the data set.</p></li>
<li><p>For each observation not yet chosen as a centroid, compute <span class="math inline">\(D^2(\mathbf{x}_i)\)</span>, which represents the squared Euclidean distance between <span class="math inline">\(\mathbf{x}_i\)</span> and the nearest centroid that has already been chosen.</p></li>
<li><p>Randomly sample new observation as a new centroid vector with probability proportional to <span class="math inline">\(D^2(\mathbf{x}_i)\)</span>.</p></li>
<li><p>Repeat Steps B and C until <span class="math inline">\(K\)</span> centroids have been chosen. If any of the chosen initial centroids are not distinct, add a small amount of random jitter to distinguish the non-unique centroids.</p></li>
<li><p>Proceed as per Steps 2–4 of the traditional <span class="math inline">\(K\)</span>-Means algorithm (or one of its variants).</p></li>
</ol>
<p>Although these steps take extra time, <span class="math inline">\(K\)</span>-Means itself tends to converge very quickly thereafter and thus <span class="math inline">\(K\)</span>-Means actually reduces the computational burden. A manual implementation of the <span class="math inline">\(K\)</span>-Means is provided by the function <code>kmeans_pp()</code> below. Its output can be used as the <code>centers</code> argument when running <code>kmeans()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>kmeans_pp <span class="ot">&lt;-</span> <span class="cf">function</span>(X, <span class="co"># data</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>                      K  <span class="co"># number of centroids</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>                      ) {</span>
<span id="cb1-4"><a href="#cb1-4"></a>  </span>
<span id="cb1-5"><a href="#cb1-5"></a>  <span class="co"># sample initial centroid from distinct rows of X</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>  X       <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">as.matrix</span>(X))</span>
<span id="cb1-7"><a href="#cb1-7"></a>  new_center_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X), <span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a>  centers <span class="ot">&lt;-</span> X[new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-9"><a href="#cb1-9"></a>  </span>
<span id="cb1-10"><a href="#cb1-10"></a>  <span class="co"># let x be all observations not yet chosen as a centroid</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>  X <span class="ot">&lt;-</span> X[<span class="sc">-</span>new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-12"><a href="#cb1-12"></a>  </span>
<span id="cb1-13"><a href="#cb1-13"></a>  <span class="cf">if</span>(K <span class="sc">&gt;=</span> <span class="dv">2</span>) {</span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="co"># loop over remaining centroids</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>    <span class="cf">for</span>(kk <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>K) {</span>
<span id="cb1-16"><a href="#cb1-16"></a>      </span>
<span id="cb1-17"><a href="#cb1-17"></a>      <span class="co"># calculate distances from all observations to all already chosen centroids</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>      distances <span class="ot">&lt;-</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">min</span>(<span class="fu">sum</span>((x <span class="sc">-</span> centers)<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb1-19"><a href="#cb1-19"></a>      </span>
<span id="cb1-20"><a href="#cb1-20"></a>      <span class="co"># sample new centroid with probability proportional to squared Euclidean distance</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>      probabilities <span class="ot">&lt;-</span> distances<span class="sc">/</span><span class="fu">sum</span>(distances)</span>
<span id="cb1-22"><a href="#cb1-22"></a>      new_center_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X), <span class="dv">1</span>, <span class="at">prob=</span>probabilities)</span>
<span id="cb1-23"><a href="#cb1-23"></a>      </span>
<span id="cb1-24"><a href="#cb1-24"></a>      <span class="co"># record the new centroid and remove it for the next iteration</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>      centers <span class="ot">&lt;-</span> <span class="fu">rbind</span>(centers, X[new_center_index,, <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb1-26"><a href="#cb1-26"></a>      X <span class="ot">&lt;-</span> X[<span class="sc">-</span>new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-27"><a href="#cb1-27"></a>    }</span>
<span id="cb1-28"><a href="#cb1-28"></a>  }</span>
<span id="cb1-29"><a href="#cb1-29"></a>  </span>
<span id="cb1-30"><a href="#cb1-30"></a>  <span class="co"># add random jitter to distinguish non-unique centroids and return</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>  centers[<span class="fu">duplicated</span>(centers)] <span class="ot">&lt;-</span> <span class="fu">jitter</span>(centers[<span class="fu">duplicated</span>(centers)])</span>
<span id="cb1-32"><a href="#cb1-32"></a>  <span class="fu">return</span>(centers)</span>
<span id="cb1-33"><a href="#cb1-33"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>However, it should be noted that there is still inherent randomness in <span class="math inline">\(K\)</span>-Means —note the use of <code>sample()</code> in lines 7 and 22— and the algorithm is liable to produce different initial centroids in different runs on the same data. In effect, <span class="math inline">\(K\)</span>-Means does not remove the burden of random initialisation; it is merely a way to have more informed random initialisations. Thus, it would be prudent to run <span class="math inline">\(K\)</span>-Means <em>with</em> <span class="math inline">\(K\)</span>-Means initialisation and select the solution which minimises the TWCSS, to transfer the burden of requiring multiple runs of <span class="math inline">\(K\)</span>-Means with random starting values to fewer runs of <span class="math inline">\(K\)</span>-Means followed by <span class="math inline">\(K\)</span>-Means with more informed starting values. We adopt this strategy in the later R tutorial in <a href="#sec-kmapp"><span class="quarto-unresolved-ref">sec-kmapp</span></a>.</p>
</section>
<section id="sec-kmedoids" class="level5" data-number="3.1.2.3">
<h5 data-number="3.1.2.3" class="anchored" data-anchor-id="sec-kmedoids"><span class="header-section-number">3.1.2.3</span> <span class="math inline">\(K\)</span>-Medoids and other distances</h5>
<p>The <span class="math inline">\(K\)</span>-Means objective function in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span> explicitly relies on squared Euclidean distances and requires all features in the data set to be strictly continuous. An artefact of this distance measure is that it is generally recommended to standardise all features of have a mean of zero and unit variance prior to running <span class="math inline">\(K\)</span>-Means. In general, standardisation is advisable if the values are of incomparable units (e.g., height in inches and weight in kilogram). More specifically for <span class="math inline">\(K\)</span>-Means, it is desirable to ensure all features have comparable variances to avoid having variables with higher magnitudes and variances dominate the distance calculation and have an undue prominence on the clustering partition obtained. While we employ such normalisation to the data used in our R tutorial when applying some pre-processing steps in <a href="#sec-process"><span class="quarto-unresolved-ref">sec-process</span></a>, we note that this is not sufficient to overcome all shortcomings of relying on squared Euclidean distances.</p>
<p>For these reasons and more, <span class="math inline">\(K\)</span>-Medoids —otherwise known as partitioning around medoids (PAM)— was proposed as an extension to <span class="math inline">\(K\)</span>-Means which allows using any alternative dissimilarity measure <span class="citation" data-cites="Kaufman1990-pam">[<a href="#ref-Kaufman1990-pam" role="doc-biblioref">5</a>]</span>. The <span class="math inline">\(K\)</span>-Medoids objective function is given by <span class="math display">\[\sum_{i=1}^n\sum_{k=1}^K z_{ik} d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right),\]</span> where <span class="math inline">\(d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right)\)</span> can be any distance measure rather than squared Euclidean and <span class="math inline">\(\boldsymbol{\psi}_k\)</span> is used in place of the mean vector <span class="math inline">\(\boldsymbol{\mu}_k\)</span>. The PAM algorithm works in much the same fashion as <span class="math inline">\(K\)</span>-Means, alternating between an allocation step which assigns each observation to the cluster with the closest <span class="math inline">\(\boldsymbol{\psi}_k\)</span> (according to the specified distance measure) and an update step which minimises the within-cluster total distance (WCTD). Notably, when minimising with respect to <span class="math inline">\(\boldsymbol{\psi}_k\)</span>, the notion of a cluster centroid <span class="math inline">\(\boldsymbol{\mu}_k\)</span> is redefined as a cluster medoid <span class="math inline">\(\boldsymbol{\psi}_k\)</span>, which is selected among the rows of the observed data set, i.e., the medoid is the observation <span class="math inline">\(\mathbf{x}_i\)</span> from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised. Similar to <span class="math inline">\(K\)</span>-Means, the medoids obtained at convergence again enable straightforward characterisation of a “typical” observation from each cluster and the elbow method from <a href="#sec-elbow"><span class="quarto-unresolved-ref">sec-elbow</span></a> can be adapted to guide the choice of <span class="math inline">\(K\)</span> in <span class="math inline">\(K\)</span>-Medoids by plotting a range of candidate <span class="math inline">\(K\)</span> values against the within-cluster total distance.</p>
<p>This reformulation has three main advantages. Firstly, the distance <span class="math inline">\(d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right)\)</span> is not squared, which diminishes the influence of outliers. As <span class="math inline">\(K\)</span>-Means relies on squared Euclidean distances, which inflates the distances of atypical observations, and defines centroids as means, it is not robust to outliers. Secondly, by defining the medoids as observed rows of the data, rather than finding the value of <span class="math inline">\(\boldsymbol{\psi}_k\)</span> that minimises in general, which could potentially be difficult to estimate for complex data types or particularly sophisticated dissimilarity measures, the algorithm can be much more computationally efficient. It requires only a pre-calculated pairwise dissimilarity matrix as input. Finally, the flexibility afforded by being able to modify the dissimilarity measure enables data which are not strictly continuous to be clustered. In other words, <span class="math inline">\(K\)</span>-Medoids is applicable in cases where the mean is undefined. As examples, one could use the Manhattan or general Minkowski distances as alternatives for clustering continuous data, the Hamming <span class="citation" data-cites="Hamming1950">[<a href="#ref-Hamming1950" role="doc-biblioref">41</a>]</span>, Jaccard <span class="citation" data-cites="Jaccard1901">[<a href="#ref-Jaccard1901" role="doc-biblioref">42</a>]</span>, or Sørensen-Dice <span class="citation" data-cites="Dice1945 Sorensen1948">[<a href="#ref-Dice1945" role="doc-biblioref">43</a>, <a href="#ref-Sorensen1948" role="doc-biblioref">44</a>]</span> distances for clustering binary data, or the Gower distance <span class="citation" data-cites="Gower1971">[<a href="#ref-Gower1971" role="doc-biblioref">45</a>]</span> for clustering mixed-type data with both continuous and categorical features. The closely related <span class="math inline">\(K\)</span>-Modes algorithm <span class="citation" data-cites="Huang1998">[<a href="#ref-Huang1998" role="doc-biblioref">46</a>]</span> has also been proposed specifically for purely categorical data applications, as well as the <span class="math inline">\(K\)</span>-Prototypes algorithm <span class="citation" data-cites="Huang1997">[<a href="#ref-Huang1997" role="doc-biblioref">47</a>]</span> for mixed-type variables applications; neither will considered further in this chapter). As their names suggest, they again redefine the notion of a centroid but otherwise proceed much like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids.</p>
<p>The function <code>pam()</code> in the <code>cluster</code> library in R provides an implementation of <span class="math inline">\(K\)</span>-Medoids, with options for implementing many recent additional speed improvements and improved initialisation strategies <span class="citation" data-cites="Schubert2021">[<a href="#ref-Schubert2021" role="doc-biblioref">48</a>]</span>. We will discuss these in the <span class="math inline">\(K\)</span>-Medoids portion of the later R tutorial in <a href="#sec-pamapp"><span class="quarto-unresolved-ref">sec-pamapp</span></a>. Most dissimilarity measures we will use are implement in the base-R function <code>dist()</code>, with the exception of the Gower distance which is implemented in the <code>daisy()</code> function in the <code>cluster</code> library.</p>
</section>
</section>
</section>
<section id="sec-hclust" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-hclust"><span class="header-section-number">3.2</span> Agglomerative hierarchical clustering</h3>
<p>Hierarchical clustering is another versatile and widely-used dissimilarity-based clustering paradigm. Though also dissimilarity-based, hierarchical clustering differs from partitional clustering methods like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids in that it typically doesn’t avail of the notion of computing distances to a central prototype, be that a centroid mean vector or a medoid, but instead greedily builds a hierarchy of clusters based on dissimilarities between observations themselves and sets of observations. Consequently, a hierarchical clustering solution provides a set of partitions, from a single cluster to as many clusters as observations, rather than the single partition obtained by <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids. The results of a hierarchical clustering are usually presented in the form of a dendrogram visualisation, which illustrates the arrangement of the set of partitions visited and can help guide the decision of the optimal single clustering partition to extract. However, hierarchical clustering shares some of the advantages <span class="math inline">\(K\)</span>-Medoids has over <span class="math inline">\(K\)</span>-Means. Firstly, any valid measure of distance can be used, so it is not restricted to squared Euclidean distances and not restricted to clustering purely continuous data. Secondly, hierarchical clustering algorithms do not require the data set itself as input; all that is required is a matrix of pairwise distances.</p>
<p>Broadly speaking, there are two categories of hierarchical clustering:</p>
<ul>
<li><p><em>Agglomerative</em>: Starting from the bottom of the hierarchy, begin with each observation in a cluster of its own and successively merged pairs of clusters while moving up the hierarchy, until all observations are in one cluster. This approach is sometimes referred to as agglomerative nesting (AGNES; <span class="citation" data-cites="Kaufman1990-agnes">[<a href="#ref-Kaufman1990-agnes" role="doc-biblioref">6</a>]</span>).</p></li>
<li><p><em>Divisive</em>: Starting from the top of the hierarchy, with all observations in one cluster, recursively split clusters while moving down the hierarchy, until all observations are in a cluster of their own. This approach is sometimes referred to as divisive analysis (DIANA; <span class="citation" data-cites="Kaufman1990-diana">[<a href="#ref-Kaufman1990-diana" role="doc-biblioref">49</a>]</span>).</p></li>
</ul>
<p>However, divisive clustering algorithms such as DIANA are much more computationally onerous for even moderately large data sets. Thus, we focus here on the agglomerative variant of hierarchical clustering, AGNES, which is is implemented in both the <code>agnes()</code> function in the <code>cluster</code> library and the <code>hclust()</code> function in base R. We adopt the latter in the hierarchical clustering portion of the R tutorial in <a href="#sec-hcapp"><span class="quarto-unresolved-ref">sec-hcapp</span></a>. That being said, even agglomerative hierarchical clustering has significant computation and memory burdens when <span class="math inline">\(n\)</span> is large <span class="citation" data-cites="Gilpin2013 Bouguettaya2015">[<a href="#ref-Gilpin2013" role="doc-biblioref">50</a>, <a href="#ref-Bouguettaya2015" role="doc-biblioref">51</a>]</span>.</p>
<p>There are three key decisions practitioners must make when employing agglomerative hierarchical clustering. The first of these, the distance measure, has already been discussed in the context of <span class="math inline">\(K\)</span>-Medoids. We now discuss the other two in turn; namely, the so-called linkage criterion for quantifying the distances between merged clusters as the algorithm moves up the hierarchy, and the criterion used for cutting the resulting dendrogram to produce a single partition.</p>
<section id="sec-linkage" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="sec-linkage"><span class="header-section-number">3.2.1</span> Linkage criteria</h4>
<p>Agglomerative hierarchical clustering employs two different notions of dissimilarity. There is the distance measure, <span class="math inline">\(d\)</span>, such as Euclidean, Manhattan, or Gower distance, which is used to quantify the distance between pairs of <em>single</em> observations in the data set. Different choices of distance measure can lead to markedly different clustering results and it is thus common to run the hierarchical clustering algorithm with different choices of distance measure and compare the results. However, in the agglomerative setting, individual observations are successively merged into clusters. In order to decide which clusters should be combined, it is necessary to quantify the dissimilarity of <em>sets</em> of observations as a function of the pairwise distances of observations in the sets. This gives rise to the notion of a linkage criterion. At each step, the two clusters separated by the shortest distance are combined; the linkage criteria is precisely the definition of ‘shortest distance’ which differentiates different agglomerative approaches. Again, the choice of linkage criterion can have a substantial impact on the result of the clustering so multiple solutions with different combinations of distance measure and linkage criterion should be evaluated.</p>
<p>There are a number of commonly-used linkage criteria which we now describe. A non-exhaustive list of such linkages follows —only those which we use in the later R tutorial in <a href="#sec-tutR"><span class="quarto-unresolved-ref">sec-tutR</span></a>— in which we let <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> denote two sets of observations, <span class="math inline">\(\lvert\cdot\rvert\)</span> denote the cardinality of a set, and <span class="math inline">\(d(a, b)\)</span> denote the distance between observations in those corresponding sets according to the chosen distance measure. We note that ties for the maximum or minimum distances for complete linkage and single linkage, respectively, are broken at random.</p>
<ul>
<li><p><em>Complete</em> linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are furthest away from each other according to the chosen distance measure <span class="math inline">\(d\)</span>: <span class="math display">\[\max_{a \in \mathcal{A}, b \in \mathcal{B}} d(a, b).\]</span></p></li>
<li><p><em>Single</em> linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are closest to each other according to the chosen distance measure <span class="math inline">\(d\)</span>: <span class="math display">\[\min_{a \in \mathcal{A}, b \in \mathcal{B}} d(a, b).\]</span></p></li>
<li><p><em>Average</em> linkage: Define the dissimilarity between two clusters as the average distance according to the chosen distance measure <span class="math inline">\(d\)</span> between all pairs of elements (on in each cluster): <span class="math display">\[\frac{1}{\lvert\mathcal{A}\rvert \times \lvert\mathcal{B}\rvert}\sum_{a \in \mathcal{A}}\sum_{b \in \mathcal{B}}d_(a, b).\]</span></p></li>
<li><p><em>Centroid</em> linkage: Define the dissimilarity between two clusters <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> as the distance, according to the chosen distance measure <span class="math inline">\(d\)</span>, between their corresponding centroid vectors <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{A}}\)</span> and <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{B}}\)</span>: <span class="math display">\[d(\boldsymbol{\mu}_{\mathcal{A}}, \boldsymbol{\mu}_{\mathcal{B}}).\]</span></p></li>
<li><p><em>Ward</em> linkage <span class="citation" data-cites="Ward1963">[<a href="#ref-Ward1963" role="doc-biblioref">52</a>]</span>: Instead of measuring the dissimilarity between clusters directly, define the dissimilarity as the cost of merging two clusters as the increase in total within-cluster variance after merging. In other words, minimise the total within-cluster sum-of-squares by finding the pair of clusters at each step which leads to minimum increase in total within-cluster variance after merging, where <span class="math inline">\(\mathcal{A} \cup \mathcal{B}\)</span> denotes the cluster obtained after merging, with corresponding centroid <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{A}\cup\mathcal{B}}\)</span>: <span class="math display">\[\frac{\lvert\mathcal{A}\rvert \times \lvert \mathcal{B}}{\lvert \mathcal{A} \cup \mathcal{B}\rvert}\lVert\boldsymbol{\mu}_{\mathcal{A}} - \boldsymbol{\mu}_{\mathcal{B}}\rVert_2^2 = \sum_{x\in \mathcal{A}\cup\mathcal{B}}\lVert x - \boldsymbol{\mu}_{\mathcal{A}\cup\mathcal{B}}\rVert_2^2 - \sum_{x \in \mathcal{A}}\lVert x - \boldsymbol{\mu}_{\mathcal{A}}\rVert_2^2 - \sum_{x \in \mathcal{B}}\lVert x - \boldsymbol{\mu}_{\mathcal{B}}\rVert_2^2.\]</span></p></li>
</ul>
<p>The Ward and centroid linkage criteria differ from the other linkage criteria in that they are typically meant to be used only when the initial pairwise distances between observations are squared Euclidean distances. All of the above linkage criteria are implemented in the function <code>hclust()</code>, which is available in R without requiring any add-on libraries to be loaded and specifically performs agglomerative hierarchical clustering. Its main arguments are <code>d</code>, a pre-computed pairwise dissimilarity matrix (as can be created from the function <code>dist()</code>), and <code>method</code>, which specifies the linkage criterion (e.g., <code>"complete"</code>, <code>"single"</code>, <code>"average"</code>, and <code>"centroid"</code>). Special care must be taken when employing Ward’s linkage criterion as two options are available: <code>"ward.D"</code>, which assumes that the initial pairwise distance matrix already consists of <em>squared</em> Euclidean distances, and <code>"ward.D2"</code>, which assumes the distances are merely Euclidean distances and performs the squaring internally <span class="citation" data-cites="Murtagh2014">[<a href="#ref-Murtagh2014" role="doc-biblioref">53</a>]</span>.</p>
</section>
<section id="sec-cutree" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="sec-cutree"><span class="header-section-number">3.2.2</span> Cutting the dendrogram</h4>
<p>One might notice that when calling <code>hclust()</code>, the number of clusters <span class="math inline">\(K\)</span> is not specified in advance, as it is when calling <code>kmeans()</code> or <code>pam()</code>. Instead, <code>hclust()</code> returns an object which describes the hierarchy of the tree produced by the clustering process. A visualisation of such a tree is referred to as a dendrogram, which can be thought of as a representation of a set of candidate partitions. In a dendrogram representation of an agglomerative hierarchical clustering solution, each observation is initially in a singleton cluster on its own, along the x-axis, according to their similarities. Thereafter, each observation, and subsequently each set of observations, are merged along the y-axis in a nested fashion. The scale along the y-axis is proportional to the distance, according to the chosen linkage criterion, at which two clusters are combined. In the end, the groups formed towards the bottom of the graph are close together, whereas those at the top of the graph are far apart.</p>
<p>Obtaining a single hard partition of objects into disjoint clusters is obtained by cutting the dendrogram horizontally at the corresponding height. In other words, observations are allocated to clusters by cutting the tree at an appropriate height. Generally, the lower this height, the greater the number of clusters (theoretically, there can be as many clusters as there are observations, <span class="math inline">\(n\)</span>), while the greater the height, the lower the number of clusters (theoretically, there can be as few as only one cluster, corresponding to no group structure in the data). Thus, an advantage of hierarchical clustering is that the user need not know <span class="math inline">\(K\)</span> in advance; the user can manually select <span class="math inline">\(K\)</span> after the fact by examining the constructed tree and fine-tuning the output to find clusters with a desired level of granularity. There is no universally applicable rule for determining the optimal height at which to cut the tree, but it is common to select a height in the region where there is the largest gap between merges, i.e., where there is a relatively wide range of distances over which the number of clusters in the resulting partition does not change. This is, of course, very much guided by the visualisation itself.</p>
<p>In R, one can visualise the dendrogram associated with a particular choice of distance measure and linkage criterion by calling <code>plot()</code> on the output from <code>hclust()</code>. Thereafter, the function <code>cutree()</code> can be used to obtain a single partition. This function takes the arguments <code>tree</code>, which is the result of a call to <code>hclust()</code>, <code>h</code> which is the height at which the tree should be cut, and <code>k</code> which more directly allows the desired number of clusters to be produced. Specifying <code>k</code> finds the corresponding height which yields <code>k</code> clusters and overrides the specification of <code>h</code>.</p>
<p>However, it is often the case that certain combinations of dissimilarity measure and linkage criterion produce undesirable dendrograms. In particular, complete linkage is known to perform poorly in the presence of outliers, given its reliance on maximum distances, and single linkage is known to produce a “chaining” effect on the resulting dendrogram, whereby, due to its reliance on minimum distances, observations tend to continuously join increasingly larger, existing clusters rather than being merged with other observations to form new clusters. A negative consequence of this phenomenon is lack of cohesion: observations at opposite ends of the same cluster in a dendrogram could be quite dissimilar. These limitations can also be attributed to hierarchical clustering —regardless of the linkage employed— optimising a local criterion for each merge, unlike <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids which endeavour to optimise global objectives.</p>
</section>
</section>
<section id="sec-chooseK" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="sec-chooseK"><span class="header-section-number">3.3</span> Choosing the number of clusters</h3>
<p>Determining the number of clusters in a data set is a fraught task. Throughout <a href="#sec-kmtheory"><span class="quarto-unresolved-ref">sec-kmtheory</span></a> and <a href="#sec-hclust"><span class="quarto-unresolved-ref">sec-hclust</span></a>, method-specific strategies for guiding the choice of <span class="math inline">\(K\)</span> were presented. However, they are not without their limitations. For <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, the elbow method is somewhat subjective and unreliable. Often, the presence of an elbow is not so clear at a single value of <span class="math inline">\(K\)</span>. Likewise, for agglomerative hierarchical clustering, choosing a height at which to cut the dendrogram as the criterion for choosing <span class="math inline">\(K\)</span> has also been criticised as an overly subjective method. Moreover, these strategies are only capable of identifying the best <span class="math inline">\(K\)</span> value conditional on the chosen method and do not help to identify the overall best solution among multiple competing methods. Moreover, we are required to choose more than just the optimal <span class="math inline">\(K\)</span> value when using <span class="math inline">\(K\)</span>-Medoids (for which different solutions with different dissimilarity measures and different <span class="math inline">\(K\)</span> values can be obtained) and agglomerative hierarchical clustering (for which different solutions can be obtained using different dissimilarity measures and linkage criteria).</p>
<p>Overcoming these ambiguities and identifying a more general strategy for comparing the quality of clustering partitions is a difficult task for which many criteria have been proposed. Broadly speaking, cluster quality measures fall into two categories:</p>
<ol type="1">
<li><p>Comparing of the uncovered partition to a reference clustering (or known grouping labels).</p></li>
<li><p>Measuring of internal cluster consistency without reference to ground truth labels.</p></li>
</ol>
<p>The first is typically conducted using the Rand index <span class="citation" data-cites="Rand1971">[<a href="#ref-Rand1971" role="doc-biblioref">54</a>]</span> or adjusted Rand index <span class="citation" data-cites="Hubert1985">[<a href="#ref-Hubert1985" role="doc-biblioref">55</a>]</span>, which measure the agreement between two sets of partitions. However, as we will be exploring clustering in exploratory, unsupervised settings, using data for which there is no assumed “true” group structure, we will instead focus on a quality measure of the latter kind. As previously stated, a large number of such criteria have been proposed in the literature: several are summarised in Table 7 of Chapter 10 of this book <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>, where they are used to guide the choice of <span class="math inline">\(K\)</span> for agglomerative hierarchical clustering in the context of sequence analysis. Here, however, for the sake of brevity, we describe only one commonly used criterion which we later employ in the R tutorial in <a href="#sec-tutR"><span class="quarto-unresolved-ref">sec-tutR</span></a> —which is itself a dissimilarity-based measure and is thus universally applicable to all clustering algorithms we employ— even if in most applications it would be wise to inform the choice of <span class="math inline">\(K\)</span> with several such quantitative criteria. Moreover, the practitioner’s own subject matter expertise and assessment of the interpretability of the obtained clusters should also be used to inform the choice of <span class="math inline">\(K\)</span>.</p>
<p>The quantitative criterion we employ is referred to as the average silhouette width (ASW) criterion <span class="citation" data-cites="Rousseeuw1987">[<a href="#ref-Rousseeuw1987" role="doc-biblioref">56</a>]</span>, which is routinely used to assess the cohesion and separation of the clusters uncovered by dissimilarity-based methods. Cohesion refers to the tendency to group similar objects together and separation refers to the tendency to group dissimilar objects apart in non-overlapping clusters. As the name implies, the ASW is computed as the average of observation-specific silhouette widths. Under the assumption that <span class="math inline">\(K&gt;1\)</span>, silhouette widths and the ASW criterion are calculated as follows:</p>
<ol type="A">
<li><p>Let <span class="math inline">\(a(i)\)</span> be the average dissimilarity from observation <span class="math inline">\(i\)</span> to the other members of the same cluster to which observation <span class="math inline">\(i\)</span> is assigned.</p></li>
<li><p>Compute the average dissimilarity from observation <span class="math inline">\(i\)</span> to the members of all <span class="math inline">\(K-1\)</span> other clusters: let <span class="math inline">\(b(i)\)</span> be the minimum such distance computed.</p></li>
<li><p>The silhouette for observation <span class="math inline">\(i\)</span> is then defined to be <span class="math display">\[\begin{align*}
  s(i) &amp;= \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\\
  &amp;= \begin{cases} 1 - \frac{a(i)}{b(i)} &amp; \mbox{if}\:a(i) &lt; b(i)\\
  0&amp; \mbox{if}\:a(i)=b(i)\\
  \frac{b(i)}{a(i)} - 1 &amp; \mbox{if}\:a(i) &gt; b(i),
  \end{cases}
  \end{align*}\]</span> unless observation <span class="math inline">\(i\)</span> is assigned to a cluster of size <span class="math inline">\(1\)</span>, in which case <span class="math inline">\(s(i)=0\)</span>. Notably, <span class="math inline">\(a(i)\)</span> and <span class="math inline">\(b(i)\)</span> need not be calculated using the same dissimilarity measure with which the data were clustered; it is common to adopt the Euclidean distance.</p></li>
<li><p>Define the ASW for a given partition <span class="math inline">\(\mathcal{C}\)</span> as: <span class="math inline">\(\mbox{ASW}\left(\mathcal{C}\right)=\frac{1}{n}\sum_{i=1}^n s_i.\)</span></p></li>
</ol>
<p>Given that <span class="math inline">\(-1 \le s(i) \le 1\)</span>, the interpretation of <span class="math inline">\(s(i)\)</span> is that a silhouette close to <span class="math inline">\(1\)</span> indicates that the observation has been well-clustered, a silhouette close to <span class="math inline">\(-1\)</span> indicates that the observation would be more appropriately assigned to another cluster, and a silhouette close to zero indicates that the observation lies on the boundary of two natural clusters. If most values of <span class="math inline">\(s(i)\)</span> are high, then the clustering solution can be deemed appropriate. This occurs when <span class="math inline">\(a(i) \ll b(i)\)</span>, meaning that observation <span class="math inline">\(i\)</span> must be well-matched its own cluster and poorly-matched to all other clusters. Conversely, if most <span class="math inline">\(s(i)\)</span> values are low or even negative, this provides evidence that <span class="math inline">\(K\)</span> may be too low or too high.</p>
<p>The values of <span class="math inline">\(s(i)\)</span> can be averaged over all observations assigned to the same cluster, as a measure of how tightly grouped the observations in the given cluster are. The ASW criterion itself is simply the mean silhouette width over all observations in the entire data set. Generally, clustering solutions with higher ASW are to be preferred, however it is also prudent to dismiss solutions with many negative silhouettes or particularly low cluster-specific mean silhouettes. A silhouette plot can help in this regard; such a plot depicts all values of <span class="math inline">\(s(i)\)</span>, grouped according to the corresponding cluster and in decreasing order within a cluster. In the R tutorial which follows, ASW values and silhouette plots will guide the choice of an optimal clustering solution in a comparison of multiple methods in <a href="#sec-sil"><span class="quarto-unresolved-ref">sec-sil</span></a>.</p>
</section>
</section>
<section id="sec-tutR" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-tutR"><span class="header-section-number">4</span> Tutorial with R</h2>
<p>In this section, we will learn how to perform clustering using the R programming language <span class="citation" data-cites="R2023">[<a href="#ref-R2023" role="doc-biblioref">7</a>]</span>, using all methods described throughout <a href="#sec-methods"><span class="quarto-unresolved-ref">sec-methods</span></a>. We start by loading the necessary libraries. We will use <code>cluster</code> <span class="citation" data-cites="cluster2022">[<a href="#ref-cluster2022" role="doc-biblioref">36</a>]</span> chiefly for functions related to <span class="math inline">\(K\)</span>-Medoids and silhouettes. As per other chapters in this book, we use <code>tidyverse</code> <span class="citation" data-cites="tidyverse2019">[<a href="#ref-tidyverse2019" role="doc-biblioref">57</a>]</span> for data manipulation and <code>rio</code> <span class="citation" data-cites="rio2023">[<a href="#ref-rio2023" role="doc-biblioref">58</a>]</span> for downloading the data: see <a href="#sec-process"><span class="quarto-unresolved-ref">sec-process</span></a> for details the on the data pre-processing steps employed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rio)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We note that hierarchical clustering and <span class="math inline">\(K\)</span>-Means are implemented in base R and thus no dedicated libraries need to be loaded.</p>
<section id="sec-data" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sec-data"><span class="header-section-number">4.1</span> The data set</h3>
<p>Our case study will be to identify different groups of participants that have a similar role in the discussion forum of a massive open online course (MOOC) for teachers. For that purpose, we will rely on the centrality measures of the participants which indicate their number of contributions (<code>OutDegree</code>), replies (<code>InDegree</code>), position in the network (<code>Closeness_total</code>), worth of their connections (<code>Eigen</code>), spread of their ideas (<code>Diffusion_degree</code>), and more. For more details about the data set, please refer to the data chapter of the book (Chapter 2; <span class="citation" data-cites="Lopez-Pernas2024-dat">[<a href="#ref-Lopez-Pernas2024-dat" role="doc-biblioref">59</a>]</span>). To learn more about centrality measures and how to calculate them, refer to the social network analysis chapter (Chapter 15; <span class="citation" data-cites="Saqr2024-sna">[<a href="#ref-Saqr2024-sna" role="doc-biblioref">60</a>]</span>). We will henceforth refer to these data as “the MOOC centralities data set”. We can download and preview the data with the following commands:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>URL <span class="ot">&lt;-</span> <span class="st">"https://github.com/lamethods/data/raw/main/6_snaMOOC/"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="fu">paste0</span>(URL, <span class="st">"Centralities.csv"</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    name InDegree OutDegree Closeness_total   Betweenness        Eigen
1      1       20        33    0.0010952903  1258.1431850 0.2055232624
2      2        2         5    0.0008084074    26.5242892 0.0107177894
3      3        2         4    0.0007987220    30.6011204 0.0086239241
4      4        2        14    0.0010193680    72.5234541 0.0802648338
5      5       16        17    0.0010604454   309.0327391 0.1615036536
6      6        9        24    0.0010764263   250.3765497 0.1550094982
7      7       32        26    0.0011135857  1934.7200566 0.2302041085
8      8       13        18    0.0010604454   163.7078956 0.1363902568
9      9        2        12    0.0010395010    69.5153192 0.1193482282
10    10        8        12    0.0010582011   716.3507873 0.0874871529
11    11       47        74    0.0011415525  1030.1256784 0.5367529235
12    12        6        18    0.0009017133   113.9356190 0.0792423386
13    13       20        15    0.0010504202   396.2736532 0.0814983741
14    14        5        20    0.0010615711   323.5015525 0.0851784950
15    15       13        18    0.0010810811   824.0809988 0.0918144938
16    16        2         1    0.0007610350     2.4596368 0.0085424677
17    17        2        22    0.0010649627   292.6257167 0.0822010944
18    18        0        19    0.0010482180   125.1398610 0.0681777721
19    19       44        35    0.0011135857  3477.0829186 0.3246377117
20    20        1         2    0.0007535795     9.9781104 0.0101110123
21    21        2         0    0.0007524454     0.7902310 0.0036688846
22    22        8        15    0.0010504202    97.1915580 0.1682192728
23    23        6         0    0.0007874016     5.4497380 0.0180802121
24    24       21        36    0.0011123471  1388.2905421 0.2551222942
25    25        6         6    0.0009900990   215.0169840 0.0528201458
26    26       22        12    0.0010672359   853.1151654 0.0890963591
27    27       13        18    0.0010427529   990.4338200 0.1110229843
28    28        0         1    0.0007220217     0.0000000 0.0013039917
29    29       12        23    0.0010845987  1758.3222101 0.0910494829
30    30       32        58    0.0011185682  1082.0392103 0.4008891477
31    31        0         1    0.0007342144     0.0000000 0.0013570881
32    32       11         4    0.0009940358   348.1051140 0.0254295297
33    33       17         6    0.0010214505   263.0572324 0.0553454645
34    34       17        22    0.0010764263   568.8484248 0.1368169776
35    35       15        19    0.0010416667   669.3602403 0.1322268526
36    36       26        21    0.0011111111  2369.6371414 0.1554575978
37    37        7         4    0.0008130081    56.0357925 0.0159166865
38    38        7         2    0.0010070493    52.4812897 0.0240341490
39    39        5        13    0.0010504202   318.4100709 0.1123570051
40    40        0         1    0.0006269592     0.0000000 0.0009526790
41    41       29         6    0.0008658009   647.9906068 0.0639169509
42    42       10         9    0.0008291874   115.9592816 0.0481324558
43    43        1         7    0.0007917656    13.5338944 0.0134537327
44    44       47        70    0.0011494253  3068.9224639 0.4682891907
45    45        1         2    0.0009900990    29.4644832 0.0193875218
46    46        2         9    0.0008810573   130.2540153 0.0191762968
47    47        0         5    0.0008554320    15.5516046 0.0166120276
48    48        2         4    0.0009871668    51.7756650 0.0187293424
49    49       19        23    0.0010810811  1250.7696743 0.1485283643
50    50       16        14    0.0010626993   484.3529452 0.0999589521
51    51        6        13    0.0010395010    32.6934911 0.0673800053
52    52        4        13    0.0008944544    88.1521237 0.0427412033
53    53       19        16    0.0009337068   682.3111940 0.0694507887
54    54       14        28    0.0010834236  1345.3974363 0.1479616291
55    55        0         4    0.0008382230     1.2298762 0.0226620069
56    56        4        20    0.0009099181   222.3806295 0.0976355523
57    57        7         4    0.0009891197   305.8704317 0.0369728967
58    58       23        16    0.0010822511   524.5896242 0.1246945448
59    59        7        14    0.0010649627   971.3848297 0.0581854175
60    60       25        40    0.0011025358  2177.6429106 0.2368267894
61    61       29        24    0.0010857763  2184.5120982 0.2241756310
62    62       16        17    0.0010718114   133.2216585 0.1254680380
63    63       11        25    0.0010537408   109.5871078 0.1553457884
64    64       26        19    0.0010706638  1835.1876540 0.1217718194
65    65        1         2    0.0009803922     0.0000000 0.0185349570
66    66        4         4    0.0010362694   232.7577008 0.0334718629
67    67       21         9    0.0010810811   961.2135898 0.0856583118
68    68       21        40    0.0010764263   374.4623769 0.3405814112
69    69        4         6    0.0010330579   522.1215068 0.0347591675
70    70        3         1    0.0009832842    46.6600386 0.0179186564
71    71        1        12    0.0010245902    25.1852945 0.0991316666
72    72        3         3    0.0009881423    40.3550087 0.0342750551
73    73        0         2    0.0007309942     7.8528133 0.0017029045
74    74        6        10    0.0008771930   284.1545938 0.0249516706
75    75        3         7    0.0010351967   198.3385407 0.0482709864
76    76        1         4    0.0009813543    56.5269743 0.0194673530
77    77        3         8    0.0010288066    58.8542150 0.0415561810
78    78        3         3    0.0009881423    35.9312246 0.0344712126
79    79        0         4    0.0007363770    11.1939611 0.0032643760
80    80        1         6    0.0008203445    61.6404817 0.0209536405
81    81        3         6    0.0009960159   260.7955917 0.0212576197
82    82        1         6    0.0010277492   256.9924053 0.0293885075
83    83        1        13    0.0010526316   153.0658241 0.0790166252
84    84        5         0    0.0007524454   445.1039951 0.0048074022
85    85        1         6    0.0010330579   274.5465215 0.0292892559
86    86        0         1    0.0005652911     0.0000000 0.0000716541
87    87        3         9    0.0010482180   387.5102444 0.0656313725
88    88       16        14    0.0010548523   675.5725548 0.1096442277
89    89        0         1    0.0007204611     0.0000000 0.0016342418
90    90        1         3    0.0009784736    77.9221439 0.0191879706
91    91        7        11    0.0010384216   353.9543387 0.0475815700
92    92       17        16    0.0010729614   972.0850348 0.0799758753
93    93        1         2    0.0007331378     3.9856015 0.0019786851
94    94        6         1    0.0010204082   305.7537756 0.0438144242
95    95        4         3    0.0009784736   482.5073366 0.0180067198
96    96        0        12    0.0010341262    50.6176824 0.0839651399
97    97        0         5    0.0008438819   179.7650960 0.0128118347
98    98       16        12    0.0010330579    98.0851626 0.0885073388
99    99        3         3    0.0010330579    48.9651050 0.0343347235
100  100       22         7    0.0010741139   829.5885066 0.0974449345
101  101        9         3    0.0010384216   286.7287890 0.0334968424
102  102        0         6    0.0010266940     0.0000000 0.0555380035
103  103        5         9    0.0010330579   460.2065860 0.0594344802
104  104        6        15    0.0009124088   321.5050214 0.0430348022
105  105        2         3    0.0009940358    25.2618419 0.0205745242
106  106        2         4    0.0008051530    10.8406570 0.0163664056
107  107        4        15    0.0010101010   241.3700473 0.0356575372
108  108        0         4    0.0007733952     2.1090963 0.0070117896
109  109        4        18    0.0010245902   954.9946478 0.0967498757
110  110        2         2    0.0009832842   443.7347692 0.0167843259
111  111        0         1    0.0006863418     0.0000000 0.0002501696
112  112        3         2    0.0009842520   227.0754458 0.0202573996
113  113        4         6    0.0009950249    61.1674899 0.0230728887
114  114        6         5    0.0008928571    29.9737396 0.0356927379
115  115       20         6    0.0010373444  1699.6077221 0.0922388296
116  116       32         3    0.0010893246  2890.6619280 0.0563781883
117  117        5         1    0.0007987220    19.7901475 0.0207779040
118  118        1         5    0.0007547170    24.1764397 0.0048142608
119  119        5         2    0.0007800312   126.0135194 0.0060411570
120  120        1         2    0.0009813543   440.0000000 0.0157487639
121  121        1        10    0.0010416667    44.9144907 0.0630623146
122  122        2         1    0.0007530120     8.9998322 0.0025060641
123  123        0         5    0.0007656968   482.6751534 0.0044268576
124  124        0         2    0.0009794319     0.0000000 0.0157452652
125  125        0         1    0.0007363770     0.0000000 0.0008403141
126  126        0         3    0.0009794319     0.0000000 0.0306502162
127  127        0         2    0.0009794319     0.0000000 0.0157452652
128  128       10        14    0.0010266940   411.8043778 0.0513999888
129  129        6         8    0.0010000000   108.4983277 0.0517922910
130  130        0         1    0.0007363770     0.0000000 0.0008403141
131  131        1         4    0.0009852217     2.8975473 0.0333357647
132  132        4         2    0.0007949126    87.1025412 0.0082675182
133  133        2         7    0.0009970090     7.7272429 0.0447899909
134  134        0         2    0.0009727626    52.1522217 0.0150281780
135  135        0         2    0.0007022472     3.8803538 0.0005796703
136  136       12        13    0.0010449321   458.4896246 0.0875081311
137  137       22         8    0.0010718114   978.1210929 0.1056889689
138  138        3         6    0.0010266940    54.8776845 0.0521602724
139  139        0         4    0.0009794319    11.2712265 0.0178767452
140  140        0         3    0.0007097232     0.2500000 0.0015598988
141  141        2         3    0.0009803922   171.2448612 0.0315235328
142  142        5         9    0.0010405827   170.2087175 0.0685809621
143  143        0         6    0.0009861933     0.3495421 0.0489224233
144  144        9        12    0.0010080645    92.4113176 0.0507191236
145  145        1         1    0.0007336757    10.2456310 0.0009332329
146  146        1         2    0.0010162602    24.3588245 0.0255955946
147  147        1        12    0.0010384216   377.3284116 0.0390849360
148  148        0         8    0.0008620690    13.7625434 0.0188352534
149  149        0         1    0.0006825939     0.0000000 0.0004445273
150  150        2         2    0.0009756098   490.3497550 0.0298241369
151  151        1         1    0.0006944444    20.3881741 0.0005105094
152  152        3         2    0.0009861933   141.1772594 0.0199401458
153  153        3         0    0.0007235890     1.7488502 0.0023026212
154  154        4         6    0.0010298661   312.6869558 0.0620743601
155  155        9         9    0.0010482180   587.6814506 0.0724724968
156  156        1         2    0.0007782101     6.9262318 0.0066962098
157  157        3         8    0.0010319917    80.5753815 0.0719291535
158  158        8         7    0.0010020040    80.8673604 0.0833330069
159  159        1         4    0.0008481764    27.1955419 0.0140727593
160  160        0         3    0.0009756098   298.2103338 0.0161681095
161  161        6        12    0.0010235415   171.3821846 0.0508329345
162  162        4         3    0.0010214505    34.7075480 0.0437987803
163  163        2        10    0.0010373444   247.6296528 0.0619569322
164  164        0         1    0.0007102273     0.0000000 0.0008858680
165  165        3         5    0.0010080645   202.9417906 0.0290158123
166  166        0         6    0.0010224949   105.2524755 0.0373375424
167  167       11         3    0.0010030090  1885.6398810 0.0560607838
168  168        0         2    0.0007097232     0.0000000 0.0016903805
169  169        0         5    0.0009794319     0.0000000 0.0323550799
170  170        2         7    0.0009881423    40.0301424 0.0348557849
171  171        1         6    0.0009813543   440.0000000 0.0472705324
172  172        2         6    0.0009920635    30.5377976 0.0523325237
173  173        3         6    0.0009950249   232.5688954 0.0212902804
174  174        1         1    0.0007178751     0.3333333 0.0008041698
175  175        5         0    0.0007547170    43.3982491 0.0069637572
176  176        6         8    0.0010341262   229.9712484 0.0787377717
177  177        9         6    0.0009066183   126.6449910 0.0629222554
178  178        3         5    0.0009861933   260.1443389 0.0183758937
179  179        0         6    0.0010214505    64.0872677 0.0272298518
180  180        0         1    0.0006839945     0.0000000 0.0002683893
181  181        2         1    0.0009727626     0.0000000 0.0305675645
182  182       10         0    0.0008116883    78.3674632 0.0180970474
183  183        3         9    0.0008920607   192.4800476 0.0372420007
184  184        3         9    0.0010384216   132.3586657 0.0575771687
185  185        1        13    0.0010395010   208.1889369 0.0556226290
186  186        0         2    0.0009718173    65.5148380 0.0156849647
187  187        3         0    0.0009727626    16.3998342 0.0169433333
188  188        1         3    0.0009803922     0.0000000 0.0320239158
189  189        3         0    0.0009737098     0.5400000 0.0158122606
190  190        1         4    0.0010193680    31.2438910 0.0378001435
191  191        1         3    0.0007745933    32.8800677 0.0107367339
192  192       16         2    0.0010235415  1193.6517699 0.0498541830
193  193       11        11    0.0010405827   176.2265128 0.0847202775
194  194        5         1    0.0009910803    74.3723282 0.0229173250
195  195        8         4    0.0010570825    76.6709219 0.0676492965
196  196        0         5    0.0009910803    17.5952548 0.0239817322
197  197        1         6    0.0008703220   146.1649322 0.0157551603
198  198       20        16    0.0010660981   290.7078834 0.1042230268
199  199        6         8    0.0010395010    80.8549619 0.0567562325
200  200        2         3    0.0009871668    51.3345018 0.0519314819
201  201       11        11    0.0010471204   527.7475944 0.0567464640
202  202        0        13    0.0010526316   225.6903417 0.0575876420
203  203       10         2    0.0010070493   295.8091378 0.0537772881
204  204        0         2    0.0009737098     0.0000000 0.0157064989
205  205        5        12    0.0010020040   392.2964639 0.0273158502
206  206        1         2    0.0009756098    46.9467739 0.0164000996
207  207        9         7    0.0009165903   171.3910716 0.0554082574
208  208        5         2    0.0007849294   121.1755284 0.0095822381
209  209        2         1    0.0008403361    26.5912048 0.0115754605
210  210        0         2    0.0008354219     0.0000000 0.0109664706
211  211        6         4    0.0010319917   163.6478396 0.0365124544
212  212        5         7    0.0010288066   120.9864244 0.0776605067
213  213        0         1    0.0007423905     0.0000000 0.0035298917
214  214        0         2    0.0009765625     0.0000000 0.0157507543
215  215        0         5    0.0008554320     1.7438446 0.0284214531
216  216        5         2    0.0007836991    66.0314214 0.0158874247
217  217        7         6    0.0010030090   317.5681011 0.0597054394
218  218        3         2    0.0009910803   131.6984210 0.0215004695
219  219       17        13    0.0010309278  1070.3376397 0.0705282998
220  220        0         1    0.0007473842     0.0000000 0.0048387092
221  221        8         2    0.0010162602   208.8539968 0.0375372465
222  222        1         1    0.0007473842     0.0000000 0.0096774184
223  223       24        15    0.0010752688  1113.8768685 0.1578556334
224  224        0         2    0.0007616146     0.0000000 0.0058247139
225  225        0         1    0.0007473842     0.0000000 0.0048387092
226  226        7         8    0.0010526316    91.9025758 0.0680670392
227  227        1         1    0.0007473842     0.0000000 0.0096774184
228  228        0         2    0.0008561644     0.0000000 0.0152609634
229  229        0         1    0.0007057163     0.0000000 0.0007430742
230  230        0         3    0.0007087172     4.6995428 0.0011942190
231  231        0         7    0.0007923930    38.9602667 0.0125992653
232  232        0         3    0.0010193680    17.4863252 0.0260702794
233  233        0         1    0.0007087172     0.0000000 0.0008547973
234  234       37         0    0.0010298661  2851.4012937 0.0573498886
235  235        1         3    0.0010214505    70.9930984 0.0410869536
236  236        0         3    0.0009775171     0.0000000 0.0306646994
237  237        0         1    0.0007087172     0.0000000 0.0008547973
238  238        0         2    0.0007087172     0.0000000 0.0017095946
239  239        1         2    0.0009784736    84.1146373 0.0157829985
240  240        0         2    0.0009775171     0.0000000 0.0157597483
241  241        0         3    0.0009775171     0.0000000 0.0166145456
242  242        0         4    0.0009784736     5.3492631 0.0311842231
243  243        5         3    0.0009940358   927.0811263 0.0328499650
244  244        0         1    0.0007087172     0.0000000 0.0008547973
245  245        0         3    0.0009794319   440.0000000 0.0157632502
246  246        1         5    0.0009852217     5.4026663 0.0486620118
247  247        2         6    0.0010298661   467.6399119 0.0461366771
248  248        6         3    0.0010060362   706.8709599 0.0521014775
249  249        5         7    0.0010298661   323.7554984 0.0874339172
250  250        1         3    0.0010193680    48.5477346 0.0268267582
251  251        4         1    0.0009803922    51.4277058 0.0170808745
252  252        2         1    0.0009775171    14.0935595 0.0189942395
253  253        2         1    0.0009775171    37.7957666 0.0308611229
254  254        1         1    0.0007112376     0.3055556 0.0015770901
255  255        0         2    0.0009727626     0.0000000 0.0151624080
256  256        3         2    0.0009823183    35.5354370 0.0172732509
257  257        3         1    0.0009852217    34.7576035 0.0178121189
258  258        1         4    0.0010193680     8.0146483 0.0422381191
259  259        0         3    0.0009823183    39.5816107 0.0169567553
260  260        0         1    0.0006811989     0.0000000 0.0002222069
261  261        1         1    0.0009727626   440.0000000 0.0149082630
262  262        2         2    0.0008431703    43.3071066 0.0218962762
263  263        1         0    0.0006844627     0.0000000 0.0002349505
264  264        0         2    0.0008382230     5.5808500 0.0108012801
265  265        3         1    0.0009803922   110.7862938 0.0160869373
266  266        1         4    0.0007342144    74.2826689 0.0024213399
267  267        0         2    0.0009832842     0.0000000 0.0183361320
268  268        7         2    0.0008203445    59.9868186 0.0224325314
269  269        0         3    0.0007616146    11.6538688 0.0057784314
270  270        1         1    0.0007122507     0.0000000 0.0027496305
271  271        2         2    0.0009756098     2.3708333 0.0305519411
272  272        1         1    0.0007299270     0.0000000 0.0047056610
273  273        4         0    0.0007680492     4.1656922 0.0036789553
274  274        0         1    0.0007633588     0.0000000 0.0069798275
275  275        1         4    0.0008576329     2.5586654 0.0289128773
276  276        1         4    0.0010245902     9.8113596 0.0621169348
277  277        1         5    0.0008576329     5.1173308 0.0357901986
278  278        0         5    0.0010288066   298.4194982 0.0326629191
279  279        8         4    0.0010141988   716.8506163 0.0290905950
280  280        0         3    0.0009775171     0.0000000 0.0308611229
281  281        5         3    0.0010245902   130.5551629 0.0301679929
282  282        0         1    0.0006915629     0.0000000 0.0004896271
283  283        0         1    0.0006811989     0.0000000 0.0002222069
284  284        1         1    0.0009727626   440.0000000 0.0149082630
285  285        2         4    0.0007710100     2.9681070 0.0085748162
286  286        2         0    0.0007122507     2.3448773 0.0014145675
287  287        3         1    0.0007763975    92.1185201 0.0069937774
288  288        0         2    0.0007147963     8.5459352 0.0016207161
289  289        0         3    0.0007309942    19.5988153 0.0018396395
290  290        0         1    0.0007122507     0.0000000 0.0013748152
291  291        0         1    0.0007122507     0.0000000 0.0013748152
292  292        1         1    0.0009737098     0.0000000 0.0161677026
293  293        2         1    0.0008438819    13.0353242 0.0136698219
294  294        0         1    0.0006854010     0.0000000 0.0007045650
295  295        4         1    0.0009756098   486.2369710 0.0174085357
296  296        0         1    0.0006825939     0.0000000 0.0002594734
297  297        0         2    0.0009727626     0.0000000 0.0151644244
298  298        0         3    0.0010204082     0.0000000 0.0276800358
299  299        0         2    0.0009737098     0.0000000 0.0154644419
300  300       12         3    0.0010504202   602.5685102 0.0449483760
301  301       11         3    0.0010351967    86.4154966 0.0580706106
302  302        1         4    0.0009813543     0.7878834 0.0470691627
303  303        5         2    0.0010288066   228.9672159 0.0361830492
304  304        0         2    0.0009718173     0.0000000 0.0154442576
305  305        1         1    0.0009775171     0.0000000 0.0172220389
306  306        4         0    0.0007451565    39.8290067 0.0020952991
307  307        3         3    0.0008417508    50.7875402 0.0319331044
308  308        8         0    0.0009960159    87.8979307 0.0290549708
309  309        0         1    0.0007336757     0.0000000 0.0022053608
310  310       29         1    0.0010298661  2616.6729426 0.0661528289
311  311        0         1    0.0007087172     0.0000000 0.0009860047
312  312        0         1    0.0007087172     0.0000000 0.0009860047
313  313        0         1    0.0007087172     0.0000000 0.0009860047
314  314        0         2    0.0009813543     0.0000000 0.0158909557
315  315        0         2    0.0007122507     1.3672267 0.0018759113
316  316        0         1    0.0007087172     0.0000000 0.0009860047
317  317        1         5    0.0010266940    10.9665121 0.0425214963
318  318        2         2    0.0010224949     0.0000000 0.0424459643
319  319        2         2    0.0008396306    31.5933013 0.0119802460
320  320        0         3    0.0010183299    37.8212623 0.0265007879
321  321        1         1    0.0007102273     1.7393137 0.0018206017
322  322        4         1    0.0009920635   271.3732402 0.0253975704
323  323        1         2    0.0008424600     0.0000000 0.0210793384
324  324        4         0    0.0009784736    38.9226972 0.0171458495
325  325        1         1    0.0009756098     0.0000000 0.0155749044
326  326        0         3    0.0010162602     0.0000000 0.0259971586
327  327        0         1    0.0007183908     0.0000000 0.0006699533
328  328        0         1    0.0007087172     0.0000000 0.0006876649
329  329        3         6    0.0010235415    84.0661914 0.0729963596
330  330        1         2    0.0009765625   199.5382239 0.0150082859
331  331        2         4    0.0009746589     0.3333333 0.0608536111
332  332        0         1    0.0006854010     0.0000000 0.0002347346
333  333        0         2    0.0009737098     7.8757113 0.0155463827
334  334        2         0    0.0007776050     0.4935323 0.0105097192
335  335        4         2    0.0009891197     6.2401291 0.0382938279
336  336        9         3    0.0010515247   377.5951790 0.0433613070
337  337        3         1    0.0009784736   187.5044542 0.0153453721
338  338        1         3    0.0009775171     0.0000000 0.0457660740
339  339        1         1    0.0009794319    14.2349766 0.0171103119
340  340        1         2    0.0009737098   197.1708453 0.0152022422
341  341        9         2    0.0008802817    86.0695399 0.0331514384
342  342        3         1    0.0009756098   328.6909821 0.0164979346
343  343        1         1    0.0009727626     0.0000000 0.0151315397
344  344        1         1    0.0009727626    97.7934609 0.0149227508
345  345        2         1    0.0009784736    32.3985660 0.0181619796
346  346        1         2    0.0009737098     0.0000000 0.0298459920
347  347        2         3    0.0010341262   198.4910607 0.0335603555
348  348        0         1    0.0007347539     0.0000000 0.0033413268
349  349        0         1    0.0007347539     0.0000000 0.0033413268
350  350        5         1    0.0009940358    35.0007894 0.0199309943
351  351        5         1    0.0008517888    23.5528728 0.0249139158
352  352        0         1    0.0007326007     0.0000000 0.0012767329
353  353        0         2    0.0007468260     0.8040404 0.0035635296
354  354        0         3    0.0010172940     0.0000000 0.0258264741
355  355        1         3    0.0009718173    74.4734700 0.0452013107
356  356        3         2    0.0009794319     4.5595749 0.0326373140
357  357        0         2    0.0009737098    51.7845917 0.0153809124
358  358        1         1    0.0009823183     0.0000000 0.0184348427
359  359        0         1    0.0006973501     0.0000000 0.0007765700
360  360        0         4    0.0009813543    44.9757779 0.0164609490
361  361        1        20    0.0008403361   337.1514998 0.0231037659
362  362        0         1    0.0009708738     0.0000000 0.0149049510
363  363        0         1    0.0009708738     0.0000000 0.0149049510
364  364        0         1    0.0009708738     0.0000000 0.0149049510
365  365        0         1    0.0009708738     0.0000000 0.0149049510
366  366        0         2    0.0009708738     0.0000000 0.0298099021
367  367        0         1    0.0009708738     0.0000000 0.0149049510
368  368        0         1    0.0009708738     0.0000000 0.0149049510
369  369        0         1    0.0009708738     0.0000000 0.0149049510
370  370        0         1    0.0007102273     0.0000000 0.0005180837
371  371        0         1    0.0009708738     0.0000000 0.0149049510
372  372        0         1    0.0009708738     0.0000000 0.0149049510
373  373        0         1    0.0009708738     0.0000000 0.0149049510
374  374        0         1    0.0009708738     0.0000000 0.0149049510
375  375        0         1    0.0009708738     0.0000000 0.0149049510
376  376        0         3    0.0010141988     0.0000000 0.0402321563
377  377        0         2    0.0010141988     0.0000000 0.0253272053
378  378        0         2    0.0010141988     0.0000000 0.0253272053
379  379        0         2    0.0010141988     0.0000000 0.0253272053
380  380        0         1    0.0009708738     0.0000000 0.0149049510
381  381        0         1    0.0009708738     0.0000000 0.0149049510
382  382        0         2    0.0010141988     0.0000000 0.0253272053
383  383        0         1    0.0009708738     0.0000000 0.0149049510
384  384        0         1    0.0009708738     0.0000000 0.0149049510
385  385        0         1    0.0009708738     0.0000000 0.0149049510
386  386        0         1    0.0009708738     0.0000000 0.0149049510
387  387        0         1    0.0009708738     0.0000000 0.0149049510
388  388        0         2    0.0010141988     0.0000000 0.0253272053
389  389        0         1    0.0009708738     0.0000000 0.0149049510
390  390        0         1    0.0009708738     0.0000000 0.0149049510
391  391        0         1    0.0009708738     0.0000000 0.0149049510
392  392        0         1    0.0009708738     0.0000000 0.0149049510
393  393        0         1    0.0009708738     0.0000000 0.0149049510
394  394        0         1    0.0009708738     0.0000000 0.0149049510
395  395        0         1    0.0009708738     0.0000000 0.0149049510
396  396        0         1    0.0009708738     0.0000000 0.0149049510
397  397        0         3    0.0010141988     0.0000000 0.0402321563
398  398        0         1    0.0009708738     0.0000000 0.0149049510
399  399        0         2    0.0010141988     0.0000000 0.0253272053
400  400        0         1    0.0009708738     0.0000000 0.0149049510
401  401        0         1    0.0009708738     0.0000000 0.0149049510
402  402        0         1    0.0009708738     0.0000000 0.0149049510
403  403        0         2    0.0010141988     0.0000000 0.0253272053
404  404        0         1    0.0009708738     0.0000000 0.0149049510
405  405        0         1    0.0009708738     0.0000000 0.0149049510
406  406        0         3    0.0010141988     0.0000000 0.0402321563
407  407        0         1    0.0009708738     0.0000000 0.0149049510
408  408        0         1    0.0009708738     0.0000000 0.0149049510
409  409        0         2    0.0010141988     0.0000000 0.0253272053
410  410        0         3    0.0010141988     0.0000000 0.0357494595
411  411        0         1    0.0009708738     0.0000000 0.0149049510
412  412        0         1    0.0009708738     0.0000000 0.0149049510
413  413        1         1    0.0009708738     0.0000000 0.0298099021
414  414        0         1    0.0009708738     0.0000000 0.0149049510
415  415        0         2    0.0009708738     0.0000000 0.0298099021
416  416        0         1    0.0008340284     0.0000000 0.0104222542
417  417        0         1    0.0008340284     0.0000000 0.0104222542
418  418        0         1    0.0008340284     0.0000000 0.0104222542
419  419        0         1    0.0008340284     0.0000000 0.0104222542
420  420        0         1    0.0008340284     0.0000000 0.0104222542
421  421        0         1    0.0008340284     0.0000000 0.0104222542
422  422        3         1    0.0007501875     4.0905305 0.0081089942
423  423        0         1    0.0008340284     0.0000000 0.0104222542
424  424        0         1    0.0009708738     0.0000000 0.0149049510
425  425        0         1    0.0009708738     0.0000000 0.0149049510
426  426        0         1    0.0009708738     0.0000000 0.0149049510
427  427        0         1    0.0009708738     0.0000000 0.0149049510
428  428        0         1    0.0009708738     0.0000000 0.0149049510
429  429        0         1    0.0009708738     0.0000000 0.0149049510
430  430        0         1    0.0009708738     0.0000000 0.0149049510
431  431        0         1    0.0009708738     0.0000000 0.0149049510
432  432        5        38    0.0009242144  1192.2821069 0.0846484230
433  433        1         1    0.0008340284     0.0000000 0.0208445085
434  434        1         2    0.0008481764     8.5677669 0.0131751104
435  435        0         1    0.0008340284     0.0000000 0.0104222542
436  436        0         1    0.0008340284     0.0000000 0.0104222542
437  437        1         1    0.0007142857     0.0000000 0.0039416695
438  438        1         1    0.0008389262     7.5253736 0.0111396661
439  439        1         1    0.0007147963     0.0000000 0.0033095843
440  440        0         1    0.0009708738     0.0000000 0.0149049510
441  441        0         0              NA     0.0000000 0.0000000000
442  442        0         0              NA     0.0000000 0.0000000000
443  443        0         0              NA     0.0000000 0.0000000000
444  444      475       106    0.0016949153 58205.5873579 1.0000000000
445  445      276        56    0.0013175231 16690.4261052 0.6992478010
    Diffusion.degree Coreness Cross_clique_connectivity
1               1865       18                       305
2                218        6                        13
3                191        6                        11
4                965       13                        37
5               1508       18                       154
6               1607       18                       141
7               2088       21                       588
8               1483       18                       131
9               1216       13                        50
10              1432       17                        88
11              2694       31                      1423
12               896       18                        71
13              1420       18                       241
14              1583       18                       204
15              1632       17                       174
16               130        3                         5
17              1573       17                       221
18              1243       12                        62
19              1940       26                       364
20                94        3                         3
21                78        2                         3
22              1431       17                        90
23               284        6                        16
24              2092       21                       553
25               740       10                        12
26              1617       18                       194
27              1273       17                       143
28                21        1                         2
29              1690       17                       234
30              2224       28                       624
31                36        1                         2
32               775       10                        31
33              1020       13                        69
34              1834       21                       418
35              1280       18                       112
36              1785       18                       265
37               314        9                        23
38               771        9                        22
39              1321       14                        74
40                36        1                         2
41               762       18                        89
42               418       12                        23
43               250        7                        14
44              2608       31                      1168
45               684        3                         5
46               525        8                        16
47               481        5                        11
48               651        6                         8
49              1819       18                       307
50              1598       18                       160
51              1185       18                        43
52               728       14                        64
53              1323       18                       336
54              1744       18                       255
55               361        4                         5
56               790       14                        75
57               739        9                        13
58              1836       20                       367
59              1269       12                        61
60              2109       24                       526
61              1770       21                       326
62              1798       18                       455
63              1480       18                       151
64              1623       18                       288
65               629        3                         4
66              1066        7                        28
67              1645       15                       207
68              1942       28                       383
69              1087        8                        24
70               651        4                         7
71               962        9                        16
72               666        5                         9
73                41        2                         3
74               614       10                        29
75              1087        8                        32
76               677        5                        12
77              1048        9                        24
78               697        5                        13
79                75        4                         7
80               348        5                        11
81               711        6                        19
82               997        6                        16
83              1187       11                        37
84                81        4                         6
85              1002        5                        15
86                 6        1                         2
87              1209       10                        33
88              1415       18                       157
89                31        1                         2
90               630        4                         6
91              1256       12                        84
92              1590       18                       241
93                41        3                         4
94               973        5                        17
95               617        5                        13
96               987        8                        22
97               372        4                         8
98              1295       18                       195
99              1097        6                        15
100             1511       14                       176
101             1076        7                        31
102             1006        6                        16
103             1048       10                        34
104              798       10                        50
105              708        5                        13
106              289        6                        11
107              891       11                        44
108              143        4                         8
109              980       11                        39
110              620        3                         7
111                5        1                         2
112              671        5                         9
113              719        6                        26
114              642        9                        37
115              985       13                        42
116             1416       14                       125
117              314        6                        14
118               94        5                         7
119              144        6                        11
120              620        2                         5
121             1161        9                        38
122               63        3                         4
123              100        3                         6
124              618        2                         4
125               36        1                         2
126              619        3                         4
127              618        2                         4
128              994       14                        51
129              733       10                        20
130               36        1                         2
131              672        5                         8
132              164        5                         8
133              731        7                        10
134              589        2                         3
135               17        2                         3
136             1238       16                        64
137             1580       17                       173
138             1004        8                        18
139              641        4                         7
140               48        3                         6
141              640        4                        10
142             1201       11                        40
143              676        6                        12
144              966       17                        38
145               28        2                         3
146              923        3                         6
147             1169        9                        32
148              478        8                        20
149                5        1                         2
150              588        2                         4
151               11        2                         3
152              682        4                        11
153               36        3                         4
154             1048        8                        25
155             1302       12                        73
156              133        3                         5
157             1053        9                        25
158              741       10                        16
159              398        4                         9
160              601        2                         5
161              978       14                        42
162              992        7                        14
163             1218       11                        42
164               15        1                         2
165              854        6                        26
166              951        6                        13
167              873        7                        55
168               53        2                         4
169              637        5                         8
170              670        7                        16
171              640        6                         9
172              721        7                        12
173              704        6                        23
174               16        2                         3
175              120        5                         9
176             1088       11                        34
177              767       11                        35
178              653        7                        10
179              945        5                        10
180                8        1                         2
181              602        3                         4
182              324        8                        20
183              628        8                        26
184             1214        9                        45
185             1161       10                        45
186              591        2                         4
187              617        3                         6
188              621        4                         8
189              599        3                         6
190              966        4                         8
191              144        3                         5
192              948       11                        35
193             1191       16                        50
194              708        4                        13
195             1405       11                        81
196              743        4                        10
197              444        4                        10
198             1562       18                       200
199             1140       10                        46
200              710        5                         5
201             1257       14                        63
202             1352       11                        74
203              894       10                        23
204              595        2                         4
205              789       10                        36
206              610        3                         8
207              840       11                        65
208              189        6                        11
209              378        3                         5
210              344        2                         4
211             1067        9                        27
212             1152       10                        26
213               66        1                         2
214              605        2                         4
215              451        5                         8
216              173        5                         7
217              753       10                        18
218              705        4                        11
219              999       12                        64
220               80        1                         2
221              973        7                        34
222               81        2                         2
223             1533       24                       147
224              111        2                         4
225               80        1                         2
226             1256       11                        62
227               81        2                         2
228              413        2                         4
229               19        1                         2
230               27        2                         4
231              188        6                        10
232              934        3                         6
233               38        1                         2
234             1004       10                        95
235              954        4                         6
236              621        3                         4
237               38        1                         2
238               39        2                         2
239              624        3                         6
240              620        2                         4
241              621        3                         4
242              631        4                         8
243              676        6                        10
244               38        1                         2
245              622        2                         5
246              674        6                         8
247             1035        6                        23
248              854        8                        21
249             1080       11                        23
250              946        4                         9
251              653        5                         8
252              631        3                         6
253              614        3                         4
254               22        2                         3
255              588        2                         4
256              647        4                        12
257              653        4                         7
258              968        4                         8
259              636        3                         5
260                3        1                         2
261              584        1                         3
262              364        4                         4
263                4        1                         2
264              349        2                         3
265              613        4                         9
266               54        4                         6
267              641        2                         4
268              325        8                        17
269              105        3                         4
270               28        2                         2
271              601        4                         5
272               41        2                         2
273              115        4                         6
274              118        1                         2
275              472        5                        12
276             1035        5                         8
277              472        5                        12
278             1047        4                        11
279              859        8                        23
280              614        3                         4
281             1015        7                        21
282                9        1                         2
283                3        1                         2
284              584        1                         3
285              152        6                         9
286               19        2                         3
287              119        4                         5
288               32        2                         3
289               45        3                         4
290               27        1                         2
291               27        1                         2
292              605        2                         4
293              399        3                         5
294                8        1                         2
295              615        3                         9
296                6        1                         2
297              588        2                         4
298              955        3                         8
299              593        2                         4
300             1257       10                        71
301             1060        8                        38
302              638        5                         6
303             1087        5                        24
304              590        2                         4
305              630        2                         4
306               55        3                         5
307              350        4                         6
308              806        8                        19
309               43        1                         2
310              933       10                        56
311               31        1                         2
312               31        1                         2
313               31        1                         2
314              613        2                         4
315               45        2                         4
316               31        1                         2
317              958        6                        10
318              959        4                         8
319              358        4                         6
320              930        3                         8
321               29        2                         3
322              743        4                        11
323              342        3                         4
324              631        4                         8
325              598        2                         4
326              931        3                         8
327               16        1                         2
328                9        1                         2
329              984        8                        18
330              592        3                         4
331              610        6                         6
332                4        1                         2
333              604        2                         3
334              184        2                         4
335              730        6                        10
336             1265        9                        52
337              602        3                         5
338              615        4                         4
339              625        2                         4
340              592        2                         5
341              620        9                        34
342              608        3                         7
343              586        2                         4
344              586        2                         3
345              642        3                         8
346              589        3                         3
347             1052        4                        13
348               54        1                         2
349               54        1                         2
350              704        6                        12
351              411        6                         7
352               31        1                         2
353               60        2                         3
354              928        3                         8
355              590        4                         4
356              629        4                        10
357              589        2                         3
358              648        2                         4
359               10        1                         2
360              631        4                         7
361              445       10                        35
362              582        1                         2
363              582        1                         2
364              582        1                         2
365              582        1                         2
366              583        2                         2
367              582        1                         2
368              582        1                         2
369              582        1                         2
370               11        1                         2
371              582        1                         2
372              582        1                         2
373              582        1                         2
374              582        1                         2
375              582        1                         2
376              916        3                         4
377              915        2                         4
378              915        2                         4
379              915        2                         4
380              582        1                         2
381              582        1                         2
382              915        2                         4
383              582        1                         2
384              582        1                         2
385              582        1                         2
386              582        1                         2
387              582        1                         2
388              915        2                         4
389              582        1                         2
390              582        1                         2
391              582        1                         2
392              582        1                         2
393              582        1                         2
394              582        1                         2
395              582        1                         2
396              582        1                         2
397              916        3                         4
398              582        1                         2
399              915        2                         4
400              582        1                         2
401              582        1                         2
402              582        1                         2
403              915        2                         4
404              582        1                         2
405              582        1                         2
406              916        3                         4
407              582        1                         2
408              582        1                         2
409              915        2                         4
410              916        3                         4
411              582        1                         2
412              582        1                         2
413              583        2                         2
414              582        1                         2
415              583        2                         2
416              333        1                         2
417              333        1                         2
418              333        1                         2
419              333        1                         2
420              333        1                         2
421              333        1                         2
422               92        4                         5
423              333        1                         2
424              582        1                         2
425              582        1                         2
426              582        1                         2
427              582        1                         2
428              582        1                         2
429              582        1                         2
430              582        1                         2
431              582        1                         2
432             1119       18                       239
433              334        2                         2
434              395        3                         6
435              333        1                         2
436              333        1                         2
437               36        2                         2
438              353        2                         3
439               33        2                         2
440              582        1                         2
441                0        0                         1
442                0        0                         1
443                0        0                         1
444             4342       31                      2830
445             3574       31                      2218</code></pre>
</div>
</div>
<p>Additionally, a number of categorical variables pertaining to demographic characteristics are available for the same participants. Again, please refer to the data chapter of the book (Chapter 2; <span class="citation" data-cites="Lopez-Pernas2024-dat">[<a href="#ref-Lopez-Pernas2024-dat" role="doc-biblioref">59</a>]</span>) for more details on these variables. With an appropriate distance measure, namely the Gower distance measure, we will incorporate some of these variables in our applications of <span class="math inline">\(K\)</span>-Medoids and agglomerative hierarchical clustering (but <em>not</em> <span class="math inline">\(K\)</span>-Means) in addition to the continuous variables contained in <code>df</code>, largely for the purpose of demonstrating clustering methods which are capable of clustering variables of mixed type. We can download and preview the auxiliary categorical data set with the commands below. Note that we select only the following variables:</p>
<ul>
<li><code>experience</code> (coded as a level of experience, 1–3),</li>
<li><code>gender</code> (female or male),</li>
<li><code>region</code> (Midwest, Northeast, South, and West U.S.A., along with International),</li>
</ul>
<p>for the sake of simplifying the demonstration of mixed-type variables clustering and reducing the computational burden. We also extract the <code>UID</code> column, a user ID which corresponds to the <code>name</code> column in <code>df</code>, which will be required for later merging these two data sets. We also ensure that all but this leading <code>UID</code> column is formatted as a <code>factor</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="fu">paste0</span>(URL, <span class="st">"DLT1%20Nodes.csv"</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(UID, experience, gender, region) <span class="sc">|&gt;</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_at</span>(<span class="fu">vars</span>(<span class="sc">-</span>UID), as.factor)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>demog</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    UID experience gender        region
1     1          1 female         South
2     2          1 female         South
3     3          2 female     Northeast
4     4          2 female         South
5     5          3 female         South
6     6          1 female         South
7     7          2 female       Midwest
8     8          1 female International
9     9          1 female         South
10   10          2   male         South
11   11          3 female International
12   12          3 female         South
13   13          2 female          West
14   14          1 female         South
15   15          3 female         South
16   16          1   male International
17   17          1 female         South
18   18          1 female         South
19   19          3   male International
20   20          1   male       Midwest
21   21          1 female         South
22   22          1   male         South
23   23          3 female         South
24   24          2 female       Midwest
25   25          2 female       Midwest
26   26          3 female         South
27   27          1 female       Midwest
28   28          3 female     Northeast
29   29          2 female         South
30   30          3 female         South
31   31          1   male          West
32   32          1 female          West
33   33          3   male          West
34   34          3 female         South
35   35          2   male       Midwest
36   36          2 female          West
37   37          2   male          West
38   38          1   male     Northeast
39   39          2 female         South
40   40          3 female          West
41   41          1   male     Northeast
42   42          3   male     Northeast
43   43          3 female     Northeast
44   44          2   male         South
45   45          2 female          West
46   46          2 female     Northeast
47   47          3 female     Northeast
48   48          3   male       Midwest
49   49          3 female     Northeast
50   50          3 female         South
51   51          3   male         South
52   52          3   male     Northeast
53   53          3 female     Northeast
54   54          2 female     Northeast
55   55          1 female         South
56   56          3 female     Northeast
57   57          3 female       Midwest
58   58          2 female International
59   59          1 female     Northeast
60   60          2 female         South
61   61          3 female         South
62   62          1 female     Northeast
63   63          1 female         South
64   64          2 female     Northeast
65   65          2 female     Northeast
66   66          2 female         South
67   67          3 female         South
68   68          3 female International
69   69          3 female     Northeast
70   70          1 female         South
71   71          3 female          West
72   72          2 female         South
73   73          3 female         South
74   74          3 female     Northeast
75   75          3 female     Northeast
76   76          3 female       Midwest
77   77          2 female         South
78   78          3 female International
79   79          1 female       Midwest
80   80          2 female         South
81   81          3 female          West
82   82          1 female         South
83   83          3 female     Northeast
84   84          1 female     Northeast
85   85          1 female         South
86   86          3   male     Northeast
87   87          2 female     Northeast
88   88          1   male       Midwest
89   89          3 female       Midwest
90   90          3 female       Midwest
91   91          2   male       Midwest
92   92          3   male       Midwest
93   93          1   male         South
94   94          1   male          West
95   95          1 female     Northeast
96   96          2 female         South
97   97          1 female         South
98   98          2 female         South
99   99          3   male          West
100 100          2   male         South
101 101          1 female         South
102 102          3 female     Northeast
103 103          2   male     Northeast
104 104          3 female     Northeast
105 105          2   male         South
106 106          3   male         South
107 107          2 female         South
108 108          1 female     Northeast
109 109          3 female         South
110 110          1   male         South
111 111          2 female         South
112 112          2   male       Midwest
113 113          2 female          West
114 114          2   male International
115 115          2 female     Northeast
116 116          3 female       Midwest
117 117          1 female         South
118 118          1   male International
119 119          2 female         South
120 120          3 female       Midwest
121 121          2   male         South
122 122          3 female         South
123 123          2 female         South
124 124          2   male         South
125 125          3 female     Northeast
126 126          2   male     Northeast
127 127          2   male         South
128 128          1 female International
129 129          2 female       Midwest
130 130          3 female     Northeast
131 131          2 female         South
132 132          1 female       Midwest
133 133          3   male         South
134 134          1   male         South
135 135          1 female         South
136 136          3   male     Northeast
137 137          3   male          West
138 138          2 female     Northeast
139 139          2 female         South
140 140          3 female International
141 141          3 female          West
142 142          2 female         South
143 143          2 female     Northeast
144 144          2   male         South
145 145          1 female       Midwest
146 146          1 female          West
147 147          2 female     Northeast
148 148          2 female         South
149 149          1   male       Midwest
150 150          1   male         South
151 151          2   male     Northeast
152 152          2   male     Northeast
153 153          1   male          West
154 154          3 female       Midwest
155 155          3 female         South
156 156          3 female     Northeast
157 157          1   male         South
158 158          3 female       Midwest
159 159          2   male     Northeast
160 160          3 female       Midwest
161 161          3 female         South
162 162          3 female         South
163 163          1   male     Northeast
164 164          2   male         South
165 165          3 female     Northeast
166 166          1 female     Northeast
167 167          3   male         South
168 168          3 female       Midwest
169 169          3 female     Northeast
170 170          1   male       Midwest
171 171          2   male       Midwest
172 172          2 female International
173 173          2   male          West
174 174          2   male International
175 175          3 female     Northeast
176 176          2   male     Northeast
177 177          2 female     Northeast
178 178          2   male       Midwest
179 179          2   male         South
180 180          3   male International
181 181          1 female          West
182 182          1 female         South
183 183          2   male         South
184 184          1   male     Northeast
185 185          2 female         South
186 186          1 female       Midwest
187 187          2   male          West
188 188          3   male     Northeast
189 189          3 female International
190 190          1 female International
191 191          3 female International
192 192          3 female     Northeast
193 193          2 female     Northeast
194 194          3 female         South
195 195          3 female         South
196 196          1 female         South
197 197          3 female          West
198 198          2 female         South
199 199          3 female     Northeast
200 200          2 female       Midwest
201 201          1 female     Northeast
202 202          2 female         South
203 203          3 female       Midwest
204 204          2 female          West
205 205          1 female         South
206 206          2   male          West
207 207          1   male         South
208 208          2   male     Northeast
209 209          3   male       Midwest
210 210          2   male     Northeast
211 211          2   male         South
212 212          3   male     Northeast
213 213          2 female     Northeast
214 214          1 female     Northeast
215 215          1 female     Northeast
216 216          3 female         South
217 217          3   male          West
218 218          1   male         South
219 219          3 female         South
220 220          3 female          West
221 221          1 female     Northeast
222 222          3   male     Northeast
223 223          2   male          West
224 224          3 female       Midwest
225 225          3 female         South
226 226          3   male     Northeast
227 227          2 female     Northeast
228 228          1   male         South
229 229          2   male International
230 230          3 female         South
231 231          3 female         South
232 232          3 female         South
233 233          2 female         South
234 234          3 female       Midwest
235 235          2 female         South
236 236          1 female       Midwest
237 237          2   male          West
238 238          1   male       Midwest
239 239          1   male          West
240 240          1 female         South
241 241          1 female International
242 242          1 female International
243 243          1   male       Midwest
244 244          3 female         South
245 245          1 female         South
246 246          3 female       Midwest
247 247          2   male       Midwest
248 248          1   male       Midwest
249 249          2 female       Midwest
250 250          2 female         South
251 251          2   male          West
252 252          2   male         South
253 253          3 female     Northeast
254 254          3 female       Midwest
255 255          2 female     Northeast
256 256          3 female     Northeast
257 257          3   male          West
258 258          3   male         South
259 259          3 female         South
260 260          3   male     Northeast
261 261          2 female         South
262 262          3 female     Northeast
263 263          2 female         South
264 264          2 female          West
265 265          3   male       Midwest
266 266          1 female       Midwest
267 267          3   male         South
268 268          1 female         South
269 269          3 female         South
270 270          1 female     Northeast
271 271          2 female         South
272 272          3   male          West
273 273          3 female         South
274 274          3 female         South
275 275          1   male     Northeast
276 276          3   male         South
277 277          3 female          West
278 278          2 female          West
279 279          3 female     Northeast
280 280          3   male       Midwest
281 281          3   male     Northeast
282 282          1 female       Midwest
283 283          2 female         South
284 284          1   male         South
285 285          1   male         South
286 286          3 female     Northeast
287 287          1 female     Northeast
288 288          1   male International
289 289          2 female       Midwest
290 290          3 female       Midwest
291 291          1 female         South
292 292          1 female     Northeast
293 293          3 female         South
294 294          3   male         South
295 295          1   male International
296 296          3   male International
297 297          3   male         South
298 298          3 female         South
299 299          3   male     Northeast
300 300          2   male       Midwest
301 301          2 female       Midwest
302 302          3 female         South
303 303          3 female         South
304 304          1 female     Northeast
305 305          3 female       Midwest
306 306          1   male         South
307 307          2 female       Midwest
308 308          1 female     Northeast
309 309          1 female International
310 310          2   male International
311 311          2   male     Northeast
312 312          2 female     Northeast
313 313          2 female          West
314 314          2 female          West
315 315          3 female          West
316 316          3 female International
317 317          3   male          West
318 318          2 female       Midwest
319 319          2   male       Midwest
320 320          3 female         South
321 321          2   male         South
322 322          1   male         South
323 323          1 female     Northeast
324 324          3 female         South
325 325          2   male         South
326 326          3 female         South
327 327          1   male     Northeast
328 328          3   male       Midwest
329 329          2   male       Midwest
330 330          3 female          West
331 331          2   male          West
332 332          1 female         South
333 333          1 female         South
334 334          3 female         South
335 335          3 female         South
336 336          2 female         South
337 337          3   male     Northeast
338 338          3 female         South
339 339          3 female         South
340 340          3 female         South
341 341          3 female     Northeast
342 342          3   male International
343 343          1 female         South
344 344          3 female       Midwest
345 345          3 female         South
346 346          1   male         South
347 347          2   male         South
348 348          3 female         South
349 349          1 female     Northeast
350 350          2   male     Northeast
351 351          2   male          West
352 352          1 female       Midwest
353 353          1 female     Northeast
354 354          2 female         South
355 355          2 female       Midwest
356 356          2 female       Midwest
357 357          3 female         South
358 358          3 female     Northeast
359 359          3 female     Northeast
360 360          1 female         South
361 361          3 female         South
362 362          2 female       Midwest
363 363          3   male       Midwest
364 364          3   male International
365 365          2 female         South
366 366          2 female International
367 367          1 female     Northeast
368 368          2 female     Northeast
369 369          3 female     Northeast
370 370          3 female         South
371 371          2 female          West
372 372          3 female         South
373 373          1 female     Northeast
374 374          2 female         South
375 375          3 female         South
376 376          2 female     Northeast
377 377          2   male         South
378 378          1   male         South
379 379          2 female         South
380 380          3 female     Northeast
381 381          2 female     Northeast
382 382          2 female         South
383 383          3 female         South
384 384          2 female         South
385 385          3 female         South
386 386          2 female International
387 387          3 female       Midwest
388 388          2   male         South
389 389          3 female     Northeast
390 390          1 female     Northeast
391 391          3 female         South
392 392          3   male         South
393 393          2 female          West
394 394          1   male       Midwest
395 395          2 female          West
396 396          2   male       Midwest
397 397          1 female       Midwest
398 398          1 female         South
399 399          2 female     Northeast
400 400          3   male International
401 401          2 female International
402 402          2   male       Midwest
403 403          3 female       Midwest
404 404          1 female       Midwest
405 405          1   male     Northeast
406 406          2 female       Midwest
407 407          3 female     Northeast
408 408          3   male       Midwest
409 409          1 female         South
410 410          1 female          West
411 411          3 female          West
412 412          1 female         South
413 413          3 female         South
414 414          2 female          West
415 415          3   male         South
416 416          2 female          West
417 417          2 female     Northeast
418 418          2 female     Northeast
419 419          3 female         South
420 420          2 female     Northeast
421 421          2 female       Midwest
422 422          2 female         South
423 423          3   male     Northeast
424 424          2 female          West
425 425          3   male     Northeast
426 426          1 female       Midwest
427 427          3 female       Midwest
428 428          3 female     Northeast
429 429          1   male          West
430 430          2 female          West
431 431          2 female     Northeast
432 432          1 female         South
433 433          3 female         South
434 434          1 female       Midwest
435 435          1   male     Northeast
436 436          2   male         South
437 437          1 female       Midwest
438 438          1   male          West
439 439          1   NULL          NULL
440 440          2 female         South
441 441          3 female         South
442 442          3 female         South
443 443          2 female     Northeast
444 444          3   male         South
445 445          3 female         South</code></pre>
</div>
</div>
<section id="sec-process" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="sec-process"><span class="header-section-number">4.1.1</span> Pre-processing the data</h4>
<p>In <code>df</code>, the first column (<code>name</code>) is the student identifier, and the remaining columns are each of the centrality measures calculated from students’ interactions in the MOOC forum. We will eventually discard the <code>name</code> column from future analyses; we retain it for the time being so that <code>df</code> and <code>demog</code> can be merged appropriately. The data also contain a small number of observations —only three rows of <code>df</code>— with missing values for the variable <code>Closeness_total</code>, as seen by</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> <span class="fu">is.na</span>() <span class="sc">|&gt;</span> <span class="fu">which</span>(<span class="at">arr.ind=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    row col
441 441   4
442 442   4
443 443   4</code></pre>
</div>
</div>
<p>Given that none of the clustering methods described in this chapter are capable of accommodating missing data, we remove these three observations for future analyses too. Furthermore, one of the rows in <code>demog</code> has <code>NULL</code> recorded for the <code>gender</code> variable. We remove all invalid rows from both <code>df</code> and <code>demog</code>. The function <code>complete.cases()</code> constructs a completely observed data set by extracting the rows which contain one or more missing values and we augment the index of fully observed rows with an index of non-<code>NULL</code> <code>gender</code> values. Finally, we drop the redundant <code>NULL</code> level from the <code>factor</code> variable <code>gender</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>obs_ind <span class="ot">&lt;-</span> <span class="fu">complete.cases</span>(df) <span class="sc">&amp;</span> demog<span class="sc">$</span>gender <span class="sc">!=</span> <span class="st">"NULL"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>name[<span class="sc">!</span>obs_ind] <span class="co"># indices of observations with missing values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 439 441 442 443</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">|&gt;</span> <span class="fu">filter</span>(obs_ind)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> <span class="fu">filter</span>(obs_ind) <span class="sc">|&gt;</span> <span class="fu">droplevels</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before proceeding any further, it would be prudent to explore the complete data visually, which we do via the matrix of pairwise scatter plots, excluding the <code>name</code> column, in <a href="#fig-pairs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pairs</span></a>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(df[,<span class="sc">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-pairs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-pairs-1.png" class="img-fluid figure-img" width="744"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Matrix of pairwise scatter plots for all variables in the MOOC centralities data set.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From the plots in <a href="#fig-pairs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pairs</span></a>, we can see that there are clearly two quite extreme outliers. Simple exploratory analyses (not shown) confirms that these are the final two rows of the complete data set. These observations are known to correspond to the two instructors who led the discussion. They have been marked using a red cross symbol in <a href="#fig-pairs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pairs</span></a>. Though we have argued that <span class="math inline">\(K\)</span>-Medoids is a more robust clustering method than <span class="math inline">\(K\)</span>-Means, for example, we also remove these observations in order to avoid their detrimental effects on the <span class="math inline">\(K\)</span>-Means output. The rows must be removed from both <code>df</code> and <code>demog</code> so that they can later be merged. With these observations included, <span class="math inline">\(K\)</span>-Means for instance would likely add one additional cluster containing just these two observations, about whom we already know their role differs substantially from the other observations, as they are quite far from the bulk of the data in terms of squared Euclidean distance. That is not to say, however, that there will not still be outliers in <code>df</code> after removing these two cases.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>keep_ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">|&gt;</span> <span class="fu">slice</span>(keep_ind)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> <span class="fu">slice</span>(keep_ind)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As is good practice when using dissimilarity-based clustering algorithms, we pre-process the purely continuous data by normalising each variable to have a mean of 0 and a variance of 1, by constructing the scaled data frame <code>sdf</code> using the function <code>scale()</code>, again excluding the <code>name</code> column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>sdf <span class="ot">&lt;-</span> <span class="fu">scale</span>(df[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">center=</span><span class="cn">TRUE</span>, <span class="at">scale=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>sdf</code> can be used for all clustering methods described herein. To also accommodate the categorical demographic variables, we use <code>merge()</code> to combine both the scaled continuous data and the categorical data. This requires some manipulation of the <code>name</code> column, with which the two data sets are merged, but we ultimately discard the superfluous <code>name</code> column, which we do not want to contribute to any pairwise distance matrices or clustering solutions, from both <code>merged_df</code> and <code>sdf</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>merged_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">name=</span>df<span class="sc">$</span>name, sdf) <span class="sc">|&gt;</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">merge</span>(demog, <span class="at">by=</span><span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, before proceeding to apply various clustering methods to these data, we present summaries of the counts of each level of the categorical variables <code>experience</code>, <code>gender</code>, and <code>region</code>. These variables imply groupings of size three, two, and five, respectively, and it will be interesting to see if this is borne out in any of the mixed-type clustering applications.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>experience)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  1   2   3 
118 150 171 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>gender)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
female   male 
   299    140 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>region)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
International       Midwest     Northeast         South          West 
           32            77           110           168            52 </code></pre>
</div>
</div>
</section>
</section>
<section id="sec-apps" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="sec-apps"><span class="header-section-number">4.2</span> Clustering applications</h3>
<p>We now show how each of the clustering methods described above can be implemented in R, using these data throughout and taking care to address all practical concerns previously raised. For each method —<span class="math inline">\(K\)</span>-Means in <a href="#sec-kmapp"><span class="quarto-unresolved-ref">sec-kmapp</span></a>, <span class="math inline">\(K\)</span>-Medoids in <a href="#sec-pamapp"><span class="quarto-unresolved-ref">sec-pamapp</span></a>, and agglomerative hierarchical clustering in <a href="#sec-hcapp"><span class="quarto-unresolved-ref">sec-hcapp</span></a>— we show clustering results following the method-specific guidelines for choosing <span class="math inline">\(K\)</span>. However, we conclude by comparing results across different methods using the average silhouette width criterion to further guide the choice of <span class="math inline">\(K\)</span> in <a href="#sec-sil"><span class="quarto-unresolved-ref">sec-sil</span></a>. We refrain from providing an interpretation of the uncovered clusters until <a href="#sec-optimal"><span class="quarto-unresolved-ref">sec-optimal</span></a>, after the optimal clustering solution is identified.</p>
<p>Before we proceed, we set the seed to ensure that results relying on random number generation are reproducible.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2024</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sec-kmapp" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="sec-kmapp"><span class="header-section-number">4.2.1</span> <span class="math inline">\(K\)</span>-Means application</h4>
<p>We begin by showing a naive use of the <code>kmeans()</code> function, supplying only the scaled data <code>sdf</code> and the pre-specified number of clusters <span class="math inline">\(K\)</span> via the <code>centers</code> argument. For now, we assume for no particular reason that there are <span class="math inline">\(K=3\)</span> clusters, just to demonstrate the use of the <code>kmeans()</code> function and its arguments. A number of aspects of the results are printed when we examine the resulting <code>km1</code> object.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>km1 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="dv">3</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>km1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>K-means clustering with 3 clusters of sizes 48, 129, 262

Cluster means:
    InDegree  OutDegree Closeness_total Betweenness       Eigen
1  2.2941694  1.9432559       1.0923551   1.9697769  2.00747791
2 -0.4088814 -0.4311073      -1.4086380  -0.3975783 -0.55451137
3 -0.2189864 -0.1437536       0.4934399  -0.1651209 -0.09475944
  Diffusion.degree   Coreness Cross_clique_connectivity
1        1.9078646  2.1923592                 2.0066265
2       -1.1038258 -0.5622732                -0.3094092
3        0.1939543 -0.1248092                -0.2152835

Clustering vector:
  [1] 1 2 2 3 1 1 1 1 3 1 1 3 1 1 1 2 1 3 1 2 2 1 2 1 3 1 1 2 1 1 2 3 3 1 1 1 2
 [38] 3 3 2 1 3 2 1 3 3 2 3 1 1 3 3 1 1 2 3 3 1 3 1 1 1 1 1 3 3 1 1 3 3 3 3 2 3
 [75] 3 3 3 3 2 2 3 3 3 2 3 2 3 1 2 3 3 1 2 3 3 3 2 1 3 1 3 3 3 3 3 2 3 2 3 3 2
[112] 3 3 3 1 1 2 2 2 3 3 2 2 3 2 3 3 3 3 2 3 2 3 3 2 3 1 3 3 2 3 3 3 3 2 3 3 3
[149] 2 3 2 3 2 3 3 2 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 3 3
[186] 3 3 3 3 3 2 3 3 3 3 3 2 1 3 3 3 3 3 3 3 3 3 2 2 2 3 3 2 3 2 2 3 3 1 2 3 2
[223] 1 2 2 3 2 2 2 2 2 3 2 1 3 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3
[260] 2 3 2 2 2 3 2 3 2 2 2 3 2 2 2 2 3 2 3 3 3 3 2 2 3 2 2 2 2 2 2 2 3 2 2 3 2
[297] 3 3 3 3 3 3 3 3 3 2 2 3 2 1 2 2 2 3 2 2 3 3 2 3 2 3 2 3 3 3 2 2 3 3 3 2 3
[334] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 2
[371] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
[408] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 3

Within cluster sum of squares by cluster:
[1] 858.90295  72.01655 414.30502
 (between_SS / total_SS =  61.6 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
</div>
<p>Among other things, this output shows the estimated centroid vectors <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> upon convergence of the algorithm (<code>$centers</code>), an indicator vector showing the assignment of each observation to one of the <span class="math inline">\(K=3\)</span> clusters (<code>$cluster</code>), the size of each cluster in terms of the number of allocated observations (<code>$size</code>), the within-cluster sum-of-squares <em>per cluster</em> (<code>$withinss</code>), and the ratio of the between-cluster sum-of-squares (<code>$betweenss</code>) to the total sum of squares (<code>$totss</code>). Ideally, this ratio should be large, if the total within-cluster sum-of-squares is minimised. We can access the TWCSS quantity by typing</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>km1<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1345.225</code></pre>
</div>
</div>
<p>However, these results were obtained using the default values of ten maximum iterations (<code>iter.max=10</code>, by default) and only one random set of initial centroid vectors (<code>nstart=1</code>, by default). To increase the likelihood of converging to the global minimum, it is prudent to increase <code>iter.max</code> and <code>nstart</code>, to avoid having the algorithm terminate prematurely and avoid converging to a local minimum, as discussed in <a href="#sec-kmpp"><span class="quarto-unresolved-ref">sec-kmpp</span></a>. We use <code>nstart=50</code>, which is reasonably high but not so high as to incur too large a computational burden.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>km2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="dv">3</span>, <span class="at">nstart=</span><span class="dv">50</span>, <span class="at">iter.max=</span><span class="dv">100</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>km2<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1339.226</code></pre>
</div>
</div>
<p>We can see that the solution associated with the best random starting values achieves a lower TWCSS than the earlier naive attempt. Next, we see if an even lower value can be obtained using the <span class="math inline">\(K\)</span>-Means initialisation strategy, by invoking the <code>kmeans_pp()</code> function from <a href="#sec-kmpp"><span class="quarto-unresolved-ref">sec-kmpp</span></a> with <code>K=3</code> centers and supplying these centroid vectors directly to <code>kmeans()</code>, rather than indicating the number of random <code>centers</code> to use.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>km3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">3</span>), <span class="at">iter.max=</span><span class="dv">100</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>km3<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1343.734</code></pre>
</div>
</div>
<p>In this case, using the <span class="math inline">\(K\)</span>-Means initialisation strategy did not further reduce the TWCSS; in fact it is worse than the solution obtained using <code>nstart=50</code>. However, recall that <span class="math inline">\(K\)</span>-Means is itself subject to randomness and should therefore also be run several times, though the number of <span class="math inline">\(K\)</span>-Means runs need not be so high as 50. In the code below, we use <code>replicate()</code> to invoke both <code>kmeans_pp()</code> and <code>kmeans()</code> itself ten times in order to obtain a better solution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>KMPP <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10</span>, <span class="fu">list</span>(<span class="fu">kmeans</span>(sdf, <span class="at">iter.max=</span><span class="dv">100</span>,</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">3</span>))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Among these ten solutions, five are identical and achieve the same minimum TWCSS value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>TWCSS <span class="ot">&lt;-</span> <span class="fu">sapply</span>(KMPP, <span class="st">"[["</span>, <span class="st">"tot.withinss"</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>TWCSS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1345.225 1339.226 1339.226 1339.226 1340.457 1339.226 1345.225 1345.225
 [9] 1345.225 1339.226</code></pre>
</div>
</div>
<p>Thereafter, we can extract a solution which minimises the <code>tot.withinss</code> as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>km4 <span class="ot">&lt;-</span> KMPP[[<span class="fu">which.min</span>(TWCSS)]]</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>km4<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1339.226</code></pre>
</div>
</div>
<p>Finally, this approach resulted in an identical solution to <code>km2</code> being obtained — with just ten runs of <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Means rather than <code>nstart=50</code> runs of the <span class="math inline">\(K\)</span>-Means algorithm alone— which is indeed superior to the solution obtained with just one uninformed random start in <code>km1</code>. We will thus henceforth adopt this initialisation strategy always.</p>
<p>To date, the <span class="math inline">\(K\)</span>-Means algorithm has only been ran with the fixed number of clusters <span class="math inline">\(K=3\)</span>, which may be suboptimal. The following code iterates over a range of <span class="math inline">\(K\)</span> values, storing both the <code>kmeans()</code> output itself and the TWCSS value for each <span class="math inline">\(K\)</span>. The range <span class="math inline">\(K=1,\ldots,10\)</span> notably includes <span class="math inline">\(K=1\)</span>, corresponding to no group structure in the data. The reason for storing the <code>kmeans()</code> output itself is to avoid having to run <code>kmeans()</code> again after determining the optimal <span class="math inline">\(K\)</span> value; such a subsequent run may not converge to the same minimised TWCSS and having to run the algorithm again would be computationally wasteful.</p>
<div class="cell" data-hash="ch8-clus_cache/html/unnamed-chunk-22_c01f3c148c02144b39ff1e647d4224d9">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># set upper limit on range of K values</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>TWCSS <span class="ot">&lt;-</span> <span class="fu">numeric</span>(K) <span class="co"># allocate space for TWCSS estimates</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>KM <span class="ot">&lt;-</span> <span class="fu">list</span>() <span class="co"># allocate space for kmeans() output</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) { <span class="co"># loop over k=1,...,K</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Run K-means using K-Means++ initialisation: </span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use the current k value and do so ten times if k &gt; 1</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>  KMPP    <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="fu">ifelse</span>(k <span class="sc">&gt;</span> <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">1</span>), </span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">list</span>(<span class="fu">kmeans</span>(sdf, <span class="at">iter.max=</span><span class="dv">100</span>,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span>k))))</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract and store the solution which minimises the TWCSS</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>  KM[[k]] <span class="ot">&lt;-</span> KMPP[[<span class="fu">which.min</span>(<span class="fu">sapply</span>(KMPP, <span class="st">"[["</span>, <span class="st">"tot.withinss"</span>))]]</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract the TWCSS value for the current value of k</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>  TWCSS[k] <span class="ot">&lt;-</span> KM[[k]]<span class="sc">$</span>tot.withinss</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As previously stated in <a href="#sec-elbow"><span class="quarto-unresolved-ref">sec-elbow</span></a>, the so-called “elbow method” consists of plotting the range of <span class="math inline">\(K\)</span> values on the x-axis against the corresponding obtained TWCSS values on the y-axis and looking for an kink in the resulting curve.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">1</span><span class="sc">:</span>K, <span class="at">y=</span>TWCSS, <span class="at">type=</span><span class="st">"b"</span>,</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"K"</span>, <span class="at">ylab=</span><span class="st">"Total Within-Cluster</span><span class="sc">\n</span><span class="st"> Sum-of-Squares"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-elbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-elbow-1.png" class="img-fluid figure-img" width="528"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> Elbow plot showing a range of <span class="math inline">\(K\)</span> values against the corresponding obtained TWCSS for <span class="math inline">\(K\)</span>-Means applied to the MOOC centralities data set using <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K\)</span>-Means<sup>++</sup> initialisation.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Here, it appears that <span class="math inline">\(K=4\)</span> would produce the best results: beyond <span class="math inline">\(K=4\)</span>, the decrease in TWCSS is minimal, which suggests that an extra cluster is not required to model the data well; between <span class="math inline">\(K=3\)</span> and <span class="math inline">\(K=4\)</span>, there is a more substantial decrease in TWCSS, which suggests that the fourth cluster is necessary. This method is of course highly subjective, and we will further inform our choice of <span class="math inline">\(K\)</span> for <span class="math inline">\(K\)</span>-Means using silhouette widths and the ASW criterion in <a href="#sec-sil"><span class="quarto-unresolved-ref">sec-sil</span></a>.</p>
<p>For now, we can interrogate the <span class="math inline">\(K\)</span>-Means solution with <span class="math inline">\(K=4\)</span> by examining the sizes of the clusters and their centroid vectors, using the corresponding fourth element of the list <code>KM</code> by setting <code>K &lt;- 4</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>KM[[K]]<span class="sc">$</span>size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 127  57   8 247</code></pre>
</div>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>KM[[K]]<span class="sc">$</span>centers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    InDegree  OutDegree Closeness_total Betweenness      Eigen Diffusion.degree
1 -0.4086612 -0.4381057      -1.4220978  -0.3994123 -0.5596480       -1.1138889
2  1.5706416  1.1372352       0.9247804   1.3859592  1.0781087        1.4216179
3  4.1035975  5.0512503       1.5201941   3.4673658  5.4946785        3.1631065
4 -0.2852445 -0.2007813       0.4685522  -0.2267743 -0.1390054        0.1422138
    Coreness Cross_clique_connectivity
1 -0.5672245                -0.3102236
2  1.7423739                 0.9615041
3  3.4821417                 5.5033583
4 -0.2232184                -0.2406243</code></pre>
</div>
</div>
<p>However, these centroids correspond to the <em>scaled</em> version of the data created in <a href="#sec-process"><span class="quarto-unresolved-ref">sec-process</span></a>. Interpretation can be made more straightforward by undoing the scaling transformation on these centroids, so that they are on the same scale as the data <code>df</code> rather than the scaled data <code>sdf</code> which was used as input to <code>kmeans()</code>. The column-wise means and standard deviations used when <code>scale()</code> was invoked are available as the attributes <code>"scaled:center"</code> and <code>"scaled:scale"</code>, respectively and are used in the code below. We show these centroids rounded to four decimal places in <a href="#tbl-centroids">Table&nbsp;<span class="quarto-unresolved-ref">tbl-centroids</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>rescaled_centroids <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(KM[[K]]<span class="sc">$</span>centers, <span class="dv">1</span>, <span class="cf">function</span>(r) {</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>                        r <span class="sc">*</span> <span class="fu">attr</span>(sdf, <span class="st">"scaled:scale"</span>) <span class="sc">+</span> <span class="fu">attr</span>(sdf, <span class="st">"scaled:center"</span>) </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>                        } ))</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rescaled_centroids, <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-centroids" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;1<strong>.</strong> Centroids from the <span class="math inline">\(K=4\)</span> <span class="math inline">\(K\)</span>-Means solution on the original data scale.</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 15%">
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 16%">
<col style="width: 8%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1.1024</td>
<td style="text-align: right;">1.7480</td>
<td style="text-align: right;">0.0008</td>
<td style="text-align: right;">20.7010</td>
<td style="text-align: right;">0.0071</td>
<td style="text-align: right;">144.1732</td>
<td style="text-align: right;">2.6378</td>
<td style="text-align: right;">4.6850</td>
</tr>
<tr class="even">
<td style="text-align: right;">15.3684</td>
<td style="text-align: right;">14.8421</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">849.9328</td>
<td style="text-align: right;">0.0996</td>
<td style="text-align: right;">1370.1053</td>
<td style="text-align: right;">16.1053</td>
<td style="text-align: right;">157.5789</td>
</tr>
<tr class="odd">
<td style="text-align: right;">33.6250</td>
<td style="text-align: right;">47.3750</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">1816.6608</td>
<td style="text-align: right;">0.3492</td>
<td style="text-align: right;">2212.1250</td>
<td style="text-align: right;">26.2500</td>
<td style="text-align: right;">703.6250</td>
</tr>
<tr class="even">
<td style="text-align: right;">1.9919</td>
<td style="text-align: right;">3.7206</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">100.8842</td>
<td style="text-align: right;">0.0308</td>
<td style="text-align: right;">751.5061</td>
<td style="text-align: right;">4.6437</td>
<td style="text-align: right;">13.0526</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Now, we can more clearly see that the first cluster, with <span class="math inline">\(n_{k=1}=127\)</span> observations, has the lowest mean value for all <span class="math inline">\(d=8\)</span> centrality measures, while the last cluster, the largest with <span class="math inline">\(n_{k=4}=247\)</span> observations, has moderately larger values for all centrality measures. The two smaller clusters, cluster two with <span class="math inline">\(n_{k=2}=57\)</span> observations and cluster three with only <span class="math inline">\(n_{k=3}=8\)</span> observations have the second-largest and largest values for each measure, respectively. As previously stated, we defer a more-detailed interpretation of uncovered clusters to <a href="#sec-optimal"><span class="quarto-unresolved-ref">sec-optimal</span></a>, after the optimal clustering solution has been identified.</p>
</section>
<section id="sec-pamapp" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="sec-pamapp"><span class="header-section-number">4.2.2</span> <span class="math inline">\(K\)</span>-Medoids application</h4>
<p>The function <code>pam()</code> in the <code>cluster</code> library implements the PAM algorithm for <span class="math inline">\(K\)</span>-Medoids clustering. By default, this function requires only the arguments <code>x</code> (a pre-computed pairwise dissimilarity matrix, as can be created from the functions <code>dist()</code>, <code>daisy()</code>, and more) and <code>k</code>, the number of clusters. However, there are many options for many additional speed improvements and initialisation strategies <span class="citation" data-cites="Schubert2021">[<a href="#ref-Schubert2021" role="doc-biblioref">48</a>]</span>. Here, we invoke a faster variant of the PAM algorithm which necessitates specification of <code>nstart</code> as a number greater than one, to ensure the algorithm is evaluated with multiple random initial medoid vectors, in a similar fashion to <code>kmeans()</code>. Thus, we call <code>pam()</code> with <code>variant="faster"</code> and <code>nstart=50</code> throughout.</p>
<p>Firstly though, we need to construct the pairwise dissimilarity matrices to be used as input to the <code>pam()</code> function. Unlike <span class="math inline">\(K\)</span>-Means, we are not limited to <em>squared</em> Euclidean distances. It is prudent, therefore, to explore <span class="math inline">\(K\)</span>-Medoids solutions with several different dissimilarity measures and compare the different solutions obtained for different measures. Each distance measure will yield different results at the same <span class="math inline">\(K\)</span> value, thus the value of <span class="math inline">\(K\)</span> and the distance measure must be considered as a pair when determining the optimal solution.</p>
<p>We begin by calculating the Euclidean, Manhattan, and Minkowski (with <span class="math inline">\(p=3\)</span>) distances on the scaled continuous data in <code>sdf</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>dist_euclidean <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"euclidean"</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>dist_manhattan <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"manhattan"</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>dist_minkowski3 <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"minkowski"</span>, <span class="at">p=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Secondly, we avail of another principal advantage of <span class="math inline">\(K\)</span>-Medoids; namely, the ability to incorporate categorical variables in mixed-type data sets, by calculating pairwise Gower distances between each row of <code>merged_df</code>, using <code>daisy()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>dist_gower <span class="ot">&lt;-</span> <span class="fu">daisy</span>(merged_df, <span class="at">metric=</span><span class="st">"gower"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As per the <span class="math inline">\(K\)</span>-Means tutorial in <a href="#sec-kmapp"><span class="quarto-unresolved-ref">sec-kmapp</span></a>, we can produce an elbow plot by running the algorithm over a range of <span class="math inline">\(K\)</span> values and extracting the minimised within-cluster total distance achieved upon convergence for each value of <span class="math inline">\(K\)</span>. We demonstrate this for the <code>dist_euclidean</code> input below.</p>
<div class="cell" data-hash="ch8-clus_cache/html/unnamed-chunk-30_487e99e4450bc202f415ef979458e7c3">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>WCTD_euclidean <span class="ot">&lt;-</span> <span class="fu">numeric</span>(K)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>pam_euclidean <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>  pam_euclidean[[k]] <span class="ot">&lt;-</span> <span class="fu">pam</span>(dist_euclidean, <span class="at">k=</span>k, <span class="at">variant=</span><span class="st">"faster"</span>, <span class="at">nstart=</span><span class="dv">50</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  WCTD_euclidean[k] <span class="ot">&lt;-</span> pam_euclidean[[k]]<span class="sc">$</span>objective[<span class="dv">2</span>]</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Equivalent code chunks for the <code>dist_manhattan</code>, <code>dist_minkowski3</code>, and <code>dist_gower</code> inputs are almost identical, so we omit them here for the sake of brevity. Suffice to say, equivalent lists <code>pam_manhattan</code>, <code>pam_minkowski3</code>, and <code>pam_gower</code>, as well as equivalent vectors <code>WCTD_manhattan</code>, <code>WCTD_minkowski3</code>, and <code>WCTD_gower</code>, can all be obtained. Using these objects, corresponding elbow plots for all four dissimilarity measures are showcased in <a href="#fig-pamelbow">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pamelbow</span></a>.</p>
<p>Some of the elbow plots in <a href="#fig-pamelbow">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pamelbow</span></a> are more conclusive than others. As examples, there are reasonably clear elbows at <span class="math inline">\(K=3\)</span> for the Euclidean and Minkowski distances, arguably an elbow at <span class="math inline">\(K=4\)</span> for the Manhattan distance, and no clear, unambiguous elbow under the Gower distance. In any case, the elbow method only helps to identify the optimal <span class="math inline">\(K\)</span> value for a given dissimilarity measure; we defer a discussion of how to choose the overall best <span class="math inline">\(K\)</span>-Medoids clustering solution to later in this tutorial.</p>
<div class="cell" data-hash="ch8-clus_cache/html/fig-pamelbow_e3e6a4214631d2011521959ad5585399">
<div class="cell-output-display">
<div id="fig-pamelbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-pamelbow-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Elbow plots for <span class="math inline">\(K\)</span>-Medoids clustering evaluated with different dissimilarity measures over a range of <span class="math inline">\(K\)</span> values. In clockwise order, beginning with the top-left panel, these are the Euclidean distance, Manhattan distance, Minkowski distance and, for the merged data with additional categorical covariates, the Gower distance.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For now, let’s interrogate the <span class="math inline">\(K=3\)</span> solution obtained using the Euclidean distance. Recall that the results are already stored in the list <code>pam_euclidean</code>, so we merely need to access the third component of that list by setting <code>K &lt;- 3</code>. Firstly, we can examine the size of each cluster by tabulating the cluster-membership indicator vector as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pam_euclidean[[K]]<span class="sc">$</span>clustering)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  1   2   3 
 67 122 250 </code></pre>
</div>
</div>
<p>Examining the medoids which serve as prototypes of each cluster is rendered difficult by virtue of the input having been a distance matrix rather than the data set itself. Though <span class="math inline">\(K\)</span>-Medoids defines the medoids to be the rows in the data set from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised, the <code>medoids</code> component of the output instead gives the <em>indices</em> of the medoids within the data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>pam_euclidean[[K]]<span class="sc">$</span>medoids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  88 272  45</code></pre>
</div>
</div>
<p>Fortunately, these indices within <code>sdf</code> correspond to the same, unscaled observations within <code>df</code>. Allowing for the fact that observations with <code>name</code> greater than the largest index here were removed due to missingness, they are effectively the values of the <code>name</code> column corresponding to the medoids. Thus, we can easily examine the medoids on their original scale. In <a href="#tbl-medoids">Table&nbsp;<span class="quarto-unresolved-ref">tbl-medoids</span></a>, they include the <code>name</code> column, which was <em>not</em> used when calculating the pairwise distance matrices, and are rounded to four decimal places.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">as.numeric</span>(pam_euclidean[[K]]<span class="sc">$</span>medoids)) <span class="sc">|&gt;</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-medoids" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;2<strong>.</strong> Medoids for the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solution obtained using the Euclidean distance on the original data scale, with the corresponding observation index in the column.</caption>
<colgroup>
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 6%">
<col style="width: 15%">
<col style="width: 8%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">name</th>
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">88</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">675.5726</td>
<td style="text-align: right;">0.1096</td>
<td style="text-align: right;">1415</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">157</td>
</tr>
<tr class="even">
<td style="text-align: right;">272</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.0007</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.0047</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">45</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">29.4645</td>
<td style="text-align: right;">0.0194</td>
<td style="text-align: right;">684</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Thus, we can see that there is a small cluster with <span class="math inline">\(n_{k=1}=67\)</span> observations which has the largest values for all <span class="math inline">\(d=8\)</span> centrality measures, a slightly larger cluster with <span class="math inline">\(n_{k=2}=122\)</span> observations and the lowest values for all variables, and the largest cluster with <span class="math inline">\(n_{k=3}=250\)</span> and intermediate values for all variables. The cluster sizes of the <span class="math inline">\(K\)</span>-Medoids solution being more evenly balanced than the earlier <span class="math inline">\(K\)</span>-Means solution is an artefact of <span class="math inline">\(K\)</span>-Medoids being less susceptible to outlying observations by virtue of not squaring the distances. We can explore this by cross-tabulating the clusters obtained by <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K=4\)</span> and <span class="math inline">\(K\)</span>-Medoids with <span class="math inline">\(K=3\)</span> and the Euclidean distance in <a href="#tbl-crosstab">Table&nbsp;<span class="quarto-unresolved-ref">tbl-crosstab</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pam_euclidean[[<span class="dv">3</span>]]<span class="sc">$</span>clustering,</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>      KM[[<span class="dv">4</span>]]<span class="sc">$</span>cluster)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-crosstab" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;3<strong>.</strong> Cross-tabulation of the clusters obtained by <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K=4\)</span> (along the columns) and <span class="math inline">\(K\)</span>-Medoids with <span class="math inline">\(K=3\)</span> and the Euclidean distance (along the rows).</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 8%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td>1</td>
<td></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td>2</td>
<td></td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>3</td>
<td></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">245</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>From the cross-tabulation in <a href="#tbl-crosstab">Table&nbsp;<span class="quarto-unresolved-ref">tbl-crosstab</span></a>, we can see that the <span class="math inline">\(n_k=8\)</span> observations in the smallest <span class="math inline">\(K\)</span>-Means cluster were absorbed into a larger cluster under the <span class="math inline">\(K\)</span>-Medoids solution, thereby demonstrating the robustness of <span class="math inline">\(K\)</span>-Medoids to outliers. Otherwise, both solutions are broadly in agreement.</p>
</section>
<section id="sec-hcapp" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="sec-hcapp"><span class="header-section-number">4.2.3</span> Agglomerative hierarchical clustering application</h4>
<p>Performing agglomerative hierarchical clustering is straightforward now that the distance matrices have already been created for the purposes of running <code>pam()</code>. All that is required is to specify the distance matrix and an appropriate linkage criterion as the <code>method</code> when calling <code>hclust()</code>. We demonstrate this below for a subset of all possible distance measure and linkage criterion combinations among those described in <a href="#sec-linkage"><span class="quarto-unresolved-ref">sec-linkage</span></a>. Recall that for the Ward criterion, the underlying distance measure is usually assumed to be squared Euclidean and that <code>"ward.D2"</code> is the correct <code>method</code> to use when the Euclidean distances are not already squared. For <code>method="centroid"</code>, we manually square the Euclidean distances.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>hc_minkowski3_complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_minkowski3, <span class="at">method=</span><span class="st">"complete"</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>hc_manhattan_single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_manhattan, <span class="at">method=</span><span class="st">"single"</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>hc_gower_average <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_gower, <span class="at">method=</span><span class="st">"average"</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>hc_euclidean_ward <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidean, <span class="at">method=</span><span class="st">"ward.D2"</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>hc_euclidean2_centroid <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidean<span class="sc">^</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"ward.D2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plotting the resulting dendrograms is also straightforward. Simply calling <code>plot()</code> on any of the items above will produce a dendrogram visualisation. We do so here for four of the hierarchical clustering solutions constructed above —the undepicted <code>hc_euclidean2_centroid</code> dendrogram is virtually indistinguishable from that of <code>hc_euclidean_ward</code>— while suppressing the observation indices along the x-axis for clarity by specifying <code>labels=FALSE</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_minkowski3_complete, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Minkwoski Distance (p=3) with Complete Linkage"</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_manhattan_single, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Manhattan Distance with Single Linkage"</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_gower_average, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Gower Distance with Average Linkage"</span>)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_euclidean_ward, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Euclidean Distance with the Ward Criterion"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dendro" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-dendro-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> Dendrograms obtained by agglomerative hierarchical clustering with a selection of dissimilarity measures and linkage criteria.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As previously alluded to in <a href="#sec-cutree"><span class="quarto-unresolved-ref">sec-cutree</span></a>, some combinations of dissimilarity measure and linkage criterion are liable to produce undesirable results. The susceptibility to outliers of complete linkage clustering is visible in the top-left panel of <a href="#fig-dendro">Figure&nbsp;<span class="quarto-unresolved-ref">fig-dendro</span></a>, where just two observations form a cluster at a low height and are never again merged. The tendency of single linkage clustering to exhibit a “chaining” effect whereby all observations are successively merged into just one ever-larger cluster is evident in the top-right panel of <a href="#fig-dendro">Figure&nbsp;<span class="quarto-unresolved-ref">fig-dendro</span></a>, and similar behaviour is observed for the Gower distance with average linkage depicted in the bottom-left panel. The most reasonable results appear to arise from using the Ward criterion in conjunction with Euclidean distances.</p>
<p>Taking the set of candidate partitions in <code>hc_euclidean_ward</code>, for the reasons outlined above, all that remains is to cut this dendrogram at an appropriate height. Practitioners have the freedom to explore different levels of granularity in the final partition. <a href="#fig-cutree">Figure&nbsp;<span class="quarto-unresolved-ref">fig-cutree</span></a> shows the dendrogram from the bottom-right panel of <a href="#fig-dendro">Figure&nbsp;<span class="quarto-unresolved-ref">fig-dendro</span></a> cut horizontally at different heights, indicated by lines of different colours, as well as the corresponding implied <span class="math inline">\(K\)</span> values.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cutree" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-cutree-1.png" class="img-fluid figure-img" width="480"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;5<strong>.</strong> Dendrogram obtained using the Euclidean distance and Ward criterion cut at different heights with the corresponding implied <span class="math inline">\(K\)</span> indicated.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Thereafter, one simply extracts the resulting partition by invoking <code>cutree()</code> with the appropriate height <code>h</code>. For example, to extract the clustering with <span class="math inline">\(K=3\)</span>, which we choose here because of the wide range of heights at which a <span class="math inline">\(K=3\)</span> solution could be obtained:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>hc_ward2 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_euclidean_ward, <span class="at">h=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>hc_ward2</code> is now simply an vector indicating the cluster-membership of each observation in the data set. We show only the first few, for brevity, and then tabulate this vector to compute the cluster sizes. However, interpretation of these clusters is more difficult than in the case of <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, as there is no centroid or medoid prototype with which to characterise each cluster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(hc_ward2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 2 2 2 1 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(hc_ward2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>hc_ward2
  1   2 
 49 390 </code></pre>
</div>
</div>
<p>In this section, we have not presented an exhaustive evaluation of all possible pairs of dissimilarity measure and linkage criterion, but note that the code to do so is trivial. In any case, much like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, we must turn to other cluster quality indices to guide the choice of the best overall solution, be that choosing the best distance and linkage settings for agglomerative hierarchical clustering, or choosing the best clustering method in general among several competing methods. We now turn to finding the optimal clustering solution among multiple competing methods, guided by silhouette widths and plots thereof.</p>
</section>
<section id="sec-sil" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="sec-sil"><span class="header-section-number">4.2.4</span> Identifying the optimal clustering solution</h4>
<p>In our application of <span class="math inline">\(K\)</span>-Means, the elbow method appeared to suggest that <span class="math inline">\(K=4\)</span> yielded the best solution. In our applications of <span class="math inline">\(K\)</span>-Medoids, the elbow method suggested different values of <span class="math inline">\(K\)</span> for different distance metrics. Finally, in our applications of hierarchical clustering, we noted that visualising the resulting dendrogram could be used to guide the choice of the height at which to cut to produce a single hard partition of <span class="math inline">\(K\)</span> clusters. Now, we must determine which method yields the overall best solution. Following <a href="#sec-chooseK"><span class="quarto-unresolved-ref">sec-chooseK</span></a>, we employ silhouettes for this purpose.</p>
<p>For <span class="math inline">\(K\)</span>-Means and agglomerative hierarchical clustering, silhouettes can be computed using the <code>silhouette</code> function in the <code>cluster</code> library, in which case the function requires two arguments; the integer vector containing the cluster membership labels and an appropriate dissimilarity matrix. Thus, for instance, silhouettes could be obtained easily for the following two examples (using <code>kmeans()</code> and <code>hclust()</code>, respectively).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>kmeans_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">4</span>),</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">iter.max=</span><span class="dv">100</span>)<span class="sc">$</span>cluster, </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>                         dist_euclidean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>hclust_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">cutree</span>(<span class="fu">hclust</span>(dist_euclidean, <span class="at">method=</span><span class="st">"ward.D2"</span>), <span class="at">k=</span><span class="dv">2</span>), </span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>                         dist_euclidean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For <span class="math inline">\(K\)</span>-Medoids, it suffices only to supply the output of <code>pam()</code> itself, from which the cluster membership labels and appropriate dissimilarity matrix will be extracted internally, e.g.,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>pam_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">pam</span>(dist_euclidean, <span class="at">k=</span><span class="dv">3</span>, <span class="at">variant=</span><span class="st">"faster"</span>, <span class="at">nstart=</span><span class="dv">50</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Thereafter, <code>plot()</code> can be called on <code>kmeans_sil</code>, <code>hclust_sil</code>, or <code>pam_sil</code> to produce a silhouette plot. For an example based on <code>hclust_sil</code>, see <a href="#fig-sil1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sil1</span></a>. Note that as <span class="math inline">\(K=2\)</span> here, <code>col=2:3</code> colours the silhouettes according to their cluster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hclust_sil, <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sil1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-sil1-1.png" class="img-fluid figure-img" width="480"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;6<strong>.</strong> Silhouette plot for the <span class="math inline">\(K=2\)</span> hierarchical clustering solution obtained using the Ward criterion with Euclidean distances.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-sil1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sil1</span></a> shows that most silhouette widths are positive under this solution, indicating that most observations have been reasonably well-clustered. Cluster 2 appears to be the most cohesive, with a cluster-specific average silhouette width of <span class="math inline">\(0.70\)</span>, while cluster 1 appears to be the least cohesive, with a corresponding average of just <span class="math inline">\(0.24\)</span>. The overall ASW is <span class="math inline">\(0.65\)</span>, as indicated at the foot of the plot.</p>
<p>Given that we adopted a range of <span class="math inline">\(K=1,\ldots,10\)</span> when using <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, and four different dissimilarity measures when using <span class="math inline">\(K\)</span>-Medoids, we have <span class="math inline">\(50\)</span> non-hierarchical candidate solutions to evaluate, of which some seem more plausible than others according to the respective elbow plots. For agglomerative hierarchical clustering, an exhaustive comparison over a range of <span class="math inline">\(K\)</span> values, for each dissimilarity measure and each linkage criterion, would be far too extensive for the present tutorial. Consequently, we limit our evaluation of silhouettes to the <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids solutions already present in the objects <code>KM</code>, <code>pam_euclidean</code>, <code>pam_manhattan</code>, <code>pam_minkowski3</code>, and <code>pam_gower</code>, and the hierarchical clustering solutions which employ the Ward criterion in conjunction with Euclidean distances (of which we consider a further <span class="math inline">\(10\)</span> solutions, again with <span class="math inline">\(K=1,\ldots,10\)</span>, by considering the <span class="math inline">\(10\)</span> possible associated heights at which the dendrogram can be cut). We limit the hierarchical clustering solutions to those based on the Ward criterion given that single linkage and complete linkage have been shown to be susceptible to chaining and sensitive to outliers, respectively. We use the corresponding pre-computed dissimilarity matrices <code>dist_euclidean</code>, <code>dist_manhattan</code>, <code>dist_minkowski3</code>, and <code>dist_gower</code>, where appropriate throughout.</p>
<p>Though the ASW associated with <code>hclust_sil</code> is given on the associated silhouette plot in <a href="#fig-sil1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sil1</span></a>, we can calculate ASW values for other solutions —which we must do to determine the best solution— without producing individiual silhouette plots. To show how this can be done, we examine the structure of the <code>hclust_sil</code> object, showing only its first few rows.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(hclust_sil)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     cluster neighbor sil_width
[1,]       1        2 0.4408007
[2,]       2        1 0.7416708
[3,]       2        1 0.7401549
[4,]       2        1 0.4696606
[5,]       1        2 0.2494557
[6,]       1        2 0.1915306</code></pre>
</div>
</div>
<p>The columns relate to the cluster to which object <span class="math inline">\(i\)</span> is assigned, the cluster for which the corresponding <span class="math inline">\(b(i)\)</span> was minimised, and the <span class="math inline">\(s(i)\)</span> score itself, respectively. Calculating <code>mean(hclust_sil[,3])</code> will return the ASW. Though the code is somewhat tedious, we calculate the ASW criterion values for all <span class="math inline">\(60\)</span> candidate solutions —that is, six methods evaluated over <span class="math inline">\(K=1,\ldots,10\)</span>— using a small helper function to calculate the ASW for the sake of tidying the code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>ASW <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">mean</span>(x[,<span class="dv">3</span>])</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>silhouettes <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">K=</span><span class="dv">2</span><span class="sc">:</span>K,</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmeans=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(KM[[k]]<span class="sc">$</span>cluster, dist_euclidean))),</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_euclidean=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_euclidean[[k]]))),</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_manhattan=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_manhattan[[k]]))),</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_minkowski3=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_minkowski3[[k]]))),</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_gower=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_gower[[k]]))),</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">hc_euclidean_ward=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(<span class="fu">cutree</span>(hc_euclidean_ward, </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>                                                             k), dist_euclidean))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <a href="#fig-silall">Figure&nbsp;<span class="quarto-unresolved-ref">fig-silall</span></a>, we plot these silhouettes against <span class="math inline">\(K\)</span> using <code>matplot()</code>, omitting the code to do so for brevity.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-silall" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-silall-1.png" class="img-fluid figure-img" width="528"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;7<strong>.</strong> ASW criterion values plotted against <span class="math inline">\(K\)</span> for <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-medoids (with the Euclidean, Manhattan, Minkowski (<span class="math inline">\(p=3\)</span>), and Gower distances), and agglomerative hierarchical clustering based on Euclidean distance and the Ward criterion.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>According to <a href="#fig-silall">Figure&nbsp;<span class="quarto-unresolved-ref">fig-silall</span></a>, there is generally little support for <span class="math inline">\(K &gt; 5\)</span> across almost all methods considered, as most method’s ASW values begin to decline after this point. The ASW values also make clear that incorporating the additional categorical demographic variables in a mixed-type clustering using the Gower distance does not lead to reasonable partitions, for any number of clusters <span class="math inline">\(K\)</span>. Overall, the most promising solutions in terms of having the highest ASW are <span class="math inline">\(K\)</span>-Means, Ward hierarchical clustering, and <span class="math inline">\(K\)</span>-Medoids with the Manhattan distance, all with <span class="math inline">\(K=2\)</span>, and <span class="math inline">\(K\)</span>-Medoids with the Euclidean and Minkowski distances, each with <span class="math inline">\(K=3\)</span>. However, it would be wise to examine silhouette widths in more detail, rather than relying merely on the average. The silhouettes for this hierarchical clustering solution are already depicted in <a href="#fig-sil1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sil1</span></a>, so <a href="#fig-silplots">Figure&nbsp;<span class="quarto-unresolved-ref">fig-silplots</span></a> shows individual silhouette widths for the remaining solutions.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-silplots" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-silplots-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;8<strong>.</strong> Silhouette plots showing silhouette widths for a numbering of promising solutions, coloured according to cluster membership.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It is notable that the silhouettes and ASW of the <span class="math inline">\(K=2\)</span> <span class="math inline">\(K\)</span>-Means solution (top-left panel of <a href="#fig-silplots">Figure&nbsp;<span class="quarto-unresolved-ref">fig-silplots</span></a>) and the Ward hierarchical clustering solution (<a href="#fig-sil1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sil1</span></a>) appear almost identical (if one accounts for the clusters being relabelled and associated colours being swapped). Indeed, according to a cross-tabulation of their partitions (not shown), their assignments differ for just <span class="math inline">\(4\)</span> out of <span class="math inline">\(n=439\)</span> observations. Despite having the highest ASW, we can justify dismissing these solutions given that <span class="math inline">\(K=2\)</span> was not well-supported by its corresponding elbow plot in <a href="#fig-elbow">Figure&nbsp;<span class="quarto-unresolved-ref">fig-elbow</span></a>. Similar logic suggests that the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Means solution and the <span class="math inline">\(K=2\)</span> <span class="math inline">\(K\)</span>-Medoids solution based on the Manhattan distance can also be disregarded. Although we stress again that an ideal analysis would more thoroughly determine an optimal solution with reference to additional cluster quality measures and note that various clustering solutions can be legitimate, for potentially different clustering aims, on the same data set <span class="citation" data-cites="Hennig2015 Hennig2016">[<a href="#ref-Hennig2015" role="doc-biblioref">2</a>, <a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>, we can judge that —among the two <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solutions— the one based on the Euclidean distance is arguably preferable, for two reasons. Firstly, its ASW is quite close to that of the Minkowski distance solution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>silhouettes <span class="sc">|&gt;</span> </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(K <span class="sc">==</span> <span class="dv">3</span>) <span class="sc">|&gt;</span> </span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(kmedoids_euclidean, kmedoids_minkowski3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  kmedoids_euclidean kmedoids_minkowski3
1           0.470844           0.4795972</code></pre>
</div>
</div>
<p>Secondly, the first cluster has a higher cluster-specific average silhouette width under the solution based on the Euclidean distance. Indeed, this solution has fewer negative silhouette widths also:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">silhouette</span>(pam_euclidean[[<span class="dv">3</span>]])[,<span class="dv">3</span>] <span class="sc">&lt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 27</code></pre>
</div>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">silhouette</span>(pam_minkowski3[[<span class="dv">3</span>]])[,<span class="dv">3</span>] <span class="sc">&lt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 28</code></pre>
</div>
</div>
</section>
<section id="sec-optimal" class="level4" data-number="4.2.5">
<h4 data-number="4.2.5" class="anchored" data-anchor-id="sec-optimal"><span class="header-section-number">4.2.5</span> Interpreting the optimal clustering solution</h4>
<p>By now, we have identified that the <span class="math inline">\(K=3\)</span> solution obtained using <span class="math inline">\(K\)</span>-Medoids and the Euclidean distance is optimal. Although aspects of this solution were already discussed in <a href="#sec-pamapp"><span class="quarto-unresolved-ref">sec-pamapp</span></a> —in particular, <a href="#tbl-medoids">Table&nbsp;<span class="quarto-unresolved-ref">tbl-medoids</span></a> has already shown the <span class="math inline">\(K=3\)</span> medoid vectors obtained at convergence— we now turn to examining this output in greater detail, in order to provide a fuller interpretation of each of the uncovered clusters. We extract this solution for ease of manipulation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>final_pam <span class="ot">&lt;-</span> pam_euclidean[[<span class="dv">3</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Recall that this method yielded three clusters of sizes <span class="math inline">\(n_1=67\)</span>, <span class="math inline">\(n_2=122\)</span>, and <span class="math inline">\(n_3=250\)</span>. Although the categorical variables were not used by this clustering method, additional interpretative insight can be obtained by augmenting the respective medoids in <a href="#tbl-medoids">Table&nbsp;<span class="quarto-unresolved-ref">tbl-medoids</span></a> with these cluster sizes and the <code>experience</code> values of the corresponding rows of the <code>merged_df</code> data set, which includes the additional demographic features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> </span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>medoids)) <span class="sc">|&gt;</span> </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">4</span>) <span class="sc">|&gt;</span> </span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">size=</span><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering)) <span class="sc">|&gt;</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(<span class="fu">slice</span>(demog <span class="sc">|&gt;</span> </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">select</span>(UID, experience), </span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>medoids)), </span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">by=</span><span class="fu">join_by</span>(name <span class="sc">==</span> UID)) <span class="sc">|&gt;</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-medexp" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;4<strong>.</strong> Medoids for the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solution obtained using the Euclidean distance on the original data scale, augmented with the cluster sizes and the corresponding values of the variable (which was not directly used by the clustering algorithm).</caption>
<colgroup>
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 5%">
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 21%">
<col style="width: 4%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
<th style="text-align: right;">size</th>
<th style="text-align: right;">experience</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">16</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">675.5726</td>
<td style="text-align: right;">0.1096</td>
<td style="text-align: right;">1415</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">157</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.0007</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.0047</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">29.4645</td>
<td style="text-align: right;">0.0194</td>
<td style="text-align: right;">684</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Interpretation and labeling of the clustering results is the step that follows, with a focus only on the medoid values of the centrality scores (had the optimal solution been obtained by <code>kmeans()</code>, we could instead examine its centroids in its <code>$centers</code> component, i.e., mean vectors). We will follow the example papers that we used as a guide for choosing the centrality measures <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span> and <span class="citation" data-cites="Saqr2022b">[<a href="#ref-Saqr2022b" role="doc-biblioref">61</a>]</span>. Both papers have used traditional centrality measures (e.g., degree, closeness, and betweenness) as well as diffusion centralities (diffusion degree and coreness) to infer students’ roles. Furthermore, the second paper has an extended review of the roles and how they have been inferred from centrality measures, so readers are encouraged to read this review.</p>
<p>As the data shows, the first cluster has the highest degree centrality measures (<code>InDegree</code> and <code>OutDegree</code>), highest betweenness centrality, as well as the highest values of the diffusion centralities (<code>Diffusion_degree</code> and <code>Coreness</code>). These values are concordant with students who were actively engaged, received multiple replies, had their contributions discussed by others, and achieved significant diffusion. All of such criteria are concordant with the role of <em>leaders</em>. It stands to reason that the <em>leaders</em> cluster would be the smallest, with <span class="math inline">\(n_1=67\)</span>.</p>
<p>The third cluster has intermediate values for the degree centralities, high diffusion centrality values, as well as relatively high values of betweenness centrality. Such values are concordant with the role of an active participant who participates and coordinates the discussion. Therefore, we will use the label of <em>coordinators</em>.</p>
<p>Finally, the second cluster has the lowest values for all centrality measures (though its diffusion values are still fairly reasonable). Thus, this cluster could feasibly be labelled as an <em>isolates</em> cluster, gathering participants whose role in the discussions is peripheral at best. Overall, the interpretations of this <span class="math inline">\(K=3\)</span> solution are consistent with other findings in the existing literature, e.g., <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span>.</p>
<p>We can now label the clusters accordingly to facilitate more informative cluster-specific summaries. Here, we also recall the size of each cluster with the new labels invoked, to demonstrate their usefulness.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>final_pam<span class="sc">$</span>clustering <span class="ot">&lt;-</span> <span class="fu">factor</span>(final_pam<span class="sc">$</span>clustering, </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">labels=</span><span class="fu">c</span>(<span class="st">"leaders"</span>, <span class="st">"coordinators"</span>, <span class="st">"isolates"</span>))</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     leaders coordinators     isolates 
          67          122          250 </code></pre>
</div>
</div>
<p>As an example, we can use these labels to guide a study of the mean vectors of each cluster(bearing in mind that these are not centroid centroid vectors obtained by <span class="math inline">\(K\)</span>-Means, but rather mean vectors obtained calculated for each group defined by the <span class="math inline">\(K\)</span>-Medoids solution), for which the interpretation of the leader, coordinator, and isolate labels are still consistent with the conclusions drawn from the medoids in <a href="#tbl-medoids">Table&nbsp;<span class="quarto-unresolved-ref">tbl-medoids</span></a>. Note that, for the sake of readability, the group-wise summary below is transposed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(<span class="at">clusters=</span>final_pam<span class="sc">$</span>clustering) <span class="sc">|&gt;</span> </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name) <span class="sc">|&gt;</span> </span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise_all</span>(mean) <span class="sc">|&gt;</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.numeric, round, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols=</span><span class="sc">-</span>clusters, </span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to=</span><span class="st">"centrality"</span>) <span class="sc">|&gt;</span>   </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from=</span>clusters, </span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>              <span class="at">values_from=</span>value) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 8 × 4
  centrality                leaders coordinators isolates
  &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;
1 InDegree                    17.1          1.1      1.98
2 OutDegree                   18.8          1.68     3.62
3 Closeness_total              0            0        0   
4 Betweenness                943.          21.2     99.1 
5 Eigen                        0.13         0.01     0.03
6 Diffusion.degree          1466.         132.     742.  
7 Coreness                    17.2          2.56     4.58
8 Cross_clique_connectivity  220.           4.53    12.6 </code></pre>
</div>
</div>
<p>From <a href="#tbl-medexp">Table&nbsp;<span class="quarto-unresolved-ref">tbl-medexp</span></a>, we can also see that each observation which corresponds to a cluster medoid contains low, high, and medium levels of experience, respectively. However, one should be cautious not to therefore conclude that the clusters neatly map to experience levels, as the following cross-tabulation indicates little-to-no agreement between the groupings of experience levels in the data and the uncovered clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering, </span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>      merged_df<span class="sc">$</span>experience,</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn=</span><span class="fu">c</span>(<span class="st">"Clusters"</span>, <span class="st">"Experience"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Experience
Clusters        1  2  3
  leaders      17 23 27
  coordinators 40 34 48
  isolates     61 93 96</code></pre>
</div>
</div>
<p>Finally, we can produce a visualisation of the uncovered clusters in order to better understand the solution. Visualising multivariate data with <span class="math inline">\(d&gt;2\)</span> is challenging and consequently such visualisations must resort to either plotting the first two principal components or mapping the pairwise dissimilarity matrix to a configuration of points in Cartesian space using multidimensional scaling. The latter is referred to as a “CLUSPLOT” <span class="citation" data-cites="Pison1999">[<a href="#ref-Pison1999" role="doc-biblioref">62</a>]</span> and is implemented in the <code>clusplot()</code> function in the same <code>cluster</code> library as <code>pam()</code> itself. This function uses classical (metric) multi-dimensional scaling <span class="citation" data-cites="Mead1992">[<a href="#ref-Mead1992" role="doc-biblioref">63</a>]</span> to create a bivariate plot displaying the partition of the data. Observations are represented by points in the scatter plot an ellipse spanning the smallest area containing all points in the given cluster is drawn around each cluster. In the code below, only <code>clusplot(final_pam)</code> is strictly necessary to produce such a plot for the optimal <span class="math inline">\(K=3\)</span> Euclidean distance <span class="math inline">\(K\)</span>-Medoids solution; all other arguments are purely for cosmetic purposes for the sake of the resulting <a href="#fig-clusplot">Figure&nbsp;<span class="quarto-unresolved-ref">fig-clusplot</span></a> and are described in <code>?clusplot</code>.</p>
<p><a href="#fig-clusplot">Figure&nbsp;<span class="quarto-unresolved-ref">fig-clusplot</span></a> shows that the <em>leaders</em> cluster —the smallest cluster with the highest value for all centrality measures— is quite diffuse. Conversely, the larger <em>coordinators</em> cluster, with the smallest values for all centrality measures, and the <em>isolates</em> cluster, the largest of all, with intermediate values for all centrality measures, are more compact. This is consistent with the cluster-specific average silhouette widths shown in the bottom-right panel of <a href="#fig-silplots">Figure&nbsp;<span class="quarto-unresolved-ref">fig-silplots</span></a>. That being said, the large span of the <em>leaders</em> cluster again affirms the relative robustness of <span class="math inline">\(K\)</span>-Medoids to outliers, of which some (all of which are leaders) still remain despite the earlier pre-processing steps.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clusplot</span>(final_pam,                                    <span class="co"># the pam() output</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">main=</span><span class="st">""</span>, <span class="at">sub=</span><span class="cn">NA</span>,            <span class="co"># remove the main title and subtitle</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">lines=</span><span class="dv">0</span>,                     <span class="co"># do not draw any lines on the plot</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">labels=</span><span class="dv">4</span>,                              <span class="co"># only label the ellipses</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">col.clus=</span><span class="st">"black"</span>,               <span class="co"># colour for ellipses and labels</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">col.p=</span><span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>clustering) <span class="sc">+</span> <span class="dv">1</span>, <span class="co"># colours for points</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex.txt=</span><span class="fl">0.75</span>,                      <span class="co"># control size of text labels</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">17</span>)          <span class="co"># expand x-axis to avoid trimming labels</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>         )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-clusplot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-clusplot-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;9<strong>.</strong> Two-dimensional clustering plot for the final <span class="math inline">\(K=3\)</span> Euclidean distance <span class="math inline">\(K\)</span>-Medoids solution obtained via classical multidimensional scaling. The ellipses around each cluster are given the associated labels of , , and and the points are coloured according to cluster membership.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-disc" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-disc"><span class="header-section-number">5</span> Discussion &amp; further readings</h2>
<p>The present analysis of the MOOC centralities data set incorporated three of the most commonly used dissimilarity-based clustering approaches; namely, <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-Medoids, and agglomerative hierarchical clustering. Throughout the applications, emphasis was placed on the sensitivity of the results to various choices regarding algorithmic inputs available to practitioners, be that the choice of how to choose initial centroids for <span class="math inline">\(K\)</span>-Means, the choice of dissimilarity measure for <span class="math inline">\(K\)</span>-Medoids, or the choice of linkage criterion for hierarchical clustering, as examples. Consequently, our analysis considered different values of <span class="math inline">\(K\)</span>, different distances, different linkage criteria, different combinations thereof, and indeed different clustering algorithms entirely, in an attempt to uncover the “best” clustering solution for the MOOC centralities data set. We showed how elbow plots and other graphical tools can guide the choice of <span class="math inline">\(K\)</span> for a given method but ultimately identified —via the average silhouette width criterion— an optimal solution with <span class="math inline">\(K=3\)</span>, using <span class="math inline">\(K\)</span>-Medoids in conjunction with the Euclidean distance measure. Although we note that there are a vast array of other cluster quality measures in the literature which target different definitions of what constitutes a “good” clustering <span class="citation" data-cites="Hennig2016">[<a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>, the solution we obtained seems reasonable, in that the uncovered clusters which we labelled as <em>leaders</em>, <em>coordinators</em>, and <em>isolates</em> are consistent, from an interpretative point of view, with existing educational research. Indeed, several published studies have uncovered similar patterns of three groups which can be labelled in the same way <span class="citation" data-cites="KIM201962 Saqr2020-vr Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>, <a href="#ref-KIM201962" role="doc-biblioref">64</a>, <a href="#ref-Saqr2020-vr" role="doc-biblioref">65</a>]</span>.</p>
<p>Nonetheless, there are some limitations to our applications of dissimilarity-based clustering methods in this tutorial which are worth mentioning. Firstly, we note firstly that the decision to standardise the variables —in particular, to normalise them by subtracting their means and dividing by their standard deviations— was applied across the board to each clustering method we explored. Although the standardised data were only used as inputs to each algorithm (i.e., the output was always interpreted on the original scale), we note that different groups are liable to be uncovered with different standardisation schemes. In other words, not standardising the data, or using some other standardisation method (e.g., rescaling to the <span class="math inline">\([0,1]\)</span> range) may lead to different, possibly more or less meaningful clusters.</p>
<p>A second limitation is that all variables in the data were used as inputs (either directly in the case of <span class="math inline">\(K\)</span>-Means, or indirectly as inputs to the pairwise dissimilarity matrix calculations required for <span class="math inline">\(K\)</span>-Medoids and hierarchical clustering). As well as increasing the computational burden, using all variables can be potentially problematic in cases where the clustering structure is driven by some <span class="math inline">\(d^\star&lt; d\)</span> variables, i.e., if the data can be separated into homogeneous subgroups along fewer dimensions. In such instances where some of the variables are uninformative in terms of explaining the variability in the data, variable selection strategies tailored to the unsupervised paradigm may be of interest and again may lead to more meaningful clusters being found <span class="citation" data-cites="Witten2010 Hancer2020">[<a href="#ref-Witten2010" role="doc-biblioref">66</a>, <a href="#ref-Hancer2020" role="doc-biblioref">67</a>]</span>.</p>
<p>Although dissimilarity-based clustering encompasses a broad range of flexible methodologies which can be utilised in other, diverse settings —for example, dissimilarity-based clustering is applied in the context of longitudinal categorical data in the chapter on sequence analysis methods [Chapter 10; <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>]— there are other clustering paradigms which may be of interest in similar or alternative settings as the data used in the present tutorial, even if they are not yet widely adopted in educational research. We now briefly introduce some of these alternative clustering frameworks for readers interested in expanding their knowledge of the topic of clustering beyond the dissimilarity-based framework detailed herein.</p>
<p>A first alternative to dissimilarity-based clustering is the density-based clustering framework, most famously exemplified by the DBSCAN clustering algorithm <span class="citation" data-cites="Ester1996 Hahsler2019">[<a href="#ref-Ester1996" role="doc-biblioref">68</a>, <a href="#ref-Hahsler2019" role="doc-biblioref">69</a>]</span>. Density-based clustering broadly defines clusters as areas of higher density than the remainder of the data set, where objects are closely packed together with many nearby neighbours, while objects in the sparse areas which separate clusters are designated as outliers. This has been identified as one main advantage of DBSCAN by authors who applied it an education research context <span class="citation" data-cites="Du2021">[<a href="#ref-Du2021" role="doc-biblioref">70</a>]</span> —insofar as the capability to easily and effectively separate individual exceptionally poor or exceptionally outstanding students who require special attention from teachers is desirable— along with DBSCAN obviating the need to pre-specify the number of clusters <span class="math inline">\(K\)</span>.</p>
<p>Another alternative is given by the model-based clustering paradigm, which is further discussed in Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>. Although we direct readers to that chapter for a full discussion of model-based clustering using finite Gaussian mixture models, the relationship between this approach and latent profile analysis, and a tutorial in R using the <code>mclust</code> package <span class="citation" data-cites="Scrucca2016">[<a href="#ref-Scrucca2016" role="doc-biblioref">71</a>]</span>, there are some aspects and advantages which are pertinent to discuss here. Firstly, as model-based clustering is based on an underlying generative probability model, rather than relying on dissimilarity-based heuristics, it admits the use of principled, likelihood-based model-selection criteria such as the Bayesian information criterion <span class="citation" data-cites="Schwarz1978">[<a href="#ref-Schwarz1978" role="doc-biblioref">72</a>]</span>, thereby eliminating the subjectivity of elbow plots and other graphical tools for guiding the choice of <span class="math inline">\(K\)</span>. Secondly, such models can be extended to allow covariates to guide the construction of the clusters <span class="citation" data-cites="Murphy2020">[<a href="#ref-Murphy2020" role="doc-biblioref">73</a>]</span>, thereby enabling, for example, incorporation of the categorical demographic variables associated with the MOOC centralities data set used in the present tutorial.</p>
<p>A third advantage of model-based clustering is that it returns a “soft” partition, whereas dissimilarity-based approaches yield either a single “hard” partition (with each observation placed in one group only), under partitional methods like <span class="math inline">\(K\)</span>-Means or <span class="math inline">\(K\)</span>-Medoids, or a set of nested hard partitions from which a single hard partition can be extracted, under hierarchical clustering. Specifically, model-based clustering assigns each observation a probability of belonging to each cluster, such that observations can have a non-negative association with more than one cluster and the uncertainty of the cluster memberships can be quantified. This has the effect of diminishing the effect of outliers or observations which lie on the boundary of two or more natural clusters, as they are not forced to wholly belong to one cluster. In light of these concerns, another clustering paradigm of potential interest is that of fuzzy clustering, which is notable for allowing for “soft” cluster-membership assignments while still being dissimilarity-based <span class="citation" data-cites="Kaufman1990-fanny Durso2016">[<a href="#ref-Kaufman1990-fanny" role="doc-biblioref">74</a>, <a href="#ref-Durso2016" role="doc-biblioref">75</a>]</span>. Indeed, there are fuzzy variants of the <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids algorithms which relax the assumption that the latent variable <span class="math inline">\(\mathbf{z}_i\)</span> encountered in Equation <span class="math inline">\(\eqref{eq:kmeans_objective}\)</span>, for example, is binary. They are implemented in the functions <code>FKM()</code> and <code>FKM.med()</code>, respectively, in the <code>fclust</code> R package <span class="citation" data-cites="fclust2019">[<a href="#ref-fclust2019" role="doc-biblioref">76</a>]</span>.</p>
<p>Another advantage of model-based clustering over the dissimilarity-based paradigm is the flexibility it affords in relation to the shapes, orientations, volumes, and sizes of the clusters it uncovers. At their most flexible, finite Gaussian mixture models can find clusters where all of these characteristics differ between each cluster, but intermediary configurations —whereby, as but one example, clusters can be constrained to have equal volume and orientation but varying shape and size— are permissible. This is achieved via different parsimonious parameterisations of the cluster-specific covariance matrices which control the geometric characteristics of the corresponding ellipsoids; see Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span> for details. By contrast, <span class="math inline">\(K\)</span>-Means is much more restrictive. The algorithm assumes that clusters are of similar size, even if the estimated clusters can vary in size upon convergence. Moreover, by relying on squared Euclidean distances to a mean vector, with no recourse to modelling covariances, <span class="math inline">\(K\)</span>-Means implicitly assumes that all clusters are spherical in shape, have equal volume, and radiate around their centroid. This can have the damaging consequence, in more challenging applications, that a a larger number of spherical clusters may be required to fit the data well, rather than a more parsimonious and easily interpretable mixture model with fewer non-spherical components. Finally, in light of these concerns, we highlight the spectral clustering approach <span class="citation" data-cites="Ng2001">[<a href="#ref-Ng2001" role="doc-biblioref">77</a>]</span>, implemented via the function <code>specc()</code> in the <code>kernlab</code> package in R, which is, like model-based clustering, capable of uncovering clusters with more flexible shapes, particularly when the data are not linearly separable, and shares some connections with kernel <span class="math inline">\(K\)</span>-Means <span class="citation" data-cites="Dhillon2004">[<a href="#ref-Dhillon2004" role="doc-biblioref">78</a>]</span>, another flexible generalisation of the standard <span class="math inline">\(K\)</span>-Means algorithm adopted in this tutorial.</p>
<p>Overall, we encourage readers to further explore the potential of dissimilarity-based clustering methods, bear in mind the limitations and practical concerns of each algorithm discussed in this tutorial, and remain cognisant of the implications thereof for results obtained in educational research applications. We believe that by paying particular attention to the guidelines presented for choosing an optimal partition among multiple competing methodologies, with different numbers of clusters and/or different dissimilarity measures and/or different linkage criteria, more meaningful and interpretable patterns of student behaviour can be found. Finally, we hope that the additional references provided to other clustering frameworks will inspire a broader interest in the topic of cluster analysis among practitioners in the field of education research.</p>


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Everitt2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Everitt BS, Landau S, Leese M, Stahl D (2011) <span>Cluster Analysis</span>, Fifth. John Wiley &amp; Sons, New York, NY, U.S.A.</div>
</div>
<div id="ref-Hennig2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Hennig C (2015) What are the true clusters? Pattern Recognition Letters 64:53–62</div>
</div>
<div id="ref-Hennig2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Hennig C (2016) Clustering strategy and method selection. In: Hennig C, Meila M, Murtagh F, Rocci R (eds) <span class="nocase">Handbook of Cluster Analysis</span>. Chapman; Hall/CRC Press, New York, N.Y., U.S.A., pp 703–730</div>
</div>
<div id="ref-MacQueen1967" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">MacQueen JB (1967) Some methods for classification and analysis of multivariate observations. In: Cam LML, Neyman J (eds) <span class="nocase">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</span>. University of California Press, June 21–July 18, 1965; December 27 1965–January 7, 1966, Statistical Laboratory of the University of California, Berkeley, CA, U.S.A., pp 281–297</div>
</div>
<div id="ref-Kaufman1990-pam" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Partitioning around medoids (program <span>PAM</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 68–125</div>
</div>
<div id="ref-Kaufman1990-agnes" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Agglomerative nesting (program <span>AGNES</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 199–252</div>
</div>
<div id="ref-R2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">R Core Team (2023) <a href="https://www.R-project.org/">R: A language and environment for statistical computing</a>. R Foundation for Statistical Computing, Vienna, Austria</div>
</div>
<div id="ref-Scrucca2024-mbc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Scrucca L, Saqr M, López-Pernas S, Murphy K (2024) An introduction and <span>R</span> tutorial to model-based clustering in education via latent profile analysis. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Bouveyron2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Bouveyron C, Celeux G, Murphy TB, Raftery AE (2019) <span class="nocase">Model-Based Clustering and Classification for Data Science: With Applications in R</span>. Cambridge University Press, Cambridge, UK</div>
</div>
<div id="ref-Rennenallhoff1983" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Rennen-Allhoff B, Allhoff P (1983) Clusteranalysen bei psychologisch-p<span>ä</span>dagogischen <span>F</span>ragestellungen. Psychologie in Erziehung und Unterricht 30:253–261</div>
</div>
<div id="ref-Hickendorff2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Hickendorff M, Edelsbrunner PA, McMullen J, Schneider M, Trezise K (2018) Informative tools for characterizing individual differences in learning: Latent class, latent profile, and latent transition analysis. Learning and Individual Differences 66:4–15</div>
</div>
<div id="ref-Saqr2021a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2021) The longitudinal trajectories of online engagement over a full program. Computers &amp; Education 175:104325</div>
</div>
<div id="ref-Cook2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Cook CR, Kilgus SP, Burns MK (2018) Advancing the science and practice of precision education to enhance student outcomes. Journal of School Psychology 66:4–10</div>
</div>
<div id="ref-Howard2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Howard MC, Hoffman ME (2018) Variable-centered, person-centered, and person-specific approaches: Where theory meets the method. Organizational Research Methods 21:846–876</div>
</div>
<div id="ref-Richters2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Richters JE (2021) Incredible utility: The lost causes and causal debris of psychological science. Basic and Applied Social Psychology 43:366–405</div>
</div>
<div id="ref-Saqr2023a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2023) The temporal dynamics of online problem-based learning: Why and when sequence matters. International Journal of Computer-Supported Collaborative Learning 18:11–37</div>
</div>
<div id="ref-Dutt2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Dutt A (2015) Clustering algorithms applied in educational data mining. International Journal of Information and Electronics Engineering 5:112–116</div>
</div>
<div id="ref-Saqr2024-tv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Helske S, Durand M, Murphy K, Studer M, Ritschard G (2024) Sequence analysis: Basic principles, technique, and tutorial. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Beder1990" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Beder HW, Valentine T (1990) Motivational profiles of adult basic education students. Adult Education Quarterly 40:78–94</div>
</div>
<div id="ref-Clement1994" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Clément R, Dörnyei Z, Noels KA (1994) Motivation, self‐confidence, and group cohesion in the foreign language classroom. Language Learning 44:417–448</div>
</div>
<div id="ref-Fernandez-Rio2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Fernandez-Rio J, Méndez-Giménez A, Cecchini Estrada JA (2014) A cluster analysis on students’ perceived motivational climate. Implications on psycho-social variables. The Spanish Journal of Psychology 17:E18</div>
</div>
<div id="ref-Cahapin2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Cahapin EL, Malabag BA, Santiago J Cereneo Sailog, Reyes JL, Legaspi GS, Adrales KL (2023) Clustering of students admission data using <span class="math inline">\(K\)</span>-means, hierarchical, and <span>DBSCAN</span> algorithms. Bulletin of Electrical Engineering and Informatics 12:3647–3656</div>
</div>
<div id="ref-Saqr2022a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Saqr M, Tuominen V, Valtonen T, Sointu E, Väisänen S, Hirsto L (2022) Teachers’ learning profiles in learning programming: The big picture! Frontiers in Education 7:1–10</div>
</div>
<div id="ref-Jovanovic2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Jovanović J, Gašević D, Dawson S, Pardo A, Mirriahi N (2017) Learning analytics to unveil learning strategies in a flipped classroom. The Internet and Higher Education 33:74–85</div>
</div>
<div id="ref-Lopez-Pernas2021a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">López-Pernas S, Saqr M (2021) Bringing synchrony and clarity to complex multi-channel data: A learning analytics study in programming education. IEEE Access 9:166531–166541</div>
</div>
<div id="ref-Lopez-Pernas2021b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Viberg O (2021) Putting it all together: Combining learning analytics methods and data sources to understand students’ approaches to learning programming. Sustainability 13:4825</div>
</div>
<div id="ref-Fan2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Fan Y, Tan Y, Raković M, Wang Y, Cai Z, Shaffer DW, Gašević D (2022) Dissecting learning tactics in <span>MOOC</span> using ordered network analysis. Journal of Computer Assisted Learning 39:154–166</div>
</div>
<div id="ref-Saqr2021b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2021) Modelling diffusion in computer-supported collaborative learning: A large scale learning analytics study. International Journal of Computer-Supported Collaborative Learning 16:441–483</div>
</div>
<div id="ref-Perera2009" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Perera D, Kay J, Koprinska I, Yacef K, Zaïane OR (2009) Clustering and sequential pattern mining of online collaborative learning data. IEEE Transactions on Knowledge and Data Engineering 21:759–772</div>
</div>
<div id="ref-Saqr2023b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Jovanović J, Gašević D (2023) Intense, turbulent, or wallowing in the mire: A longitudinal study of cross-course online tactics, strategies, and trajectories. The Internet and Higher Education 57:100902</div>
</div>
<div id="ref-Roque2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Vieira Roque F, Cechinel C, Merino E, Villarroel R, Lemos R, Munoz R (2018) Using multimodal data to find patterns in student presentations. In: <span class="nocase">2018 XIII Latin American Conference on Learning Technologies (LACLO)</span>. São Paulo, Brazil, pp 256–263</div>
</div>
<div id="ref-Lee2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Lee J-E, Chan JY-C, Botelho A, Ottmar E (2022) Does slow and steady win the race?: Clustering patterns of students’ behaviors in an interactive online mathematics game. Educational Technology Research and Development 70:1575–1599</div>
</div>
<div id="ref-Lopez-Pernas2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Gordillo A, Barra E (2022) A learning analytics perspective on educational escape rooms. Interactive Learning Environments 1–17</div>
</div>
<div id="ref-Rosa2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Rosa PJ, Morais D, Gamito P, Oliveira J, Saraiva T (2016) The immersive virtual reality experience: A typology of users revealed through multiple correspondence analysis combined with cluster analysis technique. Cyberpsychology, Behavior and Social Networking 19:209–216</div>
</div>
<div id="ref-Wang2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Wang X, Liu Q, Pang H, Tan SC, Lei J, Wallace MP, Li L (2023) What matters in <span class="nocase">AI-supported</span> learning: A study of <span class="nocase">human-AI</span> interactions in language learning using cluster analysis and epistemic network analysis. Computers &amp; Education 194:104703</div>
</div>
<div id="ref-cluster2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Maechler M, Rousseeuw P, Struyf A, Hubert M, Hornik K (2022) <a href="\url{https://CRAN.R-project.org/package=cluster}"><span class="nocase">cluster: cluster analysis basics and extensions</span></a></div>
</div>
<div id="ref-Lloyd1982" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Lloyd SP (1982) Least squares quantization in <span>PCM</span>. IEEE Transactions on Information Theory 28:129–137</div>
</div>
<div id="ref-Forgy1965" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline">Forgy EW (1965) Cluster analysis of multivariate data: Efficiency vs interpretability of classifications. Biometrics 21:768–769</div>
</div>
<div id="ref-Hartigan1979" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline">Hartigan JA, Wong MA (1979) Algorithm <span class="nocase">AS 136: a <span class="math inline">\(K\)</span>-M</span>eans clustering algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics) 28:100–108</div>
</div>
<div id="ref-Arthur2007" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline">Arthur D, Vassilvitskii S (2007) <span class="math inline">\(K\)</span>-means<sup>++</sup>: The advantages of careful seeding. In: <span class="nocase">SODA ’07: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</span>. Society for Industrial; Applied Mathematics, Philadelphia, PA, U.S.A., pp 1027–1035</div>
</div>
<div id="ref-Hamming1950" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline">Hamming RW (1950) <span>E</span>rror detecting and error correcting codes. The Bell System Technical Journal 29:147–160</div>
</div>
<div id="ref-Jaccard1901" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline">Jaccard P (1901) Distribution de la flore alpine dans le bassin des <span>D</span>ranses et dans quelqus r<span>é</span>gions voisines. Bulletin de la Soci<span>é</span>t<span>é</span> Vaudoise des Sciences Naturelles 37:241–272</div>
</div>
<div id="ref-Dice1945" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline">Dice LR (1945) Measures of the amount of ecologic association between species. Ecology 26:397–302</div>
</div>
<div id="ref-Sorensen1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline">Sørensen T (1948) A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on <span>D</span>anish commons. Kongelige Danske Videnskabernes Selskab 5:1–34</div>
</div>
<div id="ref-Gower1971" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline">Gower JC (1971) A general coefficient of similarity and some of its properties. Biometrics 27:857–871</div>
</div>
<div id="ref-Huang1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline">Huang Z (1998) Extensions to the k-means algorithm for clustering large data sets with categorical values. Data Mining and Knowledge Discovery 2:283–304</div>
</div>
<div id="ref-Huang1997" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline">Huang Z (1997) Clustering large data sets with mixed numeric and categorical values. In: Lu H, Motoda H, Luu H (eds) <span class="nocase">KDD: Techniques and Applications</span>. World Scientific, Singapore</div>
</div>
<div id="ref-Schubert2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline">Schubert E, Rousseeuw PJ (2021) Fast and eager <span><span class="math inline">\(K\)</span>-M</span>edoids clustering: <span><span class="math inline">\(\mathcal{O}(K)\)</span></span> runtime improvement of the <span class="nocase">PAM, CLARA, and CLARANS</span> algorithms. Information Systems 101:101804</div>
</div>
<div id="ref-Kaufman1990-diana" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Divisive analysis (program <span>DIANA</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 253–279</div>
</div>
<div id="ref-Gilpin2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline">Gilpin S, Qian B, Davidson I (2013) Efficient hierarchical clustering of large high dimensional datasets. In: <span class="nocase">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</span>. Association for Computing Machinery, New York, NY, U.S.A., pp 1371–1380</div>
</div>
<div id="ref-Bouguettaya2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline">Bouguettaya A, Yu Q, Liu X, Zhou X, Song A (2015) Efficient agglomerative hierarchical clustering. Expert Systems with Applications 42:2785–2797</div>
</div>
<div id="ref-Ward1963" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline">Ward, Jr. JH (1963) Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association 58:236–244</div>
</div>
<div id="ref-Murtagh2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline">Murtagh F, Legendre P (2014) Ward’s hierarchical agglomerative clustering method: Which algorithms implement <span>W</span>ard’s criterion? Journal of Classification 31:274–295</div>
</div>
<div id="ref-Rand1971" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline">Rand WM (1971) Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association 66:846–850</div>
</div>
<div id="ref-Hubert1985" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline">Hubert L, Arabie P (1985) Comparing partitions. Journal of Classification 2:193–218</div>
</div>
<div id="ref-Rousseeuw1987" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline">Rousseeuw PJ (1987) Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Computational and Applied Mathematics 20:53–65</div>
</div>
<div id="ref-tidyverse2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline">Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019) Welcome to the <span class="nocase">tidyverse</span>. Journal of Open Source Software 4:1686</div>
</div>
<div id="ref-rio2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline">Chan C, Leeper TJ, Becker J, Schoch D (2023) <a href="\url{https://cran.r-project.org/package=rio}"><span class="nocase">rio: a Swiss-army knife for data file </span></a></div>
</div>
<div id="ref-Lopez-Pernas2024-dat" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Conde J, Del-Río-Carazo L (2024) A broad collection of datasets for educational research training and application. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Saqr2024-sna" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Conde M Ángel, Hernández-García Ángel (2024) Social betwork analysis: A primer, a guide and a tutorial in <span>R</span>. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Saqr2022b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2022) How CSCL roles emerge, persist, transition, and evolve over time: A four-year longitudinal study. Computers &amp; Education 189:104581</div>
</div>
<div id="ref-Pison1999" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline">Pison G, Struyf A, Rousseeuq PJ (1999) Displaying a clustering with <span>CLUSPLOT</span>. Computational Statistics and Data Analysis 30:381–392</div>
</div>
<div id="ref-Mead1992" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline">Mead A (1992) Review of the development of multidimensional scaling methods. Journal of the Royal Statistical Society: Series D (The Statistician) 41:27–39</div>
</div>
<div id="ref-KIM201962" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline">Kim MK, Ketenci T (2019) Learner participation profiles in an asynchronous online collaboration context. The Internet and Higher Education 41:62–76</div>
</div>
<div id="ref-Saqr2020-vr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline">Saqr M, Viberg O (2020) Using diffusion network analytics to examine and support knowledge construction in <span>CSCL</span> settings. In: Alario-Hoyos C, Rodríguez-Triana MJ, Scheffel M, Arnedillo-Sánchez I, Dennerlein SM (eds) <span class="nocase">Addressing Global Challenges and Quality Education: Proceedings of the 15th European Conference on Technology Enhanced Learning, <span>EC-TEL 2020</span>, September 14–18, 2020</span>. Springer, Cham, Switzerland, pp 158–172</div>
</div>
<div id="ref-Witten2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline">Witten DM, Tibshirani R (2010) A framework for feature selection in clustering. Journal of the American Statistical Association 105:713–726</div>
</div>
<div id="ref-Hancer2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline">Hancer E, Xue B, Zhang M (2020) A survey on feature selection approaches for clustering. Artificial Intelligence Review 53:4519–4545</div>
</div>
<div id="ref-Ester1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline">Ester M, Kriegel H-P, Sander J, Xu X (1996) A density-based algorithm for discovering clusters in large spatial databases with noise. In: Simoudis E, Han Jiawei, Fayyad UM (eds) <span class="nocase">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</span>. AAAI Press, Portland, OR, U.S.A., pp 226–231</div>
</div>
<div id="ref-Hahsler2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline">Hahsler M, Piekenbrock M, Doran D (2019) <span class="nocase">dbscan</span>: Fast density-based clustering with <span>R</span>. Journal of Statistical Software 91:1–30</div>
</div>
<div id="ref-Du2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline">Du H, Chen S, Niu H, Li Y (2021) Application of <span>DBSCAN</span> clustering algorithm in evaluating students’ learning status. In: <span class="nocase">Proceedings of the 17th International Conference on Computational Intelligence and Security, November 19–22, 2021</span>. Chengdu, China, pp 372–376</div>
</div>
<div id="ref-Scrucca2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline">Scrucca L, Fop M, Murphy TB, Raftery AE (2016) <span class="nocase">mclust</span> 5: Clustering, classification and density estimation using <span>G</span>aussian finite mixture models. The <span>R</span> Journal 8:289–317</div>
</div>
<div id="ref-Schwarz1978" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline">Schwarz GE (1978) Estimating the dimension of a model. The Annals of Statistics 6:461–464</div>
</div>
<div id="ref-Murphy2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline">Murphy K, Murphy TB (2020) Gaussian parsimonious clustering models with covariates and a noise component. Advances in Data Analysis and Classification 14:293–325</div>
</div>
<div id="ref-Kaufman1990-fanny" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Fuzzy analysis (program <span>FANNY</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 164–198</div>
</div>
<div id="ref-Durso2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline">D’Urso P (2016) Fuzzy clustering. In: Hennig C, Meila M, Murtagh F, Rocci R (eds) <span class="nocase">Handbook of Cluster Analysis</span>. Chapman; Hall/CRC Press, New York, NY, U.S.A., pp 245–575</div>
</div>
<div id="ref-fclust2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline">Ferraro MB, Giordani P, Serafini A (2019) <span class="nocase">fclust</span>: An <span>R</span> package for fuzzy clustering. The <span>R</span> Journal 11:198–210</div>
</div>
<div id="ref-Ng2001" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline">Ng AY, Jordan MI, Weiss Y (2001) On spectral clustering: Analysis and an algorithm. In: Dietterich T, Becker S, Ghahramani Z (eds) <span class="nocase">Advances in Neural Information Processing Systems</span>. MIT Press, Cambridge, MA, U.S.A., pp 849–856</div>
</div>
<div id="ref-Dhillon2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline">Dhillon IS, Guan Y, Kulis B (2004) Kernel <span><span class="math inline">\(K\)</span>-M</span>eans: Spectral clustering and normalized cuts. In: <span class="nocase">KDD ’04: Proceedings of the Tenth ACM SIGKDD International Conference of Knowledge Discovery and Data Mining, Seattle, WA, U.S.A.</span> Association for Computing Machinery, New York, NY, U.S.A., pp 551–556</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/ch07-prediction/ch7-pred.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Predictive modeling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/ch09-model-based-clustering/ch9-model.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model-based clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
  </div>
</footer>
<script>
  document.querySelector(".quarto-title").innerHTML =  '<div class="badge bs-warning bg-warning text-dark" style="float:right;">Pre-print</div>' +  document.querySelector(".quarto-title").innerHTML
  var keywords = document.querySelector('meta[name="keywords"]')
  if (keywords && keywords.content) {
    document.getElementById("title-block-header").innerHTML = document.getElementById("title-block-header").innerHTML + 
      '<div class="abstract"><div class="abstract-title">Keywords</div><div class="quarto-title-meta-contents"><p>'+
      keywords.content +
      '</p></div></div>'
  }
</script>



</body></html>