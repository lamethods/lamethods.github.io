<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Keefe Murphy">
<meta name="author" content="Sonsoles López-Pernas">
<meta name="author" content="Mohammed Saqr">
<meta name="keywords" content="agglomerative hierarchical clustering, average silhouette width, dissimilarity-based clustering, K-Means, K-Medoids, leaning analytics">

<title>Learning analytics methods and tutorials - 8&nbsp; Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/ch09-model-based-clustering/ch9-model.html" rel="next">
<link href="../../chapters/ch07-prediction/ch7-pred.html" rel="prev">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y4VBV3J9WD"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y4VBV3J9WD', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  

<link href="../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Learning analytics methods and tutorials - 8&nbsp; Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R">
<meta name="twitter:description" content="Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data.">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="[8]{.chapter-number}&nbsp; [Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R]{.chapter-title}">
<meta name="citation_abstract" content="Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data. Interest lies in partitioning heterogeneous data into homogeneous groups, whereby cases within a group are more similar to each other than cases assigned to other groups, without foreknowledge of the group labels. Clustering is also an important component of several exploratory methods, analytical techniques, and modelling approaches and therefore has been practiced for decades in education research. In this context, finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students ---and their learning processes--- and tailor their supports to different needs. This chapter introduces the theory underpinning dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, $K$-Means, $K$-Medoids, and agglomerative hierarchical clustering. The $K$-Means clustering algorithm is described including the outline of the arguments of the relevant R functions and the main limitations and practical concerns to be aware of in order to obtain the best performance. We also discuss the related $K$-Medoids algorithm and its own associated concerns and function arguments. We later introduce agglomerative hierarchical clustering and the related R functions while outlining various choices available to practitioners and their implications. Methods for choosing the optimal number of clusters are provided, especially criteria that can guide the choice of clustering solution among multiple competing methodologies ---with a particular focus on evaluating solutions obtained using different dissimilarity measures--- and not only the choice of the number of clusters $K$ for a given method. All of these issues are demonstrated in detail with a tutorial in R using a real-life educational data set.">
<meta name="citation_keywords" content="agglomerative hierarchical clustering, average silhouette width, dissimilarity-based clustering, K-Means, K-Medoids, leaning analytics">
<meta name="citation_author" content="Keefe Murphy">
<meta name="citation_author" content="Sonsoles López-Pernas">
<meta name="citation_author" content="Mohammed Saqr">
<meta name="citation_fulltext_html_url" content="https://lamethods.github.io/ch8-clus.html">
<meta name="citation_doi" content="10.1007/978-3-031-54464-4_8">
<meta name="citation_language" content="en">
<meta name="citation_firstpage" content="231">
<meta name="citation_lastpage" content="283">
<meta name="citation_reference" content="citation_title=K-means&amp;amp;amp;lt;sup&amp;gt;++&amp;lt;/sup&amp;gt;: The advantages of careful seeding;,citation_author=David Arthur;,citation_author=Sergei Vassilvitskii;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=SODA ’07: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms;,citation_conference=Society for Industrial; Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Efficient agglomerative hierarchical clustering;,citation_author=Athman Bouguettaya;,citation_author=Qi Yu;,citation_author=Xumin Liu;,citation_author=Xiangmin Zhou;,citation_author=Andy Song;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=5;,citation_volume=42;,citation_journal_title=Expert Systems with Applications;">
<meta name="citation_reference" content="citation_title=Model-Based Clustering and Classification for Data Science: With Applications in R;,citation_author=Charles Bouveyron;,citation_author=Gilles Celeux;,citation_author=T. Brendan Murphy;,citation_author=Adrian E. Raftery;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=50;,citation_series_title=Cambridge series in statistical and probabilistic mathematics;">
<meta name="citation_reference" content="citation_title=cluster: cluster analysis basics and extensions;,citation_author=Martin Maechler;,citation_author=Peter Rousseeuw;,citation_author=Anja Struyf;,citation_author=Mia Hubert;,citation_author=Kurt Hornik;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=\url{https://CRAN.R-project.org/package=cluster};">
<meta name="citation_reference" content="citation_title=Kernel K-Means: Spectral clustering and normalized cuts;,citation_author=I. S. Dhillon;,citation_author=Y. Guan;,citation_author=B. Kulis;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_inbook_title=KDD ’04: Proceedings of the Tenth ACM SIGKDD International Conference of Knowledge Discovery and Data Mining, Seattle, WA, U.S.A.;">
<meta name="citation_reference" content="citation_title=Measures of the amount of ecologic association between species;,citation_author=L. R Dice;,citation_publication_date=1945;,citation_cover_date=1945;,citation_year=1945;,citation_issue=3;,citation_volume=26;,citation_journal_title=Ecology;">
<meta name="citation_reference" content="citation_title=Application of DBSCAN clustering algorithm in evaluating students’ learning status;,citation_author=H. Du;,citation_author=S. Chen;,citation_author=H. Niu;,citation_author=Y. Li;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Proceedings of the 17th International Conference on Computational Intelligence and Security, November 19–22, 2021;,citation_series_title=CIS 2021;">
<meta name="citation_reference" content="citation_title=Fuzzy clustering;,citation_author=P. D’Urso;,citation_editor=C. Hennig;,citation_editor=M. Meila;,citation_editor=F. Murtagh;,citation_editor=R. Rocci;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_inbook_title=Handbook of Cluster Analysis;,citation_series_title=Handbooks of modern statistical methods;">
<meta name="citation_reference" content="citation_title=A density-based algorithm for discovering clusters in large spatial databases with noise;,citation_author=Martin Ester;,citation_author=Hans-Peter Kriegel;,citation_author=Jörg Sander;,citation_author=Xiaowei Xu;,citation_editor=Evangelos Simoudis;,citation_editor=undefined Han;,citation_editor=Usama M. Fayyad;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=Proceedings of the Second International Conference on Knowledge Discovery and Data Mining;,citation_conference=AAAI Press;,citation_series_title=KDD’96;">
<meta name="citation_reference" content="citation_title=Cluster Analysis;,citation_author=B. S. Everitt;,citation_author=S. Landau;,citation_author=M. Leese;,citation_author=D. Stahl;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=848;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=fclust: An R package for fuzzy clustering;,citation_author=M. B. Ferraro;,citation_author=P. Giordani;,citation_author=A. Serafini;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=11;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=Cluster analysis of multivariate data: Efficiency vs interpretability of classifications;,citation_author=E. W. Forgy;,citation_publication_date=1965;,citation_cover_date=1965;,citation_year=1965;,citation_volume=21;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=Efficient hierarchical clustering of large high dimensional datasets;,citation_author=Sean Gilpin;,citation_author=Buyue Qian;,citation_author=Ian Davidson;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 22nd ACM International Conference on Information &amp;amp;amp; Knowledge Management;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’13;">
<meta name="citation_reference" content="citation_title=A general coefficient of similarity and some of its properties;,citation_author=J. C. Gower;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_issue=4;,citation_volume=27;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=dbscan: Fast density-based clustering with R;,citation_author=M. Hahsler;,citation_author=M. Piekenbrock;,citation_author=D. Doran;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=1;,citation_volume=91;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=Error detecting and error correcting codes;,citation_author=R. W. Hamming;,citation_publication_date=1950;,citation_cover_date=1950;,citation_year=1950;,citation_issue=2;,citation_volume=29;,citation_journal_title=The Bell System Technical Journal;">
<meta name="citation_reference" content="citation_title=A survey on feature selection approaches for clustering;,citation_author=Emrah Hancer;,citation_author=Bing Xue;,citation_author=Mengjie Zhang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=53;,citation_journal_title=Artificial Intelligence Review;">
<meta name="citation_reference" content="citation_title=Algorithm AS 136: a K-Means clustering algorithm;,citation_author=J. A. Hartigan;,citation_author=M. A. Wong;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_volume=28;,citation_journal_title=Journal of the Royal Statistical Society: Series C (Applied Statistics);">
<meta name="citation_reference" content="citation_title=What are the true clusters?;,citation_author=C. Hennig;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_volume=64;,citation_journal_title=Pattern Recognition Letters;">
<meta name="citation_reference" content="citation_title=Clustering strategy and method selection;,citation_author=C. Hennig;,citation_editor=C. Hennig;,citation_editor=M. Meila;,citation_editor=F. Murtagh;,citation_editor=R. Rocci;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_inbook_title=Handbook of Cluster Analysis;,citation_series_title=Handbooks of modern statistical methods;">
<meta name="citation_reference" content="citation_title=Clustering large data sets with mixed numeric and categorical values;,citation_author=Z. Huang;,citation_editor=H. Lu;,citation_editor=H. Motoda;,citation_editor=H. Luu;,citation_publication_date=1997;,citation_cover_date=1997;,citation_year=1997;,citation_inbook_title=KDD: Techniques and Applications;">
<meta name="citation_reference" content="citation_title=Extensions to the k-means algorithm for clustering large data sets with categorical values;,citation_author=Z. Huang;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=3;,citation_volume=2;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Comparing partitions;,citation_author=L. Hubert;,citation_author=P. Arabie;,citation_publication_date=1985;,citation_cover_date=1985;,citation_year=1985;,citation_issue=1;,citation_issn=1432-1343;,citation_volume=2;,citation_journal_title=Journal of Classification;">
<meta name="citation_reference" content="citation_title=Distribution de la flore alpine dans le bassin des Dranses et dans quelqus régions voisines;,citation_author=P. Jaccard;,citation_publication_date=1901;,citation_cover_date=1901;,citation_year=1901;,citation_volume=37;,citation_journal_title=Bulletin de la Société Vaudoise des Sciences Naturelles;">
<meta name="citation_reference" content="citation_title=Partitioning around medoids (program PAM);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Agglomerative nesting (program AGNES);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Divisive analysis (program DIANA);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Fuzzy analysis (program FANNY);,citation_author=L. Kaufman;,citation_author=P. J. Rousseeuw;,citation_editor=L. Kaufman;,citation_editor=P. J. Rousseeuw;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_volume=344;,citation_inbook_title=Finding Groups in Data: An Introduction to Cluster Analysis;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Least squares quantization in PCM;,citation_author=S. P. Lloyd;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;,citation_issue=2;,citation_volume=28;,citation_journal_title=IEEE Transactions on Information Theory;">
<meta name="citation_reference" content="citation_title=A broad collection of datasets for educational research training and application;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Javier Conde;,citation_author=Laura Del-Río-Carazo;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Some methods for classification and analysis of multivariate observations;,citation_author=J. B. MacQueen;,citation_editor=L. M. Le Cam;,citation_editor=J. Neyman;,citation_publication_date=1967;,citation_cover_date=1967;,citation_year=1967;,citation_volume=1;,citation_conference_title=Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability;,citation_conference=University of California Press;">
<meta name="citation_reference" content="citation_title=Review of the development of multidimensional scaling methods;,citation_author=A. Mead;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=1;,citation_volume=41;,citation_journal_title=Journal of the Royal Statistical Society: Series D (The Statistician);">
<meta name="citation_reference" content="citation_title=Gaussian parsimonious clustering models with covariates and a noise component;,citation_author=Keefe Murphy;,citation_author=Thomas Brendan Murphy;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=14;,citation_journal_title=Advances in Data Analysis and Classification;">
<meta name="citation_reference" content="citation_title=Ward’s hierarchical agglomerative clustering method: Which algorithms implement Ward’s criterion?;,citation_author=F. Murtagh;,citation_author=P. Legendre;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_volume=31;,citation_journal_title=Journal of Classification;">
<meta name="citation_reference" content="citation_title=On spectral clustering: Analysis and an algorithm;,citation_author=A. Y. Ng;,citation_author=M. I. Jordan;,citation_author=Y. Weiss;,citation_editor=T. Dietterich;,citation_editor=S. Becker;,citation_editor=Z. Ghahramani;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_volume=14;,citation_conference_title=Advances in Neural Information Processing Systems;,citation_conference=MIT Press;">
<meta name="citation_reference" content="citation_title=Displaying a clustering with CLUSPLOT;,citation_author=G. Pison;,citation_author=A. Struyf;,citation_author=P. J. Rousseeuq;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_volume=30;,citation_journal_title=Computational Statistics and Data Analysis;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Objective criteria for the evaluation of clustering methods;,citation_author=William M. Rand;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_issue=336;,citation_volume=66;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=rio: a Swiss-army knife for data file ;,citation_author=Chung-hong Chan;,citation_author=Thomas J. Leeper;,citation_author=Jason Becker;,citation_author=David Schoch;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=\url{https://cran.r-project.org/package=rio};">
<meta name="citation_reference" content="citation_title=Silhouettes: A graphical aid to the interpretation and validation of cluster analysis;,citation_author=P. J. Rousseeuw;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_volume=20;,citation_journal_title=Computational and Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Sequence analysis: Basic principles, technique, and tutorial;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Satu Helske;,citation_author=Marion Durand;,citation_author=Keefe Murphy;,citation_author=Matthias Studer;,citation_author=Gilbert Ritschard;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Social betwork analysis: A primer, a guide and a tutorial in R;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Miguel Conde;,citation_author=undefined Hernández-García;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=Fast and eager K-Medoids clustering: \mathcal{O}(K) runtime improvement of the PAM, CLARA, and CLARANS algorithms;,citation_author=E. Schubert;,citation_author=P. J. Rousseeuw;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=101;,citation_journal_title=Information Systems;">
<meta name="citation_reference" content="citation_title=Estimating the dimension of a model;,citation_author=Gideon E Schwarz;,citation_publication_date=1978;,citation_cover_date=1978;,citation_year=1978;,citation_issue=2;,citation_volume=6;,citation_journal_title=The Annals of Statistics;,citation_publisher=The Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=An introduction and R tutorial to model-based clustering in education via latent profile analysis;,citation_author=Luca Scrucca;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Keefe Murphy;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=Learning Analytics Methods and Tutorials: A Practical Guide using R;">
<meta name="citation_reference" content="citation_title=mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models;,citation_author=Luca Scrucca;,citation_author=Michael Fop;,citation_author=T. Brendan Murphy;,citation_author=Adrian E. Raftery;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=8;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons;,citation_author=T. Sørensen;,citation_publication_date=1948;,citation_cover_date=1948;,citation_year=1948;,citation_issue=4;,citation_volume=5;,citation_journal_title=Kongelige Danske Videnskabernes Selskab;">
<meta name="citation_reference" content="citation_title=Welcome to the tidyverse;,citation_author=Hadley Wickham;,citation_author=Mara Averick;,citation_author=Jennifer Bryan;,citation_author=Winston Chang;,citation_author=Lucy D’Agostino McGowan;,citation_author=Romain François;,citation_author=Garrett Grolemund;,citation_author=Alex Hayes;,citation_author=Lionel Henry;,citation_author=Jim Hester;,citation_author=Max Kuhn;,citation_author=Thomas Lin Pedersen;,citation_author=Evan Miller;,citation_author=Stephan Milton Bache;,citation_author=Kirill Müller;,citation_author=Jeroen Ooms;,citation_author=David Robinson;,citation_author=Dana Paige Seidel;,citation_author=Vitalie Spinu;,citation_author=Kohske Takahashi;,citation_author=Davis Vaughan;,citation_author=Claus Wilke;,citation_author=Kara Woo;,citation_author=Hiroaki Yutani;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=43;,citation_volume=4;,citation_journal_title=Journal of Open Source Software;">
<meta name="citation_reference" content="citation_title=Hierarchical grouping to optimize an objective function;,citation_author=J. H. Ward, Jr.;,citation_publication_date=1963;,citation_cover_date=1963;,citation_year=1963;,citation_issue=301;,citation_volume=58;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=A framework for feature selection in clustering;,citation_author=Daniela M. Witten;,citation_author=Robert Tibshirani;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=490;,citation_volume=105;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=Motivational profiles of adult basic education students;,citation_author=Hal W. Beder;,citation_author=Thomas Valentine;,citation_publication_date=1990;,citation_cover_date=1990;,citation_year=1990;,citation_issue=2;,citation_volume=40;,citation_journal_title=Adult Education Quarterly;">
<meta name="citation_reference" content="citation_title=Clustering of students admission data using K-means, hierarchical, and DBSCAN algorithms;,citation_author=Erwin Lanceta Cahapin;,citation_author=Beverly Ambagan Malabag;,citation_author=Jr Santiago;,citation_author=Jocelyn L. Reyes;,citation_author=Gemma S. Legaspi;,citation_author=Karl Louise Adrales;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_volume=12;,citation_journal_title=Bulletin of Electrical Engineering and Informatics;">
<meta name="citation_reference" content="citation_title=Motivation, self‐confidence, and group cohesion in the foreign language classroom;,citation_author=Richard Clément;,citation_author=Zoltán Dörnyei;,citation_author=Kimberly A. Noels;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=3;,citation_volume=44;,citation_journal_title=Language Learning;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=Advancing the science and practice of precision education to enhance student outcomes;,citation_author=Clayton R. Cook;,citation_author=Stephen P. Kilgus;,citation_author=Matthew K. Burns;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=December 2017;,citation_volume=66;,citation_journal_title=Journal of School Psychology;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Clustering algorithms applied in educational data mining;,citation_author=Ashish Dutt;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=2;,citation_volume=5;,citation_journal_title=International Journal of Information and Electronics Engineering;">
<meta name="citation_reference" content="citation_title=Dissecting learning tactics in MOOC using ordered network analysis;,citation_author=Yizhou Fan;,citation_author=Yuanru Tan;,citation_author=Mladen Raković;,citation_author=Yeyu Wang;,citation_author=Zhiqiang Cai;,citation_author=David Williamson Shaffer;,citation_author=Dragan Gašević;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=1;,citation_volume=39;,citation_journal_title=Journal of Computer Assisted Learning;">
<meta name="citation_reference" content="citation_title=A cluster analysis on students’ perceived motivational climate. Implications on psycho-social variables;,citation_author=Javier Fernandez-Rio;,citation_author=Antonio Méndez-Giménez;,citation_author=Jose A. Cecchini Estrada;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_volume=17;,citation_journal_title=The Spanish Journal of Psychology;">
<meta name="citation_reference" content="citation_title=Informative tools for characterizing individual differences in learning: Latent class, latent profile, and latent transition analysis;,citation_author=Marian Hickendorff;,citation_author=Peter A. Edelsbrunner;,citation_author=Jake McMullen;,citation_author=Michael Schneider;,citation_author=Kelly Trezise;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=66;,citation_journal_title=Learning and Individual Differences;">
<meta name="citation_reference" content="citation_title=Variable-centered, person-centered, and person-specific approaches: Where theory meets the method;,citation_author=Matt C. Howard;,citation_author=Michael E. Hoffman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=21;,citation_journal_title=Organizational Research Methods;">
<meta name="citation_reference" content="citation_title=Learning analytics to unveil learning strategies in a flipped classroom;,citation_author=Jelena Jovanović;,citation_author=Dragan Gašević;,citation_author=Shane Dawson;,citation_author=Abelardo Pardo;,citation_author=Negin Mirriahi;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=33;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Does slow and steady win the race?: Clustering patterns of students’ behaviors in an interactive online mathematics game;,citation_author=Ji-Eun Lee;,citation_author=Jenny Yun-Chen Chan;,citation_author=Anthony Botelho;,citation_author=Erin Ottmar;,citation_publication_date=2022-10;,citation_cover_date=2022-10;,citation_year=2022;,citation_issue=5;,citation_volume=70;,citation_journal_title=Educational Technology Research and Development;">
<meta name="citation_reference" content="citation_title=Bringing synchrony and clarity to complex multi-channel data: A learning analytics study in programming education;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=9;,citation_journal_title=IEEE Access;">
<meta name="citation_reference" content="citation_title=A learning analytics perspective on educational escape rooms;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Aldo Gordillo;,citation_author=Enrique Barra;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=Interactive Learning Environments;">
<meta name="citation_reference" content="citation_title=Putting it all together: Combining learning analytics methods and data sources to understand students’ approaches to learning programming;,citation_author=Sonsoles López-Pernas;,citation_author=Mohammed Saqr;,citation_author=Olga Viberg;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=9;,citation_volume=13;,citation_journal_title=Sustainability;">
<meta name="citation_reference" content="citation_title=Clustering and sequential pattern mining of online collaborative learning data;,citation_author=Dilhan Perera;,citation_author=Judy Kay;,citation_author=Irena Koprinska;,citation_author=Kalina Yacef;,citation_author=Osmar R. Zaïane;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=6;,citation_volume=21;,citation_journal_title=IEEE Transactions on Knowledge and Data Engineering;">
<meta name="citation_reference" content="citation_title=Clusteranalysen bei psychologisch-pädagogischen Fragestellungen;,citation_author=B. Rennen-Allhoff;,citation_author=P. Allhoff;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=4;,citation_volume=30;,citation_journal_title=Psychologie in Erziehung und Unterricht;">
<meta name="citation_reference" content="citation_title=Incredible utility: The lost causes and causal debris of psychological science;,citation_author=John E. Richters;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=6;,citation_volume=43;,citation_journal_title=Basic and Applied Social Psychology;">
<meta name="citation_reference" content="citation_title=Using multimodal data to find patterns in student presentations;,citation_author=Felipe Vieira Roque;,citation_author=Cristian Cechinel;,citation_author=Erick Merino;,citation_author=Rodolfo Villarroel;,citation_author=Robson Lemos;,citation_author=Roberto Munoz;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=2018 XIII Latin American Conference on Learning Technologies (LACLO);">
<meta name="citation_reference" content="citation_title=The immersive virtual reality experience: A typology of users revealed through multiple correspondence analysis combined with cluster analysis technique;,citation_author=Pedro J. Rosa;,citation_author=Diogo Morais;,citation_author=Pedro Gamito;,citation_author=Jorge Oliveira;,citation_author=Tomaz Saraiva;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=3;,citation_volume=19;,citation_journal_title=Cyberpsychology, Behavior and Social Networking;">
<meta name="citation_reference" content="citation_title=The longitudinal trajectories of online engagement over a full program;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=175;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=Modelling diffusion in computer-supported collaborative learning: A large scale learning analytics study;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=4;,citation_volume=16;,citation_journal_title=International Journal of Computer-Supported Collaborative Learning;">
<meta name="citation_reference" content="citation_title=The temporal dynamics of online problem-based learning: Why and when sequence matters;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=1;,citation_volume=18;,citation_journal_title=International Journal of Computer-Supported Collaborative Learning;">
<meta name="citation_reference" content="citation_title=Intense, turbulent, or wallowing in the mire: A longitudinal study of cross-course online tactics, strategies, and trajectories;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Jelena Jovanović;,citation_author=Dragan Gašević;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=57;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Teachers’ learning profiles in learning programming: The big picture!;,citation_author=Mohammed Saqr;,citation_author=Ville Tuominen;,citation_author=Teemu Valtonen;,citation_author=Erkko Sointu;,citation_author=Sanna Väisänen;,citation_author=Laura Hirsto;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=7;,citation_journal_title=Frontiers in Education;">
<meta name="citation_reference" content="citation_title=What matters in AI-supported learning: A study of human-AI interactions in language learning using cluster analysis and epistemic network analysis;,citation_author=Xinghua Wang;,citation_author=Qian Liu;,citation_author=Hui Pang;,citation_author=Seng Chee Tan;,citation_author=Jun Lei;,citation_author=Matthew P Wallace;,citation_author=Linlin Li;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=194;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=How CSCL roles emerge, persist, transition, and evolve over time: A four-year longitudinal study;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issn=0360-1315;,citation_volume=189;,citation_journal_title=Computers &amp;amp;amp; Education;">
<meta name="citation_reference" content="citation_title=Learner participation profiles in an asynchronous online collaboration context;,citation_author=Min Kyu Kim;,citation_author=Tuba Ketenci;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issn=1096-7516;,citation_volume=41;,citation_journal_title=The Internet and Higher Education;">
<meta name="citation_reference" content="citation_title=Using diffusion network analytics to examine and support knowledge construction in CSCL settings;,citation_author=Mohammed Saqr;,citation_author=Olga Viberg;,citation_editor=C. Alario-Hoyos;,citation_editor=M. J. Rodríguez-Triana;,citation_editor=M. Scheffel;,citation_editor=I. Arnedillo-Sánchez;,citation_editor=S. M. Dennerlein;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=12315;,citation_conference_title=Addressing Global Challenges and Quality Education: Proceedings of the 15th European Conference on Technology Enhanced Learning, EC-TEL 2020, September 14–18, 2020;,citation_conference=Springer;,citation_series_title=Lecture notes in computer science;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Learning analytics methods and tutorials</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lamethods/labook-code/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contributors.html" class="sidebar-item-text sidebar-link">Contributors</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../foreword_dg.html" class="sidebar-item-text sidebar-link">Foreword by Dragan Gašević</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../foreword_rb.html" class="sidebar-item-text sidebar-link">Foreword by Ryan Baker</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch01-intro/intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Getting started</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch02-data/ch2-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch03-intro-r/ch3-intor.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Intro to R</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch04-data-cleaning/ch4-datacleaning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data cleaning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch05-basic-stats/ch5-stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Basic statistics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch06-data-visualization/ch6-viz.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data visualization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Machine Learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch07-prediction/ch7-pred.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Predictive modeling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch08-clustering/ch8-clus.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch09-model-based-clustering/ch9-model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model-based clustering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Temporal methods</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch10-sequence-analysis/ch10-seq.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch11-vasstra/ch11-vasstra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">VaSSTra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch12-markov/ch12-markov.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Markov models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch13-multichannel/ch13-multi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Multi-channel sequences</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch14-process-mining/ch14-process.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Process mining</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Network analysis</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch15-sna/ch15-sna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Social Network Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch16-community/ch16-comm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Community detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch17-temporal-networks/ch17-tna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Temporal Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch18-ena-ona/ch18-ena.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Epistemic Network Analysis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Psychometrics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch19-psychological-networks/ch19-psych.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Psychological networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch20-factor-analysis/ch20-factor.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Factor analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch21-sem/ch21-sem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Structured Equation Modeling</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch22-conclusion/ch22-conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Future of LA</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#sec-review" id="toc-sec-review" class="nav-link" data-scroll-target="#sec-review"><span class="toc-section-number">2</span>  Clustering in education: review of the literature</a></li>
  <li><a href="#sec-methods" id="toc-sec-methods" class="nav-link" data-scroll-target="#sec-methods"><span class="toc-section-number">3</span>  Clustering methodology</a>
  <ul class="collapse">
  <li><a href="#sec-kmtheory" id="toc-sec-kmtheory" class="nav-link" data-scroll-target="#sec-kmtheory"><span class="toc-section-number">3.1</span>  <span class="math inline">\(K\)</span>-Means</a></li>
  <li><a href="#sec-hclust" id="toc-sec-hclust" class="nav-link" data-scroll-target="#sec-hclust"><span class="toc-section-number">3.2</span>  Agglomerative hierarchical clustering</a></li>
  <li><a href="#sec-chooseK" id="toc-sec-chooseK" class="nav-link" data-scroll-target="#sec-chooseK"><span class="toc-section-number">3.3</span>  Choosing the number of clusters</a></li>
  </ul></li>
  <li><a href="#sec-tutR" id="toc-sec-tutR" class="nav-link" data-scroll-target="#sec-tutR"><span class="toc-section-number">4</span>  Tutorial with R</a>
  <ul class="collapse">
  <li><a href="#sec-data" id="toc-sec-data" class="nav-link" data-scroll-target="#sec-data"><span class="toc-section-number">4.1</span>  The data set</a></li>
  <li><a href="#sec-apps" id="toc-sec-apps" class="nav-link" data-scroll-target="#sec-apps"><span class="toc-section-number">4.2</span>  Clustering applications</a></li>
  </ul></li>
  <li><a href="#sec-disc" id="toc-sec-disc" class="nav-link" data-scroll-target="#sec-disc"><span class="toc-section-number">5</span>  Discussion &amp; further readings</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<p><a href="https://github.com/lamethods/code" target="_blank"> <button class="btn btn-outline-dark"> <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 496 512" style="width: 22px;vertical-align: text-top;margin-right: 9px;"> <path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z" style="width: 24px;"> </path> </svg>Download code </button> </a> <br> <br> <small>© 2023 The authors</small></p>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Keefe Murphy </p>
             <p>Sonsoles López-Pernas </p>
             <p>Mohammed Saqr </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Clustering is a collective term which refers to a broad range of techniques aimed at uncovering patterns and subgroups within data. Interest lies in partitioning heterogeneous data into homogeneous groups, whereby cases within a group are more similar to each other than cases assigned to other groups, without foreknowledge of the group labels. Clustering is also an important component of several exploratory methods, analytical techniques, and modelling approaches and therefore has been practiced for decades in education research. In this context, finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students —and their learning processes— and tailor their supports to different needs. This chapter introduces the theory underpinning dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-Medoids, and agglomerative hierarchical clustering. The <span class="math inline">\(K\)</span>-Means clustering algorithm is described including the outline of the arguments of the relevant R functions and the main limitations and practical concerns to be aware of in order to obtain the best performance. We also discuss the related <span class="math inline">\(K\)</span>-Medoids algorithm and its own associated concerns and function arguments. We later introduce agglomerative hierarchical clustering and the related R functions while outlining various choices available to practitioners and their implications. Methods for choosing the optimal number of clusters are provided, especially criteria that can guide the choice of clustering solution among multiple competing methodologies —with a particular focus on evaluating solutions obtained using different dissimilarity measures— and not only the choice of the number of clusters <span class="math inline">\(K\)</span> for a given method. All of these issues are demonstrated in detail with a tutorial in R using a real-life educational data set.
  </div>
</div>

</header>

<section id="sec-intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-intro"><span class="header-section-number">1</span> Introduction</h2>
<p>Cluster analysis is a term used to describe a broad range of techniques which have the goal of uncovering groups of observations in a data set. The typical aim of cluster analysis is to categorise observations into groups in such a way that observations within the same group are in some sense more similar to each other, while being relatively dissimilar to those in other groups <span class="citation" data-cites="Everitt2011">[<a href="#ref-Everitt2011" role="doc-biblioref">1</a>]</span>. In other words, clustering methods uncover group structure in heterogeneous populations by identifying more homogeneous groupings of observations which may represent distinct, meaningful subpopulations. Using machine learning terminology, cluster analysis corresponds to unsupervised learning, whereby groups are identified by relying solely on the intrinsic characteristics (typically dissimilarities), without guidance from unavailable ground truth group labels. Indeed, foreknowledge of the fixed number of groups in a data set is characteristic of supervised learning and the distinct field of classification analysis.</p>
<p>It is important to note that there is no universally applicable definition of what constitutes a cluster <span class="citation" data-cites="Hennig2015 Hennig2016">[<a href="#ref-Hennig2015" role="doc-biblioref">2</a>, <a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>. Indeed, in the absence of external information in the form of existing “true” group labels, different clustering methods can reveal different things about the same data. There are many different ways to cluster the same data set, and different methods may yield solutions with different assignments, or even differ in the number of groups they identify. Consequently, we present several cluster analysis algorithms in this chapter; namely, <span class="math inline">\(K\)</span>-Means <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span> in <a href="#sec-kmtheory"><span>Section&nbsp;8.3.1</span></a>, a generalisation thereof, <span class="math inline">\(K\)</span>-Medoids <span class="citation" data-cites="Kaufman1990-pam">[<a href="#ref-Kaufman1990-pam" role="doc-biblioref">5</a>]</span>, in <a href="#sec-kmedoids"><span>Section&nbsp;8.3.1.2.3</span></a>, and agglomerative hierarchical clustering <span class="citation" data-cites="Kaufman1990-agnes">[<a href="#ref-Kaufman1990-agnes" role="doc-biblioref">6</a>]</span> in <a href="#sec-hclust"><span>Section&nbsp;8.3.2</span></a>. We apply each method in a comparative study in our tutorial using R <span class="citation" data-cites="R2023">[<a href="#ref-R2023" role="doc-biblioref">7</a>]</span>, with applications to a data set about the participants from discussion forum of a massive open online course (MOOC) for teachers.</p>
<p>The clustering methods we review in this chapter are designed to find mutually exclusive, non-overlapping groups in a data set, i.e., they recover a hard partition whereby each observation belongs to one group only. This is in contrast to soft clustering approaches under which each observation is assigned a probability of belonging to each group. One such example of soft clustering is the model-based clustering paradigm, which is discussed in the later Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>. While model-based clustering methods, as the name suggests, are underpinned by the assumption of generative probabilistic models <span class="citation" data-cites="Bouveyron2019">[<a href="#ref-Bouveyron2019" role="doc-biblioref">9</a>]</span>, the more traditional dissimilarity-based methods on which this chapter is focused are purely algorithmic in nature and rely on heuristic criteria regarding the pairwise dissimilarities between objects.</p>
<p>Heuristic clustering algorithms can be further divided into two categories; partitional clustering (e.g., <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids) and hierarchical (of which we focus on the agglomerative variant). Broadly speaking, partitional clustering methods start with an initial grouping of observations and iteratively update the clustering until the “best” clustering is found, according to some notion of what defines the “best” clustering. Hierarchical clustering methods, on the other hand, is more sequential in nature; a tree-like structure of nested clusterings is built up via successive mergings of similar observations, according to a defined similarity metric. In our theoretical expositions and our applications in the R tutorial, we provide some guidelines both on how to choose certain method-specific settings to yield the best performance and how to determine the optimal clustering among a set of competing methods.</p>
<p>The remainder of this chapter proceeds as follows. In <a href="#sec-review"><span>Section&nbsp;8.2</span></a>, we review relevant literature in which dissimilarity-based clustering methods have been applied in the realm of educational research. In <a href="#sec-methods"><span>Section&nbsp;8.3</span></a>, we describe the theoretical underpinnings of each method in turn and discuss relevant practical guidelines which should be followed to secure satisfactory performance from each method throughout. Within <a href="#sec-tutR"><span>Section&nbsp;8.4</span></a>, we introduce the data set of our case study in <a href="#sec-data"><span>Section&nbsp;8.4.1</span></a> and give an overview of some required pre-processing steps in <a href="../ch12-markov/ch12-markov.html#sec-process"><span>Section&nbsp;12.4.3</span></a>, and then present a tutorial using R for each clustering method presented in this chapter in <a href="#sec-apps"><span>Section&nbsp;8.4.2</span></a>, with a specific focus on identifying the optimal clustering solution in <a href="#sec-sil"><span>Section&nbsp;8.4.2.4</span></a>. Finally, we conclude with a discussion and some recommendations for related further readings in <a href="#sec-disc"><span>Section&nbsp;8.5</span></a>, with a particular focus on some limitations of dissimilarity-based clustering which are addressed by other frameworks in the broader field of cluster analysis.</p>
</section>
<section id="sec-review" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-review"><span class="header-section-number">2</span> Clustering in education: review of the literature</h2>
<p>In education, clustering is among the oldest and most common analysis methods, predating the field of learning analytics and educational data mining by several decades. Such early adoption of clustering is due to the immense utility of cluster analysis in helping researchers to find patterns within data, which is a major pursuit of education research <span class="citation" data-cites="Rennenallhoff1983">[<a href="#ref-Rennenallhoff1983" role="doc-biblioref">10</a>]</span>. Interest was fueled by the increasing attention to heterogeneity and individual differences among students, their learning processes, and their approaches to learning <span class="citation" data-cites="Hickendorff2018 Saqr2021a">[<a href="#ref-Hickendorff2018" role="doc-biblioref">11</a>, <a href="#ref-Saqr2021a" role="doc-biblioref">12</a>]</span>. Finding such patterns or differences among students allows teachers and researchers to improve their understanding of the diversity of students and tailor their support to different needs <span class="citation" data-cites="Cook2018">[<a href="#ref-Cook2018" role="doc-biblioref">13</a>]</span>. Finding subgroups within cohorts of students is a hallmark of so-called “person-centered methods”, to which clustering belongs <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>.</p>
<p>A person-centered approach stands in contrast to the variable-centered methods which consider that most students belong to a coherent homogeneous group with little or negligible differences <span class="citation" data-cites="Howard2018">[<a href="#ref-Howard2018" role="doc-biblioref">14</a>]</span>. Variable-centered methods assume that there is a common average pattern that represents all students, that the studied phenomenon has a common causal mechanism, and that the phenomenon evolves in the same way and results in similar outcomes amongst all students. These assumptions are largely understood to be unrealistic, “demonstrably false, and invalidated by a substantial body of uncontested scientific evidence” <span class="citation" data-cites="Richters2021">[<a href="#ref-Richters2021" role="doc-biblioref">15</a>]</span>. In fact, several analytical problems necessitate clustering, e.g., where opposing patterns exist <span class="citation" data-cites="Saqr2023a">[<a href="#ref-Saqr2023a" role="doc-biblioref">16</a>]</span>. For instance, in examining attitudes towards an educational intervention using variable-centered methods, we get an average that is simply the sum of negative and positive attitudes. If the majority of students have a positive attitude towards the proposed intervention —combined with a minority against— the final result will imply that students favour such intervention. The conclusions of this approach are not only wrong but also dangerous as it risks generalising solutions to groups to whom it may cause harm.</p>
<p>Therefore, clustering has become an essential method in all subfields of education (e.g., education psychology, education technology, and learning analytics) having operationalised almost all quantitative data types and been integrated with most of the existing methods <span class="citation" data-cites="Rennenallhoff1983 Dutt2015">[<a href="#ref-Rennenallhoff1983" role="doc-biblioref">10</a>, <a href="#ref-Dutt2015" role="doc-biblioref">17</a>]</span>. For instance, clustering became an essential step in sequence analysis to discover subsequences of data that can be understood as distinct approaches or strategies of students’ behavior <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>. Similar applications can be found in social network analysis data to identify collaborative roles, or in multimodal data analysis to identify moments of interest (e.g., synchrony). Similarly, clustering has been used with survey data to find attitudinal patterns or learning approaches to mention a few <span class="citation" data-cites="Dutt2015">[<a href="#ref-Dutt2015" role="doc-biblioref">17</a>]</span>. Furthermore, identifying patterns within students’ data is a prime educational interest in its own right and therefore, has been used extensively as a standalone analysis technique to identify subgroups of students who share similar interests, attitudes, behaviors, or background.</p>
<p>Clustering has been used in education psychology for decades to find patterns within self-reported questionnaire data. Early examples include the work of Beder and Valentine <span class="citation" data-cites="Beder1990">[<a href="#ref-Beder1990" role="doc-biblioref">19</a>]</span> who used responses of a motivational questionnaire to discover subgroups of students with different motivational attitudes. Similarly, Clément et al. <span class="citation" data-cites="Clement1994">[<a href="#ref-Clement1994" role="doc-biblioref">20</a>]</span> used the responses of a questionnaire that assessed anxiety and motivation towards learning English as a second language to find clusters which differentiated students according to their attitudes and motivation. Other examples include the work by Fernandez-Rio et al. <span class="citation" data-cites="Fernandez-Rio2014">[<a href="#ref-Fernandez-Rio2014" role="doc-biblioref">21</a>]</span> who used hierarchical clustering and <span class="math inline">\(K\)</span>-Means to identify distinct student profiles according to their perception of the class climate. With the digitalisation of educational institutions, authors also sought to identify students profiles using admission data and learning records. For instance, Cahapin et al. <span class="citation" data-cites="Cahapin2023">[<a href="#ref-Cahapin2023" role="doc-biblioref">22</a>]</span> used several algorithms such as <span class="math inline">\(K\)</span>-Means and agglomerative hierarchical clustering to identify patterns in students’ admission data.</p>
<p>The rise of online education opened many opportunities to investigate students’ behavior in learning management systems based on the trace log data that students leave behind them when working on online activities. An example is the work by Saqr et al. <span class="citation" data-cites="Saqr2022a">[<a href="#ref-Saqr2022a" role="doc-biblioref">23</a>]</span>, who used <span class="math inline">\(K\)</span>-Means to cluster in-service teachers’ approaches to learning in a MOOC according to their frequency of clicks on the available learning activities. Recently, research has placed a focus on the temporality of students’ activities, rather than the mere count. For this purpose, clustering has been integrated within sequence analysis to find subsequences which represent meaningful patterns of students behavior. For instance, Jovanovic et al. <span class="citation" data-cites="Jovanovic2017">[<a href="#ref-Jovanovic2017" role="doc-biblioref">24</a>]</span> used hierarchical clustering to identify distinct learning sequential patterns based on students’ online activities in a learning management system within a flipped classroom, and found a significant association between the use of certain learning sequences and learning outcomes. Using a similar approach, López-Pernas et al. <span class="citation" data-cites="Lopez-Pernas2021a">[<a href="#ref-Lopez-Pernas2021a" role="doc-biblioref">25</a>]</span> used hierarchical clustering to identify distinctive learning sequences in students’ use of the learning management system and an automated assessment tool for programming assignments. They also found an association between students’ activity patterns and their performance in the course final exam. Several other examples exist for using clustering to find patterns within sequence data <span class="citation" data-cites="Lopez-Pernas2021b Fan2022 Saqr2023a">[<a href="#ref-Saqr2023a" role="doc-biblioref">16</a>, <a href="#ref-Lopez-Pernas2021b" role="doc-biblioref">26</a>, <a href="#ref-Fan2022" role="doc-biblioref">27</a>]</span>.</p>
<p>A growing application of clustering can be seen in the study of computer-supported collaborative learning. Saqr and López-Pernas <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span> used cluster analysis to discover students with similar emergent roles based on their forum interaction patterns. Using the <span class="math inline">\(K\)</span>-Means algorithm and students’ centrality measures in the collaboration network, they identified three roles: influencers, mediators, and isolates. Perera et al. <span class="citation" data-cites="Perera2009">[<a href="#ref-Perera2009" role="doc-biblioref">29</a>]</span> used <span class="math inline">\(K\)</span>-Means to find distinct groups of similar teams and similar individual participating students according to their contributions in an online learning environment for software engineering education. They found that several clusters which shared some distinct contribution patterns were associated with more positive outcomes. Saqr and López-Pernas <span class="citation" data-cites="Saqr2023b">[<a href="#ref-Saqr2023b" role="doc-biblioref">30</a>]</span> analysed the temporal unfolding of students’ contributions to group discussions. They used hierarchical clustering to identify patterns of distinct students’ sequences of interactions that have a similar start and found a relationship between such patterns and student achievement.</p>
<p>An interesting application of clustering in education concerns the use of multimodal data. For instance, Vieira et al. <span class="citation" data-cites="Roque2018">[<a href="#ref-Roque2018" role="doc-biblioref">31</a>]</span> used <span class="math inline">\(K\)</span>-Means clustering to find patterns in students’ presentation styles according to their voice, position, and posture data. Other innovative uses involve students’ use of educational games <span class="citation" data-cites="Lee2022 Lopez-Pernas2022">[<a href="#ref-Lee2022" role="doc-biblioref">32</a>, <a href="#ref-Lopez-Pernas2022" role="doc-biblioref">33</a>]</span>, virtual reality <span class="citation" data-cites="Rosa2016">[<a href="#ref-Rosa2016" role="doc-biblioref">34</a>]</span>, or artificial intelligence <span class="citation" data-cites="Wang2023">[<a href="#ref-Wang2023" role="doc-biblioref">35</a>]</span>. Though this chapter illustrates traditional dissimilarity-based clustering algorithms with applications using R to data on the centrality measures of the participants of a MOOC, along with related demographic characteristics, readers are also encouraged to read Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span> and Chapter 10 <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span> in which further tutorials are presented and additional literature is reviewed, specifically in the contexts of clustering using the model-based clustering paradigm and clustering longitudinal sequence data, respectively. We now turn to an explication of the cluster analysis methodologies used in this chapter’s tutorial.</p>
</section>
<section id="sec-methods" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-methods"><span class="header-section-number">3</span> Clustering methodology</h2>
<p>In this section, we describe some of the theory underpinning the clustering methods used in the later R tutorial in <a href="#sec-tutR"><span>Section&nbsp;8.4</span></a>. We focus on some of the most widely-known heuristic dissimilarity-based clustering algorithms; namely, <span class="math inline">\(K\)</span>-means, <span class="math inline">\(K\)</span>-medoids, and agglomerative hierarchical clustering. In <a href="#sec-kmtheory"><span>Section&nbsp;8.3.1</span></a>, we introduce <span class="math inline">\(K\)</span>-Means clustering by describing the algorithm, outline the arguments to the relevant R function <code>kmeans()</code>, and discuss some of the main limitations and practical concerns researchers should be aware of in order to obtain the best performance when running <span class="math inline">\(K\)</span>-Means. We also discuss the related <span class="math inline">\(K\)</span>-Medoids algorithm and the associated function <code>pam()</code> in the <code>cluster</code> library <span class="citation" data-cites="cluster2022">[<a href="#ref-cluster2022" role="doc-biblioref">36</a>]</span> in R, and situate this method in the context of an extension to <span class="math inline">\(K\)</span>-Means designed to overcome its reliance on squared Euclidean distances. In <a href="#sec-hclust"><span>Section&nbsp;8.3.2</span></a>, we introduce agglomerative hierarchical clustering and the related R function <code>hclust()</code>, while outlining various choices available to practitioners and their implications. Though method-specific strategies of choosing the optimal number of clusters <span class="math inline">\(K\)</span> are provided throughout <a href="#sec-kmtheory"><span>Section&nbsp;8.3.1</span></a> and <a href="#sec-hclust"><span>Section&nbsp;8.3.2</span></a>, we offer a more detailed treatment of this issue in <a href="#sec-chooseK"><span>Section&nbsp;8.3.3</span></a>, particularly with regard to criteria that can guide the choice of clustering solution among multiple competing methodologies, and not only the choice of the number of clusters <span class="math inline">\(K\)</span> for a given method.</p>
<section id="sec-kmtheory" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="sec-kmtheory"><span class="header-section-number">3.1</span> <span class="math inline">\(K\)</span>-Means</h3>
<p><span class="math inline">\(K\)</span>-means is a widely-used clustering algorithm in learning analytics and indeed data analysis and machine learning more broadly. It is an unsupervised technique that seeks to divide a typically multivariate data set into some pre-specified number of clusters, based on the similarities between observations. More specifically, <span class="math inline">\(K\)</span>-means aims to partition <span class="math inline">\(n\)</span> objects <span class="math inline">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span>, each having measurements on <span class="math inline">\(j=1,\ldots,d\)</span> strictly continuous covariates, into a set of <span class="math inline">\(K\)</span> groups <span class="math inline">\(\mathcal{C}=\{C_1,\ldots,C_K\}\)</span>, where <span class="math inline">\(C_k\)</span> is the set of <span class="math inline">\(n_k\)</span> objects in cluster <span class="math inline">\(k\)</span> and the number of groups <span class="math inline">\(K \le n\)</span> is pre-specified by the practitioner and remains fixed. <span class="math inline">\(K\)</span>-means constructs these partitions in such a way that the squared Euclidean distance between the row vector for observations in a given cluster and the centroid (i.e., mean vector) of the given cluster are smaller than the distances to the centroids of the remaining clusters. In other words, <span class="math inline">\(K\)</span>-means aims to learn both the cluster centroids and the cluster assignments by minimising the within-cluster sums-of-squares (i.e., variances). Equivalently, this amounts to maximising the between-cluster sums-of-squares, thereby capturing heterogeneity in a data set by partitioning the observations into homogeneous groups. What follows is a brief technical description of the <span class="math inline">\(K\)</span>-Means algorithm in <a href="#sec-kmalgo"><span>Section&nbsp;8.3.1.1</span></a>, after which we describe some limitations of <span class="math inline">\(K\)</span>-Means and discuss practical concerns to be aware of in order to optimise the performance of the method in <a href="#sec-limitations"><span>Section&nbsp;8.3.1.2</span></a>. In particular, we present the widely-used <span class="math inline">\(K\)</span>-Medoids extension in <a href="#sec-kmedoids"><span>Section&nbsp;8.3.1.2.3</span></a>.</p>
<section id="sec-kmalgo" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="sec-kmalgo"><span class="header-section-number">3.1.1</span> <span class="math inline">\(K\)</span>-Means algorithm</h4>
<p>The origins of <span class="math inline">\(K\)</span>-Means are not so straightforward to summarise. The name was initially used by James MacQueen in 1967 <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span>. However, the standard algorithm was first proposed by Stuart Lloyd in a Bell Labs technical report in 1957, which was later published as a journal article in 1982 <span class="citation" data-cites="Lloyd1982">[<a href="#ref-Lloyd1982" role="doc-biblioref">37</a>]</span>. In order to understand the ideas involved, we must first define some relevant notation. Let <span class="math inline">\(\mu_k^{(j)}\)</span> denote the centroid value for the <span class="math inline">\(j\)</span>-th variable in cluster <span class="math inline">\(C_k\)</span> by <span class="math display">\[\mu_k^{(j)} = \frac{1}{n_k}\sum_{\mathbf{x}_i \in C_k} x_{ij}\]</span> and the complete centroid vector for cluster <span class="math inline">\(C_k\)</span> by <span class="math display">\[\boldsymbol{\mu}_k = \left(\mu_k^{(1)}, \ldots, \mu_k^{(p)}\right)^\top.\]</span> These centroids therefore correspond to the arithmetic mean vector of the observations in cluster <span class="math inline">\(C_k\)</span>. Finding both these centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> and the clustering partition <span class="math inline">\(\mathcal{C}\)</span> is computationally challenging and typically proceeds by iteratively alternating between allocating observations to clusters and then updating the centroid vectors. Formally, the objective is to minimise the total within-cluster sum-of-squares <span id="eq-kmeans_objective"><span class="math display">\[
\sum_{i=1}^n\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2,
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\lVert\mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2=\sum_{j=1}^p\left(x_{ij}-\mu_k^{(j)}\right)^2\)</span> denotes the squared Euclidean distance to the centroid <span class="math inline">\(\boldsymbol{\mu}_k\)</span> — such that <span class="math inline">\(\lVert\cdot\rVert_2\)</span> denotes the <span class="math inline">\(\ell^2\)</span> norm— and <span class="math inline">\(\mathbf{z}_i=(z_{i1},\ldots,z_{iK})^\top\)</span> is a latent variable such that <span class="math inline">\(z_{ik}\)</span> denotes the cluster membership of observation <span class="math inline">\(i\)</span>; <span class="math inline">\(z_{ik}=1\)</span> if observation <span class="math inline">\(i\)</span> belongs to cluster <span class="math inline">\(C_k\)</span> and <span class="math inline">\(z_{ik}=0\)</span> otherwise. This latent variable construction implies <span class="math inline">\(\sum_{i=1}^n z_{ik}=n_k\)</span> and <span class="math inline">\(\sum_{k=1}^Kz_{ik}=1\)</span>, such that each observation belongs wholly to one cluster only. As the total variation in the data, which remains fixed, can be written as a sum of the total within-cluster sum-of-squares (TWCSS) from <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a> and the between-cluster sum-of-squares as follows</p>
<p><span class="math display">\[\sum_{i=1}^n \left(\mathbf{x}_i - \bar{\mathbf{x}}\right)^2=\sum_{i=1}^n\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2 + \sum_{k=1}^Kn_k\lVert\boldsymbol{\mu}_k - \bar{\mathbf{x}}\rVert_2^2,\]</span></p>
<p>where <span class="math inline">\(\bar{\mathbf{x}}\)</span> denotes the overall sample mean vector, minimising the TWCSS endeavours to ensure that observations in the same cluster are maximally similar to observations in the same cluster and maximally dissimilar to those in other clusters.</p>
<p>Using the notation just introduced, a generic <span class="math inline">\(K\)</span>-means algorithm would proceed as follows:</p>
<ol type="1">
<li><p><em>Initialise</em>: Select the number of desired clusters <span class="math inline">\(K\)</span> and define <span class="math inline">\(K\)</span> initial centroid vectors <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span>.</p></li>
<li><p><em>Allocate</em>: Find the optimal <span class="math inline">\(z_{ik}\)</span> values that minimise the objective, holding the <span class="math inline">\(\mu_k\)</span> values fixed.</p></li>
</ol>
<p>Calculate the squared Euclidean distance between each observation and each centroid vector and allocate each object to the cluster corresponding to the initial centroid to which it is closest in terms of squared Euclidean distance. Looking at the objective function in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a> closely and examining the contribution of observation <span class="math inline">\(i\)</span>, we need to choose the value of <span class="math inline">\(\mathbf{z}_i\)</span> which minimises the expression <span class="math inline">\(\sum_{k=1}^K z_{ik} \lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2\)</span>. This is achieved by setting <span class="math inline">\(z_{ik}=1\)</span> for the value of <span class="math inline">\(k\)</span> that has smallest <span class="math inline">\(\lVert \mathbf{x}_i - \boldsymbol{\mu}_k\rVert_2^2\)</span> and setting <span class="math inline">\(z_{ik^\prime}=0\:\forall\:k^\prime\ne k\)</span> everywhere else.</p>
<ol start="3" type="1">
<li><em>Update</em>: Find the optimal <span class="math inline">\(\boldsymbol{\mu}_k\)</span> values that minimise the objective, holding the <span class="math inline">\(z_{ik}\)</span> values fixed.</li>
</ol>
<p>If we re-write the objective function in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a> as <span class="math display">\[\sum_{i=1}^n\sum_{k=1}^K\sum_{j=1}^p z_{ik} \left(x_{ij}-\mu_k^{(j)}\right)^2,\]</span> we can use the fact that <span class="math display">\[\frac{\partial}{\partial \mu_k^{(j)}}\left(x_{ij}-\mu_k^{(j)}\right)^2=-2\left(x_{ij}-\mu_k^{(j)}\right)\]</span> to obtain <span class="math display">\[\frac{\partial}{\partial \mu_k^{(j)}}\sum_{i=1}^n\sum_{k=1}^K\sum_{j=1}^p z_{ik} \left(x_{ij}-\mu_k^{(j)}\right)^2=-2\sum_{i=1}^nz_{ik}\left(x_{ij}-\mu_k^{(j)}\right).\]</span> Solving this expression for <span class="math inline">\(\mu_k^{(j)}\)</span> yields <span class="math display">\[\mu_k^{(j)}=\frac{\sum_{i=1}^nz_{ik}x_{ij}}{\sum_{i=1}^nz_{ik}}=\frac{1}{n_k}\sum_{\mathbf{x}_i \in C_k} x_{ij}.\]</span></p>
<ol start="4" type="1">
<li><em>Iterate</em>: One full iteration of the algorithm consists of an allocation step (Step 2) and an update step (Step 3). Steps 2 and 3 are repeated until no objects can be moved between clusters, at which point the algorithm has converged to at least a local minimum of <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a>.</li>
</ol>
<p>Upon convergence, we obtain not only the estimated partition <span class="math inline">\(\mathcal{C}=\{C_1,\ldots,C_K\}\)</span>, indicating the cluster membership of each observation, but also the estimated cluster centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> which act as cluster prototypes, efficiently summarising the characteristics of each cluster. The algorithm just described is just one standard variant of <span class="math inline">\(K\)</span>-Means. there have been several algorithms proposed for the same objective which derive their names from the author who proposed them; namely MacQueen <span class="citation" data-cites="MacQueen1967">[<a href="#ref-MacQueen1967" role="doc-biblioref">4</a>]</span>, Lloyd <span class="citation" data-cites="Lloyd1982">[<a href="#ref-Lloyd1982" role="doc-biblioref">37</a>]</span>, Forgy <span class="citation" data-cites="Forgy1965">[<a href="#ref-Forgy1965" role="doc-biblioref">38</a>]</span>, and Hartigan and Wong <span class="citation" data-cites="Hartigan1979">[<a href="#ref-Hartigan1979" role="doc-biblioref">39</a>]</span>. They differ in some subtle ways, particularly with regard to how initial centroids in Step 1 are chosen and whether Steps 3 and 4 are applied to all <span class="math inline">\(n\)</span> observations simultaneously or whether allocations and centroids are updated for each observation one-by-one (i.e., some variants iterate over Step 2 and Step 3 in a loop over <span class="math inline">\(i=1,\ldots,n\)</span>). Without going into further details, we note that the default option for <code>kmeans()</code> in R uses the option <code>algorithm="Hartigan-Wong"</code> and this is what we will henceforth adopt throughout.</p>
</section>
<section id="sec-limitations" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="sec-limitations"><span class="header-section-number">3.1.2</span> <span class="math inline">\(K\)</span>-Means limitations and practical concerns</h4>
<p>Though <span class="math inline">\(K\)</span>-Means is a useful tool in many application contexts due to its conceptual and computational simplicity —so ubiquitous, in fact, that the <code>kmeans()</code> function in R is available without loading any additional libraries— it suffers from numerous limitations and some care is required in order to obtain reasonable results. We now discuss some of the main limitations in turn, but note that each is addressed explicitly throughout the <span class="math inline">\(K\)</span>-Means application portion of the R tutorial in <a href="#sec-kmapp"><span>Section&nbsp;8.4.2.1</span></a>. The concerns relate to choosing <span class="math inline">\(K\)</span> (<a href="#sec-elbow"><span>Section&nbsp;8.3.1.2.1</span></a>), choosing initial centroids <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> (<a href="#sec-kmpp"><span>Section&nbsp;8.3.1.2.2</span></a>), and relaxing the reliance on squared Euclidean distances with the more general <span class="math inline">\(K\)</span>-Medoids method (<a href="#sec-kmedoids"><span>Section&nbsp;8.3.1.2.3</span></a>).</p>
<section id="sec-elbow" class="level5" data-number="3.1.2.1">
<h5 data-number="3.1.2.1" class="anchored" data-anchor-id="sec-elbow"><span class="header-section-number">3.1.2.1</span> Fixed <span class="math inline">\(K\)</span> and the elbow method</h5>
<p>The first major drawback is that the number of clusters <span class="math inline">\(K\)</span> must be pre-specified. This is a key input parameter: if <span class="math inline">\(K\)</span> is too low, dissimilar observations will be wrongly grouped together; if <span class="math inline">\(K\)</span> is too large, observations will be partitioned into many small, similar clusters which may not be meaningfully different. Choosing the optimal <span class="math inline">\(K\)</span> necessitates running the algorithm at various fixed values of <span class="math inline">\(K\)</span> and finding the single value of <span class="math inline">\(K\)</span> which best balances interpretability, parsimony, and fit quality. Fit quality is measured by the TWCSS, i.e., the objective function in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a>. Increasing <span class="math inline">\(K\)</span> indefinitely will cause the TWCSS to decrease indefinitely, but this is not what we want. Instead, we seek a <span class="math inline">\(K\)</span> value beyond which the decrease in TWCSS is minimal, in order to yield a parsimonious solution with a reasonable number of clusters to interpret, without overfitting the data or merely subdividing the actual groups. Thus, a commonly used heuristic graphical method for determining the optimal <span class="math inline">\(K\)</span> value is to plot a range of <span class="math inline">\(K\)</span> values against the corresponding obtained TWCSS values and look for an “elbow” or kink in the resulting curve. Such a plot will guide the choice of <span class="math inline">\(K\)</span> in the <span class="math inline">\(K\)</span>-Means portion of R tutorial which follows in <a href="#sec-kmapp"><span>Section&nbsp;8.4.2.1</span></a>.</p>
</section>
<section id="sec-kmpp" class="level5" data-number="3.1.2.2">
<h5 data-number="3.1.2.2" class="anchored" data-anchor-id="sec-kmpp"><span class="header-section-number">3.1.2.2</span> Initialisation and <span class="math inline">\(K\)</span>-Means</h5>
<p>An especially pertinent limitation which must be highlighted is that <span class="math inline">\(K\)</span>-Means is liable to converge to sub-optimal local minima, i.e., it is not guaranteed to converge to the global minimum of the objective function in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a>. Many practitioners have observed that the performance of <span class="math inline">\(K\)</span>-Means is particularly sensitive to a good choice of initial cluster centroids in Step 1. Indeed, as different initial settings can lead to different clusterings of the same data, good starting values are vital to the success of the <span class="math inline">\(K\)</span>-Means algorithm. Typically, <span class="math inline">\(K\)</span> random vectors are used to define the initial <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> centroids. One means of mitigating (but not completely remedying) the problem is to run <span class="math inline">\(K\)</span>-Means with a suitably large number of random starting values and choose the solution associated with the set of initial centroids which minimise the TWCSS criterion.</p>
<p>In order to contextualise this issue, it is prudent to first describe some of the main arguments to the <code>kmeans()</code> R function. The following list is a non-exhaustive list of the available arguments to <code>kmeans()</code>:</p>
<ul>
<li><p><code>x</code>: a numeric matrix of data or a <code>data.frame</code> with all numeric columns.</p></li>
<li><p><code>centers</code>: either the number of clusters <span class="math inline">\(K\)</span> or a set of <span class="math inline">\(K\)</span> initial (distinct) cluster centroids.</p></li>
<li><p><code>nstart</code>: if <code>centers</code> is specified as a number, this represents the number of random sets of <span class="math inline">\(K\)</span> initial centroids with which to run the algorithm.</p></li>
<li><p><code>iter.max</code>: the maximum number of allocation/update cycles allowed per set of initial centroids.</p></li>
</ul>
<p>The arguments <code>nstart</code> and <code>iter.max</code> have default values of <code>1</code> and <code>10</code>, respectively. Thus, a user running <code>kmeans()</code> with <code>centers</code> specified as a number will, by default, only use one random set of initial centroids, to which the results are liable to be highly sensitive, and will have the algorithm terminate after just ten iterations, regardless of whether convergence was achieved. It would seem be an improvement, therefore, to increase the values of <code>nstart</code> and <code>iter.max</code> from these defaults. Fortunately, the function automatically returns the single optimal solution according to the random initialisation which yields the lowest TWCSS when <code>nstart</code> exceeds <code>1</code>.</p>
<p>Though this will generally lead to better results, this approach can be computationally onerous if the number of observations <span class="math inline">\(n\)</span>, number of features <span class="math inline">\(d\)</span>, or size of the range of fixed <span class="math inline">\(K\)</span> values under consideration is large. An alternative strategy, which greatly reduces the computational burden and the sensitivity of the final solution to the initial choices of <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k\)</span> is to choose a suitable set of informed starting values in a data-driven fashion. To this end, the so-called <span class="math inline">\(K\)</span>-Means algorithm was proposed <span class="citation" data-cites="Arthur2007">[<a href="#ref-Arthur2007" role="doc-biblioref">40</a>]</span> in order to improve the performance of <span class="math inline">\(K\)</span>-Means by replacing Step 1 with an iterative distance-weighting scheme to select the initial cluster centroids. Though there is still randomness inherent in <span class="math inline">\(K\)</span>-Means, this initialisation technique ensures that the initial centroids are well spread out across the data space, which increases the likelihood of converging to the true global minimum. The <span class="math inline">\(K\)</span>-Means algorithm works as follows:</p>
<ol type="A">
<li><p>Choose an initial centroid uniformly at random from the rows of the data set.</p></li>
<li><p>For each observation not yet chosen as a centroid, compute <span class="math inline">\(D^2(\mathbf{x}_i)\)</span>, which represents the squared Euclidean distance between <span class="math inline">\(\mathbf{x}_i\)</span> and the nearest centroid that has already been chosen.</p></li>
<li><p>Randomly sample new observation as a new centroid vector with probability proportional to <span class="math inline">\(D^2(\mathbf{x}_i)\)</span>.</p></li>
<li><p>Repeat Steps B and C until <span class="math inline">\(K\)</span> centroids have been chosen. If any of the chosen initial centroids are not distinct, add a small amount of random jitter to distinguish the non-unique centroids.</p></li>
<li><p>Proceed as per Steps 2–4 of the traditional <span class="math inline">\(K\)</span>-Means algorithm (or one of its variants).</p></li>
</ol>
<p>Although these steps take extra time, <span class="math inline">\(K\)</span>-Means itself tends to converge very quickly thereafter and thus <span class="math inline">\(K\)</span>-Means actually reduces the computational burden. A manual implementation of the <span class="math inline">\(K\)</span>-Means is provided by the function <code>kmeans_pp()</code> below. Its output can be used as the <code>centers</code> argument when running <code>kmeans()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>kmeans_pp <span class="ot">&lt;-</span> <span class="cf">function</span>(X, <span class="co"># data</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>                      K  <span class="co"># number of centroids</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>                      ) {</span>
<span id="cb1-4"><a href="#cb1-4"></a>  </span>
<span id="cb1-5"><a href="#cb1-5"></a>  <span class="co"># sample initial centroid from distinct rows of X</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>  X       <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">as.matrix</span>(X))</span>
<span id="cb1-7"><a href="#cb1-7"></a>  new_center_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X), <span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a>  centers <span class="ot">&lt;-</span> X[new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-9"><a href="#cb1-9"></a>  </span>
<span id="cb1-10"><a href="#cb1-10"></a>  <span class="co"># let x be all observations not yet chosen as a centroid</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>  X <span class="ot">&lt;-</span> X[<span class="sc">-</span>new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-12"><a href="#cb1-12"></a>  </span>
<span id="cb1-13"><a href="#cb1-13"></a>  <span class="cf">if</span>(K <span class="sc">&gt;=</span> <span class="dv">2</span>) {</span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="co"># loop over remaining centroids</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>    <span class="cf">for</span>(kk <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>K) {</span>
<span id="cb1-16"><a href="#cb1-16"></a>      </span>
<span id="cb1-17"><a href="#cb1-17"></a>      <span class="co"># calculate distances from all observations to all already chosen centroids</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>      distances <span class="ot">&lt;-</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">min</span>(<span class="fu">sum</span>((x <span class="sc">-</span> centers)<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb1-19"><a href="#cb1-19"></a>      </span>
<span id="cb1-20"><a href="#cb1-20"></a>      <span class="co"># sample new centroid with probability proportional to squared Euclidean distance</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>      probabilities <span class="ot">&lt;-</span> distances<span class="sc">/</span><span class="fu">sum</span>(distances)</span>
<span id="cb1-22"><a href="#cb1-22"></a>      new_center_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X), <span class="dv">1</span>, <span class="at">prob=</span>probabilities)</span>
<span id="cb1-23"><a href="#cb1-23"></a>      </span>
<span id="cb1-24"><a href="#cb1-24"></a>      <span class="co"># record the new centroid and remove it for the next iteration</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>      centers <span class="ot">&lt;-</span> <span class="fu">rbind</span>(centers, X[new_center_index,, <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb1-26"><a href="#cb1-26"></a>      X <span class="ot">&lt;-</span> X[<span class="sc">-</span>new_center_index,, drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb1-27"><a href="#cb1-27"></a>    }</span>
<span id="cb1-28"><a href="#cb1-28"></a>  }</span>
<span id="cb1-29"><a href="#cb1-29"></a>  </span>
<span id="cb1-30"><a href="#cb1-30"></a>  <span class="co"># add random jitter to distinguish non-unique centroids and return</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>  centers[<span class="fu">duplicated</span>(centers)] <span class="ot">&lt;-</span> <span class="fu">jitter</span>(centers[<span class="fu">duplicated</span>(centers)])</span>
<span id="cb1-32"><a href="#cb1-32"></a>  <span class="fu">return</span>(centers)</span>
<span id="cb1-33"><a href="#cb1-33"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>However, it should be noted that there is still inherent randomness in <span class="math inline">\(K\)</span>-Means —note the use of <code>sample()</code> in lines 7 and 22— and the algorithm is liable to produce different initial centroids in different runs on the same data. In effect, <span class="math inline">\(K\)</span>-Means does not remove the burden of random initialisation; it is merely a way to have more informed random initialisations. Thus, it would be prudent to run <span class="math inline">\(K\)</span>-Means <em>with</em> <span class="math inline">\(K\)</span>-Means initialisation and select the solution which minimises the TWCSS, to transfer the burden of requiring multiple runs of <span class="math inline">\(K\)</span>-Means with random starting values to fewer runs of <span class="math inline">\(K\)</span>-Means followed by <span class="math inline">\(K\)</span>-Means with more informed starting values. We adopt this strategy in the later R tutorial in <a href="#sec-kmapp"><span>Section&nbsp;8.4.2.1</span></a>.</p>
</section>
<section id="sec-kmedoids" class="level5" data-number="3.1.2.3">
<h5 data-number="3.1.2.3" class="anchored" data-anchor-id="sec-kmedoids"><span class="header-section-number">3.1.2.3</span> <span class="math inline">\(K\)</span>-Medoids and other distances</h5>
<p>The <span class="math inline">\(K\)</span>-Means objective function in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a> explicitly relies on squared Euclidean distances and requires all features in the data set to be strictly continuous. An artefact of this distance measure is that it is generally recommended to standardise all features of have a mean of zero and unit variance prior to running <span class="math inline">\(K\)</span>-Means. In general, standardisation is advisable if the values are of incomparable units (e.g., height in inches and weight in kilogram). More specifically for <span class="math inline">\(K\)</span>-Means, it is desirable to ensure all features have comparable variances to avoid having variables with higher magnitudes and variances dominate the distance calculation and have an undue prominence on the clustering partition obtained. While we employ such normalisation to the data used in our R tutorial when applying some pre-processing steps in <a href="../ch12-markov/ch12-markov.html#sec-process"><span>Section&nbsp;12.4.3</span></a>, we note that this is not sufficient to overcome all shortcomings of relying on squared Euclidean distances.</p>
<p>For these reasons and more, <span class="math inline">\(K\)</span>-Medoids —otherwise known as partitioning around medoids (PAM)— was proposed as an extension to <span class="math inline">\(K\)</span>-Means which allows using any alternative dissimilarity measure <span class="citation" data-cites="Kaufman1990-pam">[<a href="#ref-Kaufman1990-pam" role="doc-biblioref">5</a>]</span>. The <span class="math inline">\(K\)</span>-Medoids objective function is given by <span class="math display">\[\sum_{i=1}^n\sum_{k=1}^K z_{ik} d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right),\]</span> where <span class="math inline">\(d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right)\)</span> can be any distance measure rather than squared Euclidean and <span class="math inline">\(\boldsymbol{\psi}_k\)</span> is used in place of the mean vector <span class="math inline">\(\boldsymbol{\mu}_k\)</span>. The PAM algorithm works in much the same fashion as <span class="math inline">\(K\)</span>-Means, alternating between an allocation step which assigns each observation to the cluster with the closest <span class="math inline">\(\boldsymbol{\psi}_k\)</span> (according to the specified distance measure) and an update step which minimises the within-cluster total distance (WCTD). Notably, when minimising with respect to <span class="math inline">\(\boldsymbol{\psi}_k\)</span>, the notion of a cluster centroid <span class="math inline">\(\boldsymbol{\mu}_k\)</span> is redefined as a cluster medoid <span class="math inline">\(\boldsymbol{\psi}_k\)</span>, which is selected among the rows of the observed data set, i.e., the medoid is the observation <span class="math inline">\(\mathbf{x}_i\)</span> from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised. Similar to <span class="math inline">\(K\)</span>-Means, the medoids obtained at convergence again enable straightforward characterisation of a “typical” observation from each cluster and the elbow method from <a href="#sec-elbow"><span>Section&nbsp;8.3.1.2.1</span></a> can be adapted to guide the choice of <span class="math inline">\(K\)</span> in <span class="math inline">\(K\)</span>-Medoids by plotting a range of candidate <span class="math inline">\(K\)</span> values against the within-cluster total distance.</p>
<p>This reformulation has three main advantages. Firstly, the distance <span class="math inline">\(d\left(\mathbf{x}_i, \boldsymbol{\psi}_k\right)\)</span> is not squared, which diminishes the influence of outliers. As <span class="math inline">\(K\)</span>-Means relies on squared Euclidean distances, which inflates the distances of atypical observations, and defines centroids as means, it is not robust to outliers. Secondly, by defining the medoids as observed rows of the data, rather than finding the value of <span class="math inline">\(\boldsymbol{\psi}_k\)</span> that minimises in general, which could potentially be difficult to estimate for complex data types or particularly sophisticated dissimilarity measures, the algorithm can be much more computationally efficient. It requires only a pre-calculated pairwise dissimilarity matrix as input. Finally, the flexibility afforded by being able to modify the dissimilarity measure enables data which are not strictly continuous to be clustered. In other words, <span class="math inline">\(K\)</span>-Medoids is applicable in cases where the mean is undefined. As examples, one could use the Manhattan or general Minkowski distances as alternatives for clustering continuous data, the Hamming <span class="citation" data-cites="Hamming1950">[<a href="#ref-Hamming1950" role="doc-biblioref">41</a>]</span>, Jaccard <span class="citation" data-cites="Jaccard1901">[<a href="#ref-Jaccard1901" role="doc-biblioref">42</a>]</span>, or Sørensen-Dice <span class="citation" data-cites="Dice1945 Sorensen1948">[<a href="#ref-Dice1945" role="doc-biblioref">43</a>, <a href="#ref-Sorensen1948" role="doc-biblioref">44</a>]</span> distances for clustering binary data, or the Gower distance <span class="citation" data-cites="Gower1971">[<a href="#ref-Gower1971" role="doc-biblioref">45</a>]</span> for clustering mixed-type data with both continuous and categorical features. The closely related <span class="math inline">\(K\)</span>-Modes algorithm <span class="citation" data-cites="Huang1998">[<a href="#ref-Huang1998" role="doc-biblioref">46</a>]</span> has also been proposed specifically for purely categorical data applications, as well as the <span class="math inline">\(K\)</span>-Prototypes algorithm <span class="citation" data-cites="Huang1997">[<a href="#ref-Huang1997" role="doc-biblioref">47</a>]</span> for mixed-type variables applications; neither will considered further in this chapter). As their names suggest, they again redefine the notion of a centroid but otherwise proceed much like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids.</p>
<p>The function <code>pam()</code> in the <code>cluster</code> library in R provides an implementation of <span class="math inline">\(K\)</span>-Medoids, with options for implementing many recent additional speed improvements and improved initialisation strategies <span class="citation" data-cites="Schubert2021">[<a href="#ref-Schubert2021" role="doc-biblioref">48</a>]</span>. We will discuss these in the <span class="math inline">\(K\)</span>-Medoids portion of the later R tutorial in <a href="#sec-pamapp"><span>Section&nbsp;8.4.2.2</span></a>. Most dissimilarity measures we will use are implement in the base-R function <code>dist()</code>, with the exception of the Gower distance which is implemented in the <code>daisy()</code> function in the <code>cluster</code> library.</p>
</section>
</section>
</section>
<section id="sec-hclust" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-hclust"><span class="header-section-number">3.2</span> Agglomerative hierarchical clustering</h3>
<p>Hierarchical clustering is another versatile and widely-used dissimilarity-based clustering paradigm. Though also dissimilarity-based, hierarchical clustering differs from partitional clustering methods like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids in that it typically doesn’t avail of the notion of computing distances to a central prototype, be that a centroid mean vector or a medoid, but instead greedily builds a hierarchy of clusters based on dissimilarities between observations themselves and sets of observations. Consequently, a hierarchical clustering solution provides a set of partitions, from a single cluster to as many clusters as observations, rather than the single partition obtained by <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids. The results of a hierarchical clustering are usually presented in the form of a dendrogram visualisation, which illustrates the arrangement of the set of partitions visited and can help guide the decision of the optimal single clustering partition to extract. However, hierarchical clustering shares some of the advantages <span class="math inline">\(K\)</span>-Medoids has over <span class="math inline">\(K\)</span>-Means. Firstly, any valid measure of distance can be used, so it is not restricted to squared Euclidean distances and not restricted to clustering purely continuous data. Secondly, hierarchical clustering algorithms do not require the data set itself as input; all that is required is a matrix of pairwise distances.</p>
<p>Broadly speaking, there are two categories of hierarchical clustering:</p>
<ul>
<li><p><em>Agglomerative</em>: Starting from the bottom of the hierarchy, begin with each observation in a cluster of its own and successively merged pairs of clusters while moving up the hierarchy, until all observations are in one cluster. This approach is sometimes referred to as agglomerative nesting (AGNES; <span class="citation" data-cites="Kaufman1990-agnes">[<a href="#ref-Kaufman1990-agnes" role="doc-biblioref">6</a>]</span>).</p></li>
<li><p><em>Divisive</em>: Starting from the top of the hierarchy, with all observations in one cluster, recursively split clusters while moving down the hierarchy, until all observations are in a cluster of their own. This approach is sometimes referred to as divisive analysis (DIANA; <span class="citation" data-cites="Kaufman1990-diana">[<a href="#ref-Kaufman1990-diana" role="doc-biblioref">49</a>]</span>).</p></li>
</ul>
<p>However, divisive clustering algorithms such as DIANA are much more computationally onerous for even moderately large data sets. Thus, we focus here on the agglomerative variant of hierarchical clustering, AGNES, which is is implemented in both the <code>agnes()</code> function in the <code>cluster</code> library and the <code>hclust()</code> function in base R. We adopt the latter in the hierarchical clustering portion of the R tutorial in <a href="#sec-hcapp"><span>Section&nbsp;8.4.2.3</span></a>. That being said, even agglomerative hierarchical clustering has significant computation and memory burdens when <span class="math inline">\(n\)</span> is large <span class="citation" data-cites="Gilpin2013 Bouguettaya2015">[<a href="#ref-Gilpin2013" role="doc-biblioref">50</a>, <a href="#ref-Bouguettaya2015" role="doc-biblioref">51</a>]</span>.</p>
<p>There are three key decisions practitioners must make when employing agglomerative hierarchical clustering. The first of these, the distance measure, has already been discussed in the context of <span class="math inline">\(K\)</span>-Medoids. We now discuss the other two in turn; namely, the so-called linkage criterion for quantifying the distances between merged clusters as the algorithm moves up the hierarchy, and the criterion used for cutting the resulting dendrogram to produce a single partition.</p>
<section id="sec-linkage" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="sec-linkage"><span class="header-section-number">3.2.1</span> Linkage criteria</h4>
<p>Agglomerative hierarchical clustering employs two different notions of dissimilarity. There is the distance measure, <span class="math inline">\(d\)</span>, such as Euclidean, Manhattan, or Gower distance, which is used to quantify the distance between pairs of <em>single</em> observations in the data set. Different choices of distance measure can lead to markedly different clustering results and it is thus common to run the hierarchical clustering algorithm with different choices of distance measure and compare the results. However, in the agglomerative setting, individual observations are successively merged into clusters. In order to decide which clusters should be combined, it is necessary to quantify the dissimilarity of <em>sets</em> of observations as a function of the pairwise distances of observations in the sets. This gives rise to the notion of a linkage criterion. At each step, the two clusters separated by the shortest distance are combined; the linkage criteria is precisely the definition of ‘shortest distance’ which differentiates different agglomerative approaches. Again, the choice of linkage criterion can have a substantial impact on the result of the clustering so multiple solutions with different combinations of distance measure and linkage criterion should be evaluated.</p>
<p>There are a number of commonly-used linkage criteria which we now describe. A non-exhaustive list of such linkages follows —only those which we use in the later R tutorial in <a href="#sec-tutR"><span>Section&nbsp;8.4</span></a>— in which we let <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> denote two sets of observations, <span class="math inline">\(\lvert\cdot\rvert\)</span> denote the cardinality of a set, and <span class="math inline">\(d(a, b)\)</span> denote the distance between observations in those corresponding sets according to the chosen distance measure. We note that ties for the maximum or minimum distances for complete linkage and single linkage, respectively, are broken at random.</p>
<ul>
<li><p><em>Complete</em> linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are furthest away from each other according to the chosen distance measure <span class="math inline">\(d\)</span>: <span class="math display">\[\max_{a \in \mathcal{A}, b \in \mathcal{B}} d(a, b).\]</span></p></li>
<li><p><em>Single</em> linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are closest to each other according to the chosen distance measure <span class="math inline">\(d\)</span>: <span class="math display">\[\min_{a \in \mathcal{A}, b \in \mathcal{B}} d(a, b).\]</span></p></li>
<li><p><em>Average</em> linkage: Define the dissimilarity between two clusters as the average distance according to the chosen distance measure <span class="math inline">\(d\)</span> between all pairs of elements (on in each cluster): <span class="math display">\[\frac{1}{\lvert\mathcal{A}\rvert \times \lvert\mathcal{B}\rvert}\sum_{a \in \mathcal{A}}\sum_{b \in \mathcal{B}}d_(a, b).\]</span></p></li>
<li><p><em>Centroid</em> linkage: Define the dissimilarity between two clusters <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> as the distance, according to the chosen distance measure <span class="math inline">\(d\)</span>, between their corresponding centroid vectors <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{A}}\)</span> and <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{B}}\)</span>: <span class="math display">\[d(\boldsymbol{\mu}_{\mathcal{A}}, \boldsymbol{\mu}_{\mathcal{B}}).\]</span></p></li>
<li><p><em>Ward</em> linkage <span class="citation" data-cites="Ward1963">[<a href="#ref-Ward1963" role="doc-biblioref">52</a>]</span>: Instead of measuring the dissimilarity between clusters directly, define the dissimilarity as the cost of merging two clusters as the increase in total within-cluster variance after merging. In other words, minimise the total within-cluster sum-of-squares by finding the pair of clusters at each step which leads to minimum increase in total within-cluster variance after merging, where <span class="math inline">\(\mathcal{A} \cup \mathcal{B}\)</span> denotes the cluster obtained after merging, with corresponding centroid <span class="math inline">\(\boldsymbol{\mu}_{\mathcal{A}\cup\mathcal{B}}\)</span>: <span class="math display">\[\frac{\lvert\mathcal{A}\rvert \times \lvert \mathcal{B}}{\lvert \mathcal{A} \cup \mathcal{B}\rvert}\lVert\boldsymbol{\mu}_{\mathcal{A}} - \boldsymbol{\mu}_{\mathcal{B}}\rVert_2^2 = \sum_{x\in \mathcal{A}\cup\mathcal{B}}\lVert x - \boldsymbol{\mu}_{\mathcal{A}\cup\mathcal{B}}\rVert_2^2 - \sum_{x \in \mathcal{A}}\lVert x - \boldsymbol{\mu}_{\mathcal{A}}\rVert_2^2 - \sum_{x \in \mathcal{B}}\lVert x - \boldsymbol{\mu}_{\mathcal{B}}\rVert_2^2.\]</span></p></li>
</ul>
<p>The Ward and centroid linkage criteria differ from the other linkage criteria in that they are typically meant to be used only when the initial pairwise distances between observations are squared Euclidean distances. All of the above linkage criteria are implemented in the function <code>hclust()</code>, which is available in R without requiring any add-on libraries to be loaded and specifically performs agglomerative hierarchical clustering. Its main arguments are <code>d</code>, a pre-computed pairwise dissimilarity matrix (as can be created from the function <code>dist()</code>), and <code>method</code>, which specifies the linkage criterion (e.g., <code>"complete"</code>, <code>"single"</code>, <code>"average"</code>, and <code>"centroid"</code>). Special care must be taken when employing Ward’s linkage criterion as two options are available: <code>"ward.D"</code>, which assumes that the initial pairwise distance matrix already consists of <em>squared</em> Euclidean distances, and <code>"ward.D2"</code>, which assumes the distances are merely Euclidean distances and performs the squaring internally <span class="citation" data-cites="Murtagh2014">[<a href="#ref-Murtagh2014" role="doc-biblioref">53</a>]</span>.</p>
</section>
<section id="sec-cutree" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="sec-cutree"><span class="header-section-number">3.2.2</span> Cutting the dendrogram</h4>
<p>One might notice that when calling <code>hclust()</code>, the number of clusters <span class="math inline">\(K\)</span> is not specified in advance, as it is when calling <code>kmeans()</code> or <code>pam()</code>. Instead, <code>hclust()</code> returns an object which describes the hierarchy of the tree produced by the clustering process. A visualisation of such a tree is referred to as a dendrogram, which can be thought of as a representation of a set of candidate partitions. In a dendrogram representation of an agglomerative hierarchical clustering solution, each observation is initially in a singleton cluster on its own, along the x-axis, according to their similarities. Thereafter, each observation, and subsequently each set of observations, are merged along the y-axis in a nested fashion. The scale along the y-axis is proportional to the distance, according to the chosen linkage criterion, at which two clusters are combined. In the end, the groups formed towards the bottom of the graph are close together, whereas those at the top of the graph are far apart.</p>
<p>Obtaining a single hard partition of objects into disjoint clusters is obtained by cutting the dendrogram horizontally at the corresponding height. In other words, observations are allocated to clusters by cutting the tree at an appropriate height. Generally, the lower this height, the greater the number of clusters (theoretically, there can be as many clusters as there are observations, <span class="math inline">\(n\)</span>), while the greater the height, the lower the number of clusters (theoretically, there can be as few as only one cluster, corresponding to no group structure in the data). Thus, an advantage of hierarchical clustering is that the user need not know <span class="math inline">\(K\)</span> in advance; the user can manually select <span class="math inline">\(K\)</span> after the fact by examining the constructed tree and fine-tuning the output to find clusters with a desired level of granularity. There is no universally applicable rule for determining the optimal height at which to cut the tree, but it is common to select a height in the region where there is the largest gap between merges, i.e., where there is a relatively wide range of distances over which the number of clusters in the resulting partition does not change. This is, of course, very much guided by the visualisation itself.</p>
<p>In R, one can visualise the dendrogram associated with a particular choice of distance measure and linkage criterion by calling <code>plot()</code> on the output from <code>hclust()</code>. Thereafter, the function <code>cutree()</code> can be used to obtain a single partition. This function takes the arguments <code>tree</code>, which is the result of a call to <code>hclust()</code>, <code>h</code> which is the height at which the tree should be cut, and <code>k</code> which more directly allows the desired number of clusters to be produced. Specifying <code>k</code> finds the corresponding height which yields <code>k</code> clusters and overrides the specification of <code>h</code>.</p>
<p>However, it is often the case that certain combinations of dissimilarity measure and linkage criterion produce undesirable dendrograms. In particular, complete linkage is known to perform poorly in the presence of outliers, given its reliance on maximum distances, and single linkage is known to produce a “chaining” effect on the resulting dendrogram, whereby, due to its reliance on minimum distances, observations tend to continuously join increasingly larger, existing clusters rather than being merged with other observations to form new clusters. A negative consequence of this phenomenon is lack of cohesion: observations at opposite ends of the same cluster in a dendrogram could be quite dissimilar. These limitations can also be attributed to hierarchical clustering —regardless of the linkage employed— optimising a local criterion for each merge, unlike <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids which endeavour to optimise global objectives.</p>
</section>
</section>
<section id="sec-chooseK" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="sec-chooseK"><span class="header-section-number">3.3</span> Choosing the number of clusters</h3>
<p>Determining the number of clusters in a data set is a fraught task. Throughout <a href="#sec-kmtheory"><span>Section&nbsp;8.3.1</span></a> and <a href="#sec-hclust"><span>Section&nbsp;8.3.2</span></a>, method-specific strategies for guiding the choice of <span class="math inline">\(K\)</span> were presented. However, they are not without their limitations. For <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, the elbow method is somewhat subjective and unreliable. Often, the presence of an elbow is not so clear at a single value of <span class="math inline">\(K\)</span>. Likewise, for agglomerative hierarchical clustering, choosing a height at which to cut the dendrogram as the criterion for choosing <span class="math inline">\(K\)</span> has also been criticised as an overly subjective method. Moreover, these strategies are only capable of identifying the best <span class="math inline">\(K\)</span> value conditional on the chosen method and do not help to identify the overall best solution among multiple competing methods. Moreover, we are required to choose more than just the optimal <span class="math inline">\(K\)</span> value when using <span class="math inline">\(K\)</span>-Medoids (for which different solutions with different dissimilarity measures and different <span class="math inline">\(K\)</span> values can be obtained) and agglomerative hierarchical clustering (for which different solutions can be obtained using different dissimilarity measures and linkage criteria).</p>
<p>Overcoming these ambiguities and identifying a more general strategy for comparing the quality of clustering partitions is a difficult task for which many criteria have been proposed. Broadly speaking, cluster quality measures fall into two categories:</p>
<ol type="1">
<li><p>Comparing of the uncovered partition to a reference clustering (or known grouping labels).</p></li>
<li><p>Measuring of internal cluster consistency without reference to ground truth labels.</p></li>
</ol>
<p>The first is typically conducted using the Rand index <span class="citation" data-cites="Rand1971">[<a href="#ref-Rand1971" role="doc-biblioref">54</a>]</span> or adjusted Rand index <span class="citation" data-cites="Hubert1985">[<a href="#ref-Hubert1985" role="doc-biblioref">55</a>]</span>, which measure the agreement between two sets of partitions. However, as we will be exploring clustering in exploratory, unsupervised settings, using data for which there is no assumed “true” group structure, we will instead focus on a quality measure of the latter kind. As previously stated, a large number of such criteria have been proposed in the literature: several are summarised in Table 7 of Chapter 10 of this book <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>, where they are used to guide the choice of <span class="math inline">\(K\)</span> for agglomerative hierarchical clustering in the context of sequence analysis. Here, however, for the sake of brevity, we describe only one commonly used criterion which we later employ in the R tutorial in <a href="#sec-tutR"><span>Section&nbsp;8.4</span></a> —which is itself a dissimilarity-based measure and is thus universally applicable to all clustering algorithms we employ— even if in most applications it would be wise to inform the choice of <span class="math inline">\(K\)</span> with several such quantitative criteria. Moreover, the practitioner’s own subject matter expertise and assessment of the interpretability of the obtained clusters should also be used to inform the choice of <span class="math inline">\(K\)</span>.</p>
<p>The quantitative criterion we employ is referred to as the average silhouette width (ASW) criterion <span class="citation" data-cites="Rousseeuw1987">[<a href="#ref-Rousseeuw1987" role="doc-biblioref">56</a>]</span>, which is routinely used to assess the cohesion and separation of the clusters uncovered by dissimilarity-based methods. Cohesion refers to the tendency to group similar objects together and separation refers to the tendency to group dissimilar objects apart in non-overlapping clusters. As the name implies, the ASW is computed as the average of observation-specific silhouette widths. Under the assumption that <span class="math inline">\(K&gt;1\)</span>, silhouette widths and the ASW criterion are calculated as follows:</p>
<ol type="A">
<li><p>Let <span class="math inline">\(a(i)\)</span> be the average dissimilarity from observation <span class="math inline">\(i\)</span> to the other members of the same cluster to which observation <span class="math inline">\(i\)</span> is assigned.</p></li>
<li><p>Compute the average dissimilarity from observation <span class="math inline">\(i\)</span> to the members of all <span class="math inline">\(K-1\)</span> other clusters: let <span class="math inline">\(b(i)\)</span> be the minimum such distance computed.</p></li>
<li><p>The silhouette for observation <span class="math inline">\(i\)</span> is then defined to be <span class="math display">\[\begin{align*}
  s(i) &amp;= \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\\
  &amp;= \begin{cases} 1 - \frac{a(i)}{b(i)} &amp; \mbox{if}\:a(i) &lt; b(i)\\
  0&amp; \mbox{if}\:a(i)=b(i)\\
  \frac{b(i)}{a(i)} - 1 &amp; \mbox{if}\:a(i) &gt; b(i),
  \end{cases}
  \end{align*}\]</span> unless observation <span class="math inline">\(i\)</span> is assigned to a cluster of size <span class="math inline">\(1\)</span>, in which case <span class="math inline">\(s(i)=0\)</span>. Notably, <span class="math inline">\(a(i)\)</span> and <span class="math inline">\(b(i)\)</span> need not be calculated using the same dissimilarity measure with which the data were clustered; it is common to adopt the Euclidean distance.</p></li>
<li><p>Define the ASW for a given partition <span class="math inline">\(\mathcal{C}\)</span> as: <span class="math inline">\(\mbox{ASW}\left(\mathcal{C}\right)=\frac{1}{n}\sum_{i=1}^n s_i.\)</span></p></li>
</ol>
<p>Given that <span class="math inline">\(-1 \le s(i) \le 1\)</span>, the interpretation of <span class="math inline">\(s(i)\)</span> is that a silhouette close to <span class="math inline">\(1\)</span> indicates that the observation has been well-clustered, a silhouette close to <span class="math inline">\(-1\)</span> indicates that the observation would be more appropriately assigned to another cluster, and a silhouette close to zero indicates that the observation lies on the boundary of two natural clusters. If most values of <span class="math inline">\(s(i)\)</span> are high, then the clustering solution can be deemed appropriate. This occurs when <span class="math inline">\(a(i) \ll b(i)\)</span>, meaning that observation <span class="math inline">\(i\)</span> must be well-matched its own cluster and poorly-matched to all other clusters. Conversely, if most <span class="math inline">\(s(i)\)</span> values are low or even negative, this provides evidence that <span class="math inline">\(K\)</span> may be too low or too high.</p>
<p>The values of <span class="math inline">\(s(i)\)</span> can be averaged over all observations assigned to the same cluster, as a measure of how tightly grouped the observations in the given cluster are. The ASW criterion itself is simply the mean silhouette width over all observations in the entire data set. Generally, clustering solutions with higher ASW are to be preferred, however it is also prudent to dismiss solutions with many negative silhouettes or particularly low cluster-specific mean silhouettes. A silhouette plot can help in this regard; such a plot depicts all values of <span class="math inline">\(s(i)\)</span>, grouped according to the corresponding cluster and in decreasing order within a cluster. In the R tutorial which follows, ASW values and silhouette plots will guide the choice of an optimal clustering solution in a comparison of multiple methods in <a href="#sec-sil"><span>Section&nbsp;8.4.2.4</span></a>.</p>
</section>
</section>
<section id="sec-tutR" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-tutR"><span class="header-section-number">4</span> Tutorial with R</h2>
<p>In this section, we will learn how to perform clustering using the R programming language <span class="citation" data-cites="R2023">[<a href="#ref-R2023" role="doc-biblioref">7</a>]</span>, using all methods described throughout <a href="#sec-methods"><span>Section&nbsp;8.3</span></a>. We start by loading the necessary libraries. We will use <code>cluster</code> <span class="citation" data-cites="cluster2022">[<a href="#ref-cluster2022" role="doc-biblioref">36</a>]</span> chiefly for functions related to <span class="math inline">\(K\)</span>-Medoids and silhouettes. As per other chapters in this book, we use <code>tidyverse</code> <span class="citation" data-cites="tidyverse2019">[<a href="#ref-tidyverse2019" role="doc-biblioref">57</a>]</span> for data manipulation and <code>rio</code> <span class="citation" data-cites="rio2023">[<a href="#ref-rio2023" role="doc-biblioref">58</a>]</span> for downloading the data: see <a href="../ch12-markov/ch12-markov.html#sec-process"><span>Section&nbsp;12.4.3</span></a> for details the on the data pre-processing steps employed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rio)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We note that hierarchical clustering and <span class="math inline">\(K\)</span>-Means are implemented in base R and thus no dedicated libraries need to be loaded.</p>
<section id="sec-data" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sec-data"><span class="header-section-number">4.1</span> The data set</h3>
<p>Our case study will be to identify different groups of participants that have a similar role in the discussion forum of a massive open online course (MOOC) for teachers. For that purpose, we will rely on the centrality measures of the participants which indicate their number of contributions (<code>OutDegree</code>), replies (<code>InDegree</code>), position in the network (<code>Closeness_total</code>), worth of their connections (<code>Eigen</code>), spread of their ideas (<code>Diffusion_degree</code>), and more. For more details about the data set, please refer to the data chapter of the book (Chapter 2; <span class="citation" data-cites="Lopez-Pernas2024-dat">[<a href="#ref-Lopez-Pernas2024-dat" role="doc-biblioref">59</a>]</span>). To learn more about centrality measures and how to calculate them, refer to the social network analysis chapter (Chapter 15; <span class="citation" data-cites="Saqr2024-sna">[<a href="#ref-Saqr2024-sna" role="doc-biblioref">60</a>]</span>). We will henceforth refer to these data as “the MOOC centralities data set”. We can download and preview the data with the following commands:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>URL <span class="ot">&lt;-</span> <span class="st">"https://github.com/lamethods/data/raw/main/6_snaMOOC/"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="fu">paste0</span>(URL, <span class="st">"Centralities.csv"</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["name"],"name":[1],"type":["int"],"align":["right"]},{"label":["InDegree"],"name":[2],"type":["int"],"align":["right"]},{"label":["OutDegree"],"name":[3],"type":["int"],"align":["right"]},{"label":["Closeness_total"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Betweenness"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Eigen"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Diffusion.degree"],"name":[7],"type":["int"],"align":["right"]},{"label":["Coreness"],"name":[8],"type":["int"],"align":["right"]},{"label":["Cross_clique_connectivity"],"name":[9],"type":["int"],"align":["right"]}],"data":[{"1":"1","2":"20","3":"33","4":"0.0010952903","5":"1258.1431850","6":"0.2055232624","7":"1865","8":"18","9":"305","_rn_":"1"},{"1":"2","2":"2","3":"5","4":"0.0008084074","5":"26.5242892","6":"0.0107177894","7":"218","8":"6","9":"13","_rn_":"2"},{"1":"3","2":"2","3":"4","4":"0.0007987220","5":"30.6011204","6":"0.0086239241","7":"191","8":"6","9":"11","_rn_":"3"},{"1":"4","2":"2","3":"14","4":"0.0010193680","5":"72.5234541","6":"0.0802648338","7":"965","8":"13","9":"37","_rn_":"4"},{"1":"5","2":"16","3":"17","4":"0.0010604454","5":"309.0327391","6":"0.1615036536","7":"1508","8":"18","9":"154","_rn_":"5"},{"1":"6","2":"9","3":"24","4":"0.0010764263","5":"250.3765497","6":"0.1550094982","7":"1607","8":"18","9":"141","_rn_":"6"},{"1":"7","2":"32","3":"26","4":"0.0011135857","5":"1934.7200566","6":"0.2302041085","7":"2088","8":"21","9":"588","_rn_":"7"},{"1":"8","2":"13","3":"18","4":"0.0010604454","5":"163.7078956","6":"0.1363902568","7":"1483","8":"18","9":"131","_rn_":"8"},{"1":"9","2":"2","3":"12","4":"0.0010395010","5":"69.5153192","6":"0.1193482282","7":"1216","8":"13","9":"50","_rn_":"9"},{"1":"10","2":"8","3":"12","4":"0.0010582011","5":"716.3507873","6":"0.0874871529","7":"1432","8":"17","9":"88","_rn_":"10"},{"1":"11","2":"47","3":"74","4":"0.0011415525","5":"1030.1256784","6":"0.5367529235","7":"2694","8":"31","9":"1423","_rn_":"11"},{"1":"12","2":"6","3":"18","4":"0.0009017133","5":"113.9356190","6":"0.0792423386","7":"896","8":"18","9":"71","_rn_":"12"},{"1":"13","2":"20","3":"15","4":"0.0010504202","5":"396.2736532","6":"0.0814983741","7":"1420","8":"18","9":"241","_rn_":"13"},{"1":"14","2":"5","3":"20","4":"0.0010615711","5":"323.5015525","6":"0.0851784950","7":"1583","8":"18","9":"204","_rn_":"14"},{"1":"15","2":"13","3":"18","4":"0.0010810811","5":"824.0809988","6":"0.0918144938","7":"1632","8":"17","9":"174","_rn_":"15"},{"1":"16","2":"2","3":"1","4":"0.0007610350","5":"2.4596368","6":"0.0085424677","7":"130","8":"3","9":"5","_rn_":"16"},{"1":"17","2":"2","3":"22","4":"0.0010649627","5":"292.6257167","6":"0.0822010944","7":"1573","8":"17","9":"221","_rn_":"17"},{"1":"18","2":"0","3":"19","4":"0.0010482180","5":"125.1398610","6":"0.0681777721","7":"1243","8":"12","9":"62","_rn_":"18"},{"1":"19","2":"44","3":"35","4":"0.0011135857","5":"3477.0829186","6":"0.3246377117","7":"1940","8":"26","9":"364","_rn_":"19"},{"1":"20","2":"1","3":"2","4":"0.0007535795","5":"9.9781104","6":"0.0101110123","7":"94","8":"3","9":"3","_rn_":"20"},{"1":"21","2":"2","3":"0","4":"0.0007524454","5":"0.7902310","6":"0.0036688846","7":"78","8":"2","9":"3","_rn_":"21"},{"1":"22","2":"8","3":"15","4":"0.0010504202","5":"97.1915580","6":"0.1682192728","7":"1431","8":"17","9":"90","_rn_":"22"},{"1":"23","2":"6","3":"0","4":"0.0007874016","5":"5.4497380","6":"0.0180802121","7":"284","8":"6","9":"16","_rn_":"23"},{"1":"24","2":"21","3":"36","4":"0.0011123471","5":"1388.2905421","6":"0.2551222942","7":"2092","8":"21","9":"553","_rn_":"24"},{"1":"25","2":"6","3":"6","4":"0.0009900990","5":"215.0169840","6":"0.0528201458","7":"740","8":"10","9":"12","_rn_":"25"},{"1":"26","2":"22","3":"12","4":"0.0010672359","5":"853.1151654","6":"0.0890963591","7":"1617","8":"18","9":"194","_rn_":"26"},{"1":"27","2":"13","3":"18","4":"0.0010427529","5":"990.4338200","6":"0.1110229843","7":"1273","8":"17","9":"143","_rn_":"27"},{"1":"28","2":"0","3":"1","4":"0.0007220217","5":"0.0000000","6":"0.0013039917","7":"21","8":"1","9":"2","_rn_":"28"},{"1":"29","2":"12","3":"23","4":"0.0010845987","5":"1758.3222101","6":"0.0910494829","7":"1690","8":"17","9":"234","_rn_":"29"},{"1":"30","2":"32","3":"58","4":"0.0011185682","5":"1082.0392103","6":"0.4008891477","7":"2224","8":"28","9":"624","_rn_":"30"},{"1":"31","2":"0","3":"1","4":"0.0007342144","5":"0.0000000","6":"0.0013570881","7":"36","8":"1","9":"2","_rn_":"31"},{"1":"32","2":"11","3":"4","4":"0.0009940358","5":"348.1051140","6":"0.0254295297","7":"775","8":"10","9":"31","_rn_":"32"},{"1":"33","2":"17","3":"6","4":"0.0010214505","5":"263.0572324","6":"0.0553454645","7":"1020","8":"13","9":"69","_rn_":"33"},{"1":"34","2":"17","3":"22","4":"0.0010764263","5":"568.8484248","6":"0.1368169776","7":"1834","8":"21","9":"418","_rn_":"34"},{"1":"35","2":"15","3":"19","4":"0.0010416667","5":"669.3602403","6":"0.1322268526","7":"1280","8":"18","9":"112","_rn_":"35"},{"1":"36","2":"26","3":"21","4":"0.0011111111","5":"2369.6371414","6":"0.1554575978","7":"1785","8":"18","9":"265","_rn_":"36"},{"1":"37","2":"7","3":"4","4":"0.0008130081","5":"56.0357925","6":"0.0159166865","7":"314","8":"9","9":"23","_rn_":"37"},{"1":"38","2":"7","3":"2","4":"0.0010070493","5":"52.4812897","6":"0.0240341490","7":"771","8":"9","9":"22","_rn_":"38"},{"1":"39","2":"5","3":"13","4":"0.0010504202","5":"318.4100709","6":"0.1123570051","7":"1321","8":"14","9":"74","_rn_":"39"},{"1":"40","2":"0","3":"1","4":"0.0006269592","5":"0.0000000","6":"0.0009526790","7":"36","8":"1","9":"2","_rn_":"40"},{"1":"41","2":"29","3":"6","4":"0.0008658009","5":"647.9906068","6":"0.0639169509","7":"762","8":"18","9":"89","_rn_":"41"},{"1":"42","2":"10","3":"9","4":"0.0008291874","5":"115.9592816","6":"0.0481324558","7":"418","8":"12","9":"23","_rn_":"42"},{"1":"43","2":"1","3":"7","4":"0.0007917656","5":"13.5338944","6":"0.0134537327","7":"250","8":"7","9":"14","_rn_":"43"},{"1":"44","2":"47","3":"70","4":"0.0011494253","5":"3068.9224639","6":"0.4682891907","7":"2608","8":"31","9":"1168","_rn_":"44"},{"1":"45","2":"1","3":"2","4":"0.0009900990","5":"29.4644832","6":"0.0193875218","7":"684","8":"3","9":"5","_rn_":"45"},{"1":"46","2":"2","3":"9","4":"0.0008810573","5":"130.2540153","6":"0.0191762968","7":"525","8":"8","9":"16","_rn_":"46"},{"1":"47","2":"0","3":"5","4":"0.0008554320","5":"15.5516046","6":"0.0166120276","7":"481","8":"5","9":"11","_rn_":"47"},{"1":"48","2":"2","3":"4","4":"0.0009871668","5":"51.7756650","6":"0.0187293424","7":"651","8":"6","9":"8","_rn_":"48"},{"1":"49","2":"19","3":"23","4":"0.0010810811","5":"1250.7696743","6":"0.1485283643","7":"1819","8":"18","9":"307","_rn_":"49"},{"1":"50","2":"16","3":"14","4":"0.0010626993","5":"484.3529452","6":"0.0999589521","7":"1598","8":"18","9":"160","_rn_":"50"},{"1":"51","2":"6","3":"13","4":"0.0010395010","5":"32.6934911","6":"0.0673800053","7":"1185","8":"18","9":"43","_rn_":"51"},{"1":"52","2":"4","3":"13","4":"0.0008944544","5":"88.1521237","6":"0.0427412033","7":"728","8":"14","9":"64","_rn_":"52"},{"1":"53","2":"19","3":"16","4":"0.0009337068","5":"682.3111940","6":"0.0694507887","7":"1323","8":"18","9":"336","_rn_":"53"},{"1":"54","2":"14","3":"28","4":"0.0010834236","5":"1345.3974363","6":"0.1479616291","7":"1744","8":"18","9":"255","_rn_":"54"},{"1":"55","2":"0","3":"4","4":"0.0008382230","5":"1.2298762","6":"0.0226620069","7":"361","8":"4","9":"5","_rn_":"55"},{"1":"56","2":"4","3":"20","4":"0.0009099181","5":"222.3806295","6":"0.0976355523","7":"790","8":"14","9":"75","_rn_":"56"},{"1":"57","2":"7","3":"4","4":"0.0009891197","5":"305.8704317","6":"0.0369728967","7":"739","8":"9","9":"13","_rn_":"57"},{"1":"58","2":"23","3":"16","4":"0.0010822511","5":"524.5896242","6":"0.1246945448","7":"1836","8":"20","9":"367","_rn_":"58"},{"1":"59","2":"7","3":"14","4":"0.0010649627","5":"971.3848297","6":"0.0581854175","7":"1269","8":"12","9":"61","_rn_":"59"},{"1":"60","2":"25","3":"40","4":"0.0011025358","5":"2177.6429106","6":"0.2368267894","7":"2109","8":"24","9":"526","_rn_":"60"},{"1":"61","2":"29","3":"24","4":"0.0010857763","5":"2184.5120982","6":"0.2241756310","7":"1770","8":"21","9":"326","_rn_":"61"},{"1":"62","2":"16","3":"17","4":"0.0010718114","5":"133.2216585","6":"0.1254680380","7":"1798","8":"18","9":"455","_rn_":"62"},{"1":"63","2":"11","3":"25","4":"0.0010537408","5":"109.5871078","6":"0.1553457884","7":"1480","8":"18","9":"151","_rn_":"63"},{"1":"64","2":"26","3":"19","4":"0.0010706638","5":"1835.1876540","6":"0.1217718194","7":"1623","8":"18","9":"288","_rn_":"64"},{"1":"65","2":"1","3":"2","4":"0.0009803922","5":"0.0000000","6":"0.0185349570","7":"629","8":"3","9":"4","_rn_":"65"},{"1":"66","2":"4","3":"4","4":"0.0010362694","5":"232.7577008","6":"0.0334718629","7":"1066","8":"7","9":"28","_rn_":"66"},{"1":"67","2":"21","3":"9","4":"0.0010810811","5":"961.2135898","6":"0.0856583118","7":"1645","8":"15","9":"207","_rn_":"67"},{"1":"68","2":"21","3":"40","4":"0.0010764263","5":"374.4623769","6":"0.3405814112","7":"1942","8":"28","9":"383","_rn_":"68"},{"1":"69","2":"4","3":"6","4":"0.0010330579","5":"522.1215068","6":"0.0347591675","7":"1087","8":"8","9":"24","_rn_":"69"},{"1":"70","2":"3","3":"1","4":"0.0009832842","5":"46.6600386","6":"0.0179186564","7":"651","8":"4","9":"7","_rn_":"70"},{"1":"71","2":"1","3":"12","4":"0.0010245902","5":"25.1852945","6":"0.0991316666","7":"962","8":"9","9":"16","_rn_":"71"},{"1":"72","2":"3","3":"3","4":"0.0009881423","5":"40.3550087","6":"0.0342750551","7":"666","8":"5","9":"9","_rn_":"72"},{"1":"73","2":"0","3":"2","4":"0.0007309942","5":"7.8528133","6":"0.0017029045","7":"41","8":"2","9":"3","_rn_":"73"},{"1":"74","2":"6","3":"10","4":"0.0008771930","5":"284.1545938","6":"0.0249516706","7":"614","8":"10","9":"29","_rn_":"74"},{"1":"75","2":"3","3":"7","4":"0.0010351967","5":"198.3385407","6":"0.0482709864","7":"1087","8":"8","9":"32","_rn_":"75"},{"1":"76","2":"1","3":"4","4":"0.0009813543","5":"56.5269743","6":"0.0194673530","7":"677","8":"5","9":"12","_rn_":"76"},{"1":"77","2":"3","3":"8","4":"0.0010288066","5":"58.8542150","6":"0.0415561810","7":"1048","8":"9","9":"24","_rn_":"77"},{"1":"78","2":"3","3":"3","4":"0.0009881423","5":"35.9312246","6":"0.0344712126","7":"697","8":"5","9":"13","_rn_":"78"},{"1":"79","2":"0","3":"4","4":"0.0007363770","5":"11.1939611","6":"0.0032643760","7":"75","8":"4","9":"7","_rn_":"79"},{"1":"80","2":"1","3":"6","4":"0.0008203445","5":"61.6404817","6":"0.0209536405","7":"348","8":"5","9":"11","_rn_":"80"},{"1":"81","2":"3","3":"6","4":"0.0009960159","5":"260.7955917","6":"0.0212576197","7":"711","8":"6","9":"19","_rn_":"81"},{"1":"82","2":"1","3":"6","4":"0.0010277492","5":"256.9924053","6":"0.0293885075","7":"997","8":"6","9":"16","_rn_":"82"},{"1":"83","2":"1","3":"13","4":"0.0010526316","5":"153.0658241","6":"0.0790166252","7":"1187","8":"11","9":"37","_rn_":"83"},{"1":"84","2":"5","3":"0","4":"0.0007524454","5":"445.1039951","6":"0.0048074022","7":"81","8":"4","9":"6","_rn_":"84"},{"1":"85","2":"1","3":"6","4":"0.0010330579","5":"274.5465215","6":"0.0292892559","7":"1002","8":"5","9":"15","_rn_":"85"},{"1":"86","2":"0","3":"1","4":"0.0005652911","5":"0.0000000","6":"0.0000716541","7":"6","8":"1","9":"2","_rn_":"86"},{"1":"87","2":"3","3":"9","4":"0.0010482180","5":"387.5102444","6":"0.0656313725","7":"1209","8":"10","9":"33","_rn_":"87"},{"1":"88","2":"16","3":"14","4":"0.0010548523","5":"675.5725548","6":"0.1096442277","7":"1415","8":"18","9":"157","_rn_":"88"},{"1":"89","2":"0","3":"1","4":"0.0007204611","5":"0.0000000","6":"0.0016342418","7":"31","8":"1","9":"2","_rn_":"89"},{"1":"90","2":"1","3":"3","4":"0.0009784736","5":"77.9221439","6":"0.0191879706","7":"630","8":"4","9":"6","_rn_":"90"},{"1":"91","2":"7","3":"11","4":"0.0010384216","5":"353.9543387","6":"0.0475815700","7":"1256","8":"12","9":"84","_rn_":"91"},{"1":"92","2":"17","3":"16","4":"0.0010729614","5":"972.0850348","6":"0.0799758753","7":"1590","8":"18","9":"241","_rn_":"92"},{"1":"93","2":"1","3":"2","4":"0.0007331378","5":"3.9856015","6":"0.0019786851","7":"41","8":"3","9":"4","_rn_":"93"},{"1":"94","2":"6","3":"1","4":"0.0010204082","5":"305.7537756","6":"0.0438144242","7":"973","8":"5","9":"17","_rn_":"94"},{"1":"95","2":"4","3":"3","4":"0.0009784736","5":"482.5073366","6":"0.0180067198","7":"617","8":"5","9":"13","_rn_":"95"},{"1":"96","2":"0","3":"12","4":"0.0010341262","5":"50.6176824","6":"0.0839651399","7":"987","8":"8","9":"22","_rn_":"96"},{"1":"97","2":"0","3":"5","4":"0.0008438819","5":"179.7650960","6":"0.0128118347","7":"372","8":"4","9":"8","_rn_":"97"},{"1":"98","2":"16","3":"12","4":"0.0010330579","5":"98.0851626","6":"0.0885073388","7":"1295","8":"18","9":"195","_rn_":"98"},{"1":"99","2":"3","3":"3","4":"0.0010330579","5":"48.9651050","6":"0.0343347235","7":"1097","8":"6","9":"15","_rn_":"99"},{"1":"100","2":"22","3":"7","4":"0.0010741139","5":"829.5885066","6":"0.0974449345","7":"1511","8":"14","9":"176","_rn_":"100"},{"1":"101","2":"9","3":"3","4":"0.0010384216","5":"286.7287890","6":"0.0334968424","7":"1076","8":"7","9":"31","_rn_":"101"},{"1":"102","2":"0","3":"6","4":"0.0010266940","5":"0.0000000","6":"0.0555380035","7":"1006","8":"6","9":"16","_rn_":"102"},{"1":"103","2":"5","3":"9","4":"0.0010330579","5":"460.2065860","6":"0.0594344802","7":"1048","8":"10","9":"34","_rn_":"103"},{"1":"104","2":"6","3":"15","4":"0.0009124088","5":"321.5050214","6":"0.0430348022","7":"798","8":"10","9":"50","_rn_":"104"},{"1":"105","2":"2","3":"3","4":"0.0009940358","5":"25.2618419","6":"0.0205745242","7":"708","8":"5","9":"13","_rn_":"105"},{"1":"106","2":"2","3":"4","4":"0.0008051530","5":"10.8406570","6":"0.0163664056","7":"289","8":"6","9":"11","_rn_":"106"},{"1":"107","2":"4","3":"15","4":"0.0010101010","5":"241.3700473","6":"0.0356575372","7":"891","8":"11","9":"44","_rn_":"107"},{"1":"108","2":"0","3":"4","4":"0.0007733952","5":"2.1090963","6":"0.0070117896","7":"143","8":"4","9":"8","_rn_":"108"},{"1":"109","2":"4","3":"18","4":"0.0010245902","5":"954.9946478","6":"0.0967498757","7":"980","8":"11","9":"39","_rn_":"109"},{"1":"110","2":"2","3":"2","4":"0.0009832842","5":"443.7347692","6":"0.0167843259","7":"620","8":"3","9":"7","_rn_":"110"},{"1":"111","2":"0","3":"1","4":"0.0006863418","5":"0.0000000","6":"0.0002501696","7":"5","8":"1","9":"2","_rn_":"111"},{"1":"112","2":"3","3":"2","4":"0.0009842520","5":"227.0754458","6":"0.0202573996","7":"671","8":"5","9":"9","_rn_":"112"},{"1":"113","2":"4","3":"6","4":"0.0009950249","5":"61.1674899","6":"0.0230728887","7":"719","8":"6","9":"26","_rn_":"113"},{"1":"114","2":"6","3":"5","4":"0.0008928571","5":"29.9737396","6":"0.0356927379","7":"642","8":"9","9":"37","_rn_":"114"},{"1":"115","2":"20","3":"6","4":"0.0010373444","5":"1699.6077221","6":"0.0922388296","7":"985","8":"13","9":"42","_rn_":"115"},{"1":"116","2":"32","3":"3","4":"0.0010893246","5":"2890.6619280","6":"0.0563781883","7":"1416","8":"14","9":"125","_rn_":"116"},{"1":"117","2":"5","3":"1","4":"0.0007987220","5":"19.7901475","6":"0.0207779040","7":"314","8":"6","9":"14","_rn_":"117"},{"1":"118","2":"1","3":"5","4":"0.0007547170","5":"24.1764397","6":"0.0048142608","7":"94","8":"5","9":"7","_rn_":"118"},{"1":"119","2":"5","3":"2","4":"0.0007800312","5":"126.0135194","6":"0.0060411570","7":"144","8":"6","9":"11","_rn_":"119"},{"1":"120","2":"1","3":"2","4":"0.0009813543","5":"440.0000000","6":"0.0157487639","7":"620","8":"2","9":"5","_rn_":"120"},{"1":"121","2":"1","3":"10","4":"0.0010416667","5":"44.9144907","6":"0.0630623146","7":"1161","8":"9","9":"38","_rn_":"121"},{"1":"122","2":"2","3":"1","4":"0.0007530120","5":"8.9998322","6":"0.0025060641","7":"63","8":"3","9":"4","_rn_":"122"},{"1":"123","2":"0","3":"5","4":"0.0007656968","5":"482.6751534","6":"0.0044268576","7":"100","8":"3","9":"6","_rn_":"123"},{"1":"124","2":"0","3":"2","4":"0.0009794319","5":"0.0000000","6":"0.0157452652","7":"618","8":"2","9":"4","_rn_":"124"},{"1":"125","2":"0","3":"1","4":"0.0007363770","5":"0.0000000","6":"0.0008403141","7":"36","8":"1","9":"2","_rn_":"125"},{"1":"126","2":"0","3":"3","4":"0.0009794319","5":"0.0000000","6":"0.0306502162","7":"619","8":"3","9":"4","_rn_":"126"},{"1":"127","2":"0","3":"2","4":"0.0009794319","5":"0.0000000","6":"0.0157452652","7":"618","8":"2","9":"4","_rn_":"127"},{"1":"128","2":"10","3":"14","4":"0.0010266940","5":"411.8043778","6":"0.0513999888","7":"994","8":"14","9":"51","_rn_":"128"},{"1":"129","2":"6","3":"8","4":"0.0010000000","5":"108.4983277","6":"0.0517922910","7":"733","8":"10","9":"20","_rn_":"129"},{"1":"130","2":"0","3":"1","4":"0.0007363770","5":"0.0000000","6":"0.0008403141","7":"36","8":"1","9":"2","_rn_":"130"},{"1":"131","2":"1","3":"4","4":"0.0009852217","5":"2.8975473","6":"0.0333357647","7":"672","8":"5","9":"8","_rn_":"131"},{"1":"132","2":"4","3":"2","4":"0.0007949126","5":"87.1025412","6":"0.0082675182","7":"164","8":"5","9":"8","_rn_":"132"},{"1":"133","2":"2","3":"7","4":"0.0009970090","5":"7.7272429","6":"0.0447899909","7":"731","8":"7","9":"10","_rn_":"133"},{"1":"134","2":"0","3":"2","4":"0.0009727626","5":"52.1522217","6":"0.0150281780","7":"589","8":"2","9":"3","_rn_":"134"},{"1":"135","2":"0","3":"2","4":"0.0007022472","5":"3.8803538","6":"0.0005796703","7":"17","8":"2","9":"3","_rn_":"135"},{"1":"136","2":"12","3":"13","4":"0.0010449321","5":"458.4896246","6":"0.0875081311","7":"1238","8":"16","9":"64","_rn_":"136"},{"1":"137","2":"22","3":"8","4":"0.0010718114","5":"978.1210929","6":"0.1056889689","7":"1580","8":"17","9":"173","_rn_":"137"},{"1":"138","2":"3","3":"6","4":"0.0010266940","5":"54.8776845","6":"0.0521602724","7":"1004","8":"8","9":"18","_rn_":"138"},{"1":"139","2":"0","3":"4","4":"0.0009794319","5":"11.2712265","6":"0.0178767452","7":"641","8":"4","9":"7","_rn_":"139"},{"1":"140","2":"0","3":"3","4":"0.0007097232","5":"0.2500000","6":"0.0015598988","7":"48","8":"3","9":"6","_rn_":"140"},{"1":"141","2":"2","3":"3","4":"0.0009803922","5":"171.2448612","6":"0.0315235328","7":"640","8":"4","9":"10","_rn_":"141"},{"1":"142","2":"5","3":"9","4":"0.0010405827","5":"170.2087175","6":"0.0685809621","7":"1201","8":"11","9":"40","_rn_":"142"},{"1":"143","2":"0","3":"6","4":"0.0009861933","5":"0.3495421","6":"0.0489224233","7":"676","8":"6","9":"12","_rn_":"143"},{"1":"144","2":"9","3":"12","4":"0.0010080645","5":"92.4113176","6":"0.0507191236","7":"966","8":"17","9":"38","_rn_":"144"},{"1":"145","2":"1","3":"1","4":"0.0007336757","5":"10.2456310","6":"0.0009332329","7":"28","8":"2","9":"3","_rn_":"145"},{"1":"146","2":"1","3":"2","4":"0.0010162602","5":"24.3588245","6":"0.0255955946","7":"923","8":"3","9":"6","_rn_":"146"},{"1":"147","2":"1","3":"12","4":"0.0010384216","5":"377.3284116","6":"0.0390849360","7":"1169","8":"9","9":"32","_rn_":"147"},{"1":"148","2":"0","3":"8","4":"0.0008620690","5":"13.7625434","6":"0.0188352534","7":"478","8":"8","9":"20","_rn_":"148"},{"1":"149","2":"0","3":"1","4":"0.0006825939","5":"0.0000000","6":"0.0004445273","7":"5","8":"1","9":"2","_rn_":"149"},{"1":"150","2":"2","3":"2","4":"0.0009756098","5":"490.3497550","6":"0.0298241369","7":"588","8":"2","9":"4","_rn_":"150"},{"1":"151","2":"1","3":"1","4":"0.0006944444","5":"20.3881741","6":"0.0005105094","7":"11","8":"2","9":"3","_rn_":"151"},{"1":"152","2":"3","3":"2","4":"0.0009861933","5":"141.1772594","6":"0.0199401458","7":"682","8":"4","9":"11","_rn_":"152"},{"1":"153","2":"3","3":"0","4":"0.0007235890","5":"1.7488502","6":"0.0023026212","7":"36","8":"3","9":"4","_rn_":"153"},{"1":"154","2":"4","3":"6","4":"0.0010298661","5":"312.6869558","6":"0.0620743601","7":"1048","8":"8","9":"25","_rn_":"154"},{"1":"155","2":"9","3":"9","4":"0.0010482180","5":"587.6814506","6":"0.0724724968","7":"1302","8":"12","9":"73","_rn_":"155"},{"1":"156","2":"1","3":"2","4":"0.0007782101","5":"6.9262318","6":"0.0066962098","7":"133","8":"3","9":"5","_rn_":"156"},{"1":"157","2":"3","3":"8","4":"0.0010319917","5":"80.5753815","6":"0.0719291535","7":"1053","8":"9","9":"25","_rn_":"157"},{"1":"158","2":"8","3":"7","4":"0.0010020040","5":"80.8673604","6":"0.0833330069","7":"741","8":"10","9":"16","_rn_":"158"},{"1":"159","2":"1","3":"4","4":"0.0008481764","5":"27.1955419","6":"0.0140727593","7":"398","8":"4","9":"9","_rn_":"159"},{"1":"160","2":"0","3":"3","4":"0.0009756098","5":"298.2103338","6":"0.0161681095","7":"601","8":"2","9":"5","_rn_":"160"},{"1":"161","2":"6","3":"12","4":"0.0010235415","5":"171.3821846","6":"0.0508329345","7":"978","8":"14","9":"42","_rn_":"161"},{"1":"162","2":"4","3":"3","4":"0.0010214505","5":"34.7075480","6":"0.0437987803","7":"992","8":"7","9":"14","_rn_":"162"},{"1":"163","2":"2","3":"10","4":"0.0010373444","5":"247.6296528","6":"0.0619569322","7":"1218","8":"11","9":"42","_rn_":"163"},{"1":"164","2":"0","3":"1","4":"0.0007102273","5":"0.0000000","6":"0.0008858680","7":"15","8":"1","9":"2","_rn_":"164"},{"1":"165","2":"3","3":"5","4":"0.0010080645","5":"202.9417906","6":"0.0290158123","7":"854","8":"6","9":"26","_rn_":"165"},{"1":"166","2":"0","3":"6","4":"0.0010224949","5":"105.2524755","6":"0.0373375424","7":"951","8":"6","9":"13","_rn_":"166"},{"1":"167","2":"11","3":"3","4":"0.0010030090","5":"1885.6398810","6":"0.0560607838","7":"873","8":"7","9":"55","_rn_":"167"},{"1":"168","2":"0","3":"2","4":"0.0007097232","5":"0.0000000","6":"0.0016903805","7":"53","8":"2","9":"4","_rn_":"168"},{"1":"169","2":"0","3":"5","4":"0.0009794319","5":"0.0000000","6":"0.0323550799","7":"637","8":"5","9":"8","_rn_":"169"},{"1":"170","2":"2","3":"7","4":"0.0009881423","5":"40.0301424","6":"0.0348557849","7":"670","8":"7","9":"16","_rn_":"170"},{"1":"171","2":"1","3":"6","4":"0.0009813543","5":"440.0000000","6":"0.0472705324","7":"640","8":"6","9":"9","_rn_":"171"},{"1":"172","2":"2","3":"6","4":"0.0009920635","5":"30.5377976","6":"0.0523325237","7":"721","8":"7","9":"12","_rn_":"172"},{"1":"173","2":"3","3":"6","4":"0.0009950249","5":"232.5688954","6":"0.0212902804","7":"704","8":"6","9":"23","_rn_":"173"},{"1":"174","2":"1","3":"1","4":"0.0007178751","5":"0.3333333","6":"0.0008041698","7":"16","8":"2","9":"3","_rn_":"174"},{"1":"175","2":"5","3":"0","4":"0.0007547170","5":"43.3982491","6":"0.0069637572","7":"120","8":"5","9":"9","_rn_":"175"},{"1":"176","2":"6","3":"8","4":"0.0010341262","5":"229.9712484","6":"0.0787377717","7":"1088","8":"11","9":"34","_rn_":"176"},{"1":"177","2":"9","3":"6","4":"0.0009066183","5":"126.6449910","6":"0.0629222554","7":"767","8":"11","9":"35","_rn_":"177"},{"1":"178","2":"3","3":"5","4":"0.0009861933","5":"260.1443389","6":"0.0183758937","7":"653","8":"7","9":"10","_rn_":"178"},{"1":"179","2":"0","3":"6","4":"0.0010214505","5":"64.0872677","6":"0.0272298518","7":"945","8":"5","9":"10","_rn_":"179"},{"1":"180","2":"0","3":"1","4":"0.0006839945","5":"0.0000000","6":"0.0002683893","7":"8","8":"1","9":"2","_rn_":"180"},{"1":"181","2":"2","3":"1","4":"0.0009727626","5":"0.0000000","6":"0.0305675645","7":"602","8":"3","9":"4","_rn_":"181"},{"1":"182","2":"10","3":"0","4":"0.0008116883","5":"78.3674632","6":"0.0180970474","7":"324","8":"8","9":"20","_rn_":"182"},{"1":"183","2":"3","3":"9","4":"0.0008920607","5":"192.4800476","6":"0.0372420007","7":"628","8":"8","9":"26","_rn_":"183"},{"1":"184","2":"3","3":"9","4":"0.0010384216","5":"132.3586657","6":"0.0575771687","7":"1214","8":"9","9":"45","_rn_":"184"},{"1":"185","2":"1","3":"13","4":"0.0010395010","5":"208.1889369","6":"0.0556226290","7":"1161","8":"10","9":"45","_rn_":"185"},{"1":"186","2":"0","3":"2","4":"0.0009718173","5":"65.5148380","6":"0.0156849647","7":"591","8":"2","9":"4","_rn_":"186"},{"1":"187","2":"3","3":"0","4":"0.0009727626","5":"16.3998342","6":"0.0169433333","7":"617","8":"3","9":"6","_rn_":"187"},{"1":"188","2":"1","3":"3","4":"0.0009803922","5":"0.0000000","6":"0.0320239158","7":"621","8":"4","9":"8","_rn_":"188"},{"1":"189","2":"3","3":"0","4":"0.0009737098","5":"0.5400000","6":"0.0158122606","7":"599","8":"3","9":"6","_rn_":"189"},{"1":"190","2":"1","3":"4","4":"0.0010193680","5":"31.2438910","6":"0.0378001435","7":"966","8":"4","9":"8","_rn_":"190"},{"1":"191","2":"1","3":"3","4":"0.0007745933","5":"32.8800677","6":"0.0107367339","7":"144","8":"3","9":"5","_rn_":"191"},{"1":"192","2":"16","3":"2","4":"0.0010235415","5":"1193.6517699","6":"0.0498541830","7":"948","8":"11","9":"35","_rn_":"192"},{"1":"193","2":"11","3":"11","4":"0.0010405827","5":"176.2265128","6":"0.0847202775","7":"1191","8":"16","9":"50","_rn_":"193"},{"1":"194","2":"5","3":"1","4":"0.0009910803","5":"74.3723282","6":"0.0229173250","7":"708","8":"4","9":"13","_rn_":"194"},{"1":"195","2":"8","3":"4","4":"0.0010570825","5":"76.6709219","6":"0.0676492965","7":"1405","8":"11","9":"81","_rn_":"195"},{"1":"196","2":"0","3":"5","4":"0.0009910803","5":"17.5952548","6":"0.0239817322","7":"743","8":"4","9":"10","_rn_":"196"},{"1":"197","2":"1","3":"6","4":"0.0008703220","5":"146.1649322","6":"0.0157551603","7":"444","8":"4","9":"10","_rn_":"197"},{"1":"198","2":"20","3":"16","4":"0.0010660981","5":"290.7078834","6":"0.1042230268","7":"1562","8":"18","9":"200","_rn_":"198"},{"1":"199","2":"6","3":"8","4":"0.0010395010","5":"80.8549619","6":"0.0567562325","7":"1140","8":"10","9":"46","_rn_":"199"},{"1":"200","2":"2","3":"3","4":"0.0009871668","5":"51.3345018","6":"0.0519314819","7":"710","8":"5","9":"5","_rn_":"200"},{"1":"201","2":"11","3":"11","4":"0.0010471204","5":"527.7475944","6":"0.0567464640","7":"1257","8":"14","9":"63","_rn_":"201"},{"1":"202","2":"0","3":"13","4":"0.0010526316","5":"225.6903417","6":"0.0575876420","7":"1352","8":"11","9":"74","_rn_":"202"},{"1":"203","2":"10","3":"2","4":"0.0010070493","5":"295.8091378","6":"0.0537772881","7":"894","8":"10","9":"23","_rn_":"203"},{"1":"204","2":"0","3":"2","4":"0.0009737098","5":"0.0000000","6":"0.0157064989","7":"595","8":"2","9":"4","_rn_":"204"},{"1":"205","2":"5","3":"12","4":"0.0010020040","5":"392.2964639","6":"0.0273158502","7":"789","8":"10","9":"36","_rn_":"205"},{"1":"206","2":"1","3":"2","4":"0.0009756098","5":"46.9467739","6":"0.0164000996","7":"610","8":"3","9":"8","_rn_":"206"},{"1":"207","2":"9","3":"7","4":"0.0009165903","5":"171.3910716","6":"0.0554082574","7":"840","8":"11","9":"65","_rn_":"207"},{"1":"208","2":"5","3":"2","4":"0.0007849294","5":"121.1755284","6":"0.0095822381","7":"189","8":"6","9":"11","_rn_":"208"},{"1":"209","2":"2","3":"1","4":"0.0008403361","5":"26.5912048","6":"0.0115754605","7":"378","8":"3","9":"5","_rn_":"209"},{"1":"210","2":"0","3":"2","4":"0.0008354219","5":"0.0000000","6":"0.0109664706","7":"344","8":"2","9":"4","_rn_":"210"},{"1":"211","2":"6","3":"4","4":"0.0010319917","5":"163.6478396","6":"0.0365124544","7":"1067","8":"9","9":"27","_rn_":"211"},{"1":"212","2":"5","3":"7","4":"0.0010288066","5":"120.9864244","6":"0.0776605067","7":"1152","8":"10","9":"26","_rn_":"212"},{"1":"213","2":"0","3":"1","4":"0.0007423905","5":"0.0000000","6":"0.0035298917","7":"66","8":"1","9":"2","_rn_":"213"},{"1":"214","2":"0","3":"2","4":"0.0009765625","5":"0.0000000","6":"0.0157507543","7":"605","8":"2","9":"4","_rn_":"214"},{"1":"215","2":"0","3":"5","4":"0.0008554320","5":"1.7438446","6":"0.0284214531","7":"451","8":"5","9":"8","_rn_":"215"},{"1":"216","2":"5","3":"2","4":"0.0007836991","5":"66.0314214","6":"0.0158874247","7":"173","8":"5","9":"7","_rn_":"216"},{"1":"217","2":"7","3":"6","4":"0.0010030090","5":"317.5681011","6":"0.0597054394","7":"753","8":"10","9":"18","_rn_":"217"},{"1":"218","2":"3","3":"2","4":"0.0009910803","5":"131.6984210","6":"0.0215004695","7":"705","8":"4","9":"11","_rn_":"218"},{"1":"219","2":"17","3":"13","4":"0.0010309278","5":"1070.3376397","6":"0.0705282998","7":"999","8":"12","9":"64","_rn_":"219"},{"1":"220","2":"0","3":"1","4":"0.0007473842","5":"0.0000000","6":"0.0048387092","7":"80","8":"1","9":"2","_rn_":"220"},{"1":"221","2":"8","3":"2","4":"0.0010162602","5":"208.8539968","6":"0.0375372465","7":"973","8":"7","9":"34","_rn_":"221"},{"1":"222","2":"1","3":"1","4":"0.0007473842","5":"0.0000000","6":"0.0096774184","7":"81","8":"2","9":"2","_rn_":"222"},{"1":"223","2":"24","3":"15","4":"0.0010752688","5":"1113.8768685","6":"0.1578556334","7":"1533","8":"24","9":"147","_rn_":"223"},{"1":"224","2":"0","3":"2","4":"0.0007616146","5":"0.0000000","6":"0.0058247139","7":"111","8":"2","9":"4","_rn_":"224"},{"1":"225","2":"0","3":"1","4":"0.0007473842","5":"0.0000000","6":"0.0048387092","7":"80","8":"1","9":"2","_rn_":"225"},{"1":"226","2":"7","3":"8","4":"0.0010526316","5":"91.9025758","6":"0.0680670392","7":"1256","8":"11","9":"62","_rn_":"226"},{"1":"227","2":"1","3":"1","4":"0.0007473842","5":"0.0000000","6":"0.0096774184","7":"81","8":"2","9":"2","_rn_":"227"},{"1":"228","2":"0","3":"2","4":"0.0008561644","5":"0.0000000","6":"0.0152609634","7":"413","8":"2","9":"4","_rn_":"228"},{"1":"229","2":"0","3":"1","4":"0.0007057163","5":"0.0000000","6":"0.0007430742","7":"19","8":"1","9":"2","_rn_":"229"},{"1":"230","2":"0","3":"3","4":"0.0007087172","5":"4.6995428","6":"0.0011942190","7":"27","8":"2","9":"4","_rn_":"230"},{"1":"231","2":"0","3":"7","4":"0.0007923930","5":"38.9602667","6":"0.0125992653","7":"188","8":"6","9":"10","_rn_":"231"},{"1":"232","2":"0","3":"3","4":"0.0010193680","5":"17.4863252","6":"0.0260702794","7":"934","8":"3","9":"6","_rn_":"232"},{"1":"233","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0008547973","7":"38","8":"1","9":"2","_rn_":"233"},{"1":"234","2":"37","3":"0","4":"0.0010298661","5":"2851.4012937","6":"0.0573498886","7":"1004","8":"10","9":"95","_rn_":"234"},{"1":"235","2":"1","3":"3","4":"0.0010214505","5":"70.9930984","6":"0.0410869536","7":"954","8":"4","9":"6","_rn_":"235"},{"1":"236","2":"0","3":"3","4":"0.0009775171","5":"0.0000000","6":"0.0306646994","7":"621","8":"3","9":"4","_rn_":"236"},{"1":"237","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0008547973","7":"38","8":"1","9":"2","_rn_":"237"},{"1":"238","2":"0","3":"2","4":"0.0007087172","5":"0.0000000","6":"0.0017095946","7":"39","8":"2","9":"2","_rn_":"238"},{"1":"239","2":"1","3":"2","4":"0.0009784736","5":"84.1146373","6":"0.0157829985","7":"624","8":"3","9":"6","_rn_":"239"},{"1":"240","2":"0","3":"2","4":"0.0009775171","5":"0.0000000","6":"0.0157597483","7":"620","8":"2","9":"4","_rn_":"240"},{"1":"241","2":"0","3":"3","4":"0.0009775171","5":"0.0000000","6":"0.0166145456","7":"621","8":"3","9":"4","_rn_":"241"},{"1":"242","2":"0","3":"4","4":"0.0009784736","5":"5.3492631","6":"0.0311842231","7":"631","8":"4","9":"8","_rn_":"242"},{"1":"243","2":"5","3":"3","4":"0.0009940358","5":"927.0811263","6":"0.0328499650","7":"676","8":"6","9":"10","_rn_":"243"},{"1":"244","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0008547973","7":"38","8":"1","9":"2","_rn_":"244"},{"1":"245","2":"0","3":"3","4":"0.0009794319","5":"440.0000000","6":"0.0157632502","7":"622","8":"2","9":"5","_rn_":"245"},{"1":"246","2":"1","3":"5","4":"0.0009852217","5":"5.4026663","6":"0.0486620118","7":"674","8":"6","9":"8","_rn_":"246"},{"1":"247","2":"2","3":"6","4":"0.0010298661","5":"467.6399119","6":"0.0461366771","7":"1035","8":"6","9":"23","_rn_":"247"},{"1":"248","2":"6","3":"3","4":"0.0010060362","5":"706.8709599","6":"0.0521014775","7":"854","8":"8","9":"21","_rn_":"248"},{"1":"249","2":"5","3":"7","4":"0.0010298661","5":"323.7554984","6":"0.0874339172","7":"1080","8":"11","9":"23","_rn_":"249"},{"1":"250","2":"1","3":"3","4":"0.0010193680","5":"48.5477346","6":"0.0268267582","7":"946","8":"4","9":"9","_rn_":"250"},{"1":"251","2":"4","3":"1","4":"0.0009803922","5":"51.4277058","6":"0.0170808745","7":"653","8":"5","9":"8","_rn_":"251"},{"1":"252","2":"2","3":"1","4":"0.0009775171","5":"14.0935595","6":"0.0189942395","7":"631","8":"3","9":"6","_rn_":"252"},{"1":"253","2":"2","3":"1","4":"0.0009775171","5":"37.7957666","6":"0.0308611229","7":"614","8":"3","9":"4","_rn_":"253"},{"1":"254","2":"1","3":"1","4":"0.0007112376","5":"0.3055556","6":"0.0015770901","7":"22","8":"2","9":"3","_rn_":"254"},{"1":"255","2":"0","3":"2","4":"0.0009727626","5":"0.0000000","6":"0.0151624080","7":"588","8":"2","9":"4","_rn_":"255"},{"1":"256","2":"3","3":"2","4":"0.0009823183","5":"35.5354370","6":"0.0172732509","7":"647","8":"4","9":"12","_rn_":"256"},{"1":"257","2":"3","3":"1","4":"0.0009852217","5":"34.7576035","6":"0.0178121189","7":"653","8":"4","9":"7","_rn_":"257"},{"1":"258","2":"1","3":"4","4":"0.0010193680","5":"8.0146483","6":"0.0422381191","7":"968","8":"4","9":"8","_rn_":"258"},{"1":"259","2":"0","3":"3","4":"0.0009823183","5":"39.5816107","6":"0.0169567553","7":"636","8":"3","9":"5","_rn_":"259"},{"1":"260","2":"0","3":"1","4":"0.0006811989","5":"0.0000000","6":"0.0002222069","7":"3","8":"1","9":"2","_rn_":"260"},{"1":"261","2":"1","3":"1","4":"0.0009727626","5":"440.0000000","6":"0.0149082630","7":"584","8":"1","9":"3","_rn_":"261"},{"1":"262","2":"2","3":"2","4":"0.0008431703","5":"43.3071066","6":"0.0218962762","7":"364","8":"4","9":"4","_rn_":"262"},{"1":"263","2":"1","3":"0","4":"0.0006844627","5":"0.0000000","6":"0.0002349505","7":"4","8":"1","9":"2","_rn_":"263"},{"1":"264","2":"0","3":"2","4":"0.0008382230","5":"5.5808500","6":"0.0108012801","7":"349","8":"2","9":"3","_rn_":"264"},{"1":"265","2":"3","3":"1","4":"0.0009803922","5":"110.7862938","6":"0.0160869373","7":"613","8":"4","9":"9","_rn_":"265"},{"1":"266","2":"1","3":"4","4":"0.0007342144","5":"74.2826689","6":"0.0024213399","7":"54","8":"4","9":"6","_rn_":"266"},{"1":"267","2":"0","3":"2","4":"0.0009832842","5":"0.0000000","6":"0.0183361320","7":"641","8":"2","9":"4","_rn_":"267"},{"1":"268","2":"7","3":"2","4":"0.0008203445","5":"59.9868186","6":"0.0224325314","7":"325","8":"8","9":"17","_rn_":"268"},{"1":"269","2":"0","3":"3","4":"0.0007616146","5":"11.6538688","6":"0.0057784314","7":"105","8":"3","9":"4","_rn_":"269"},{"1":"270","2":"1","3":"1","4":"0.0007122507","5":"0.0000000","6":"0.0027496305","7":"28","8":"2","9":"2","_rn_":"270"},{"1":"271","2":"2","3":"2","4":"0.0009756098","5":"2.3708333","6":"0.0305519411","7":"601","8":"4","9":"5","_rn_":"271"},{"1":"272","2":"1","3":"1","4":"0.0007299270","5":"0.0000000","6":"0.0047056610","7":"41","8":"2","9":"2","_rn_":"272"},{"1":"273","2":"4","3":"0","4":"0.0007680492","5":"4.1656922","6":"0.0036789553","7":"115","8":"4","9":"6","_rn_":"273"},{"1":"274","2":"0","3":"1","4":"0.0007633588","5":"0.0000000","6":"0.0069798275","7":"118","8":"1","9":"2","_rn_":"274"},{"1":"275","2":"1","3":"4","4":"0.0008576329","5":"2.5586654","6":"0.0289128773","7":"472","8":"5","9":"12","_rn_":"275"},{"1":"276","2":"1","3":"4","4":"0.0010245902","5":"9.8113596","6":"0.0621169348","7":"1035","8":"5","9":"8","_rn_":"276"},{"1":"277","2":"1","3":"5","4":"0.0008576329","5":"5.1173308","6":"0.0357901986","7":"472","8":"5","9":"12","_rn_":"277"},{"1":"278","2":"0","3":"5","4":"0.0010288066","5":"298.4194982","6":"0.0326629191","7":"1047","8":"4","9":"11","_rn_":"278"},{"1":"279","2":"8","3":"4","4":"0.0010141988","5":"716.8506163","6":"0.0290905950","7":"859","8":"8","9":"23","_rn_":"279"},{"1":"280","2":"0","3":"3","4":"0.0009775171","5":"0.0000000","6":"0.0308611229","7":"614","8":"3","9":"4","_rn_":"280"},{"1":"281","2":"5","3":"3","4":"0.0010245902","5":"130.5551629","6":"0.0301679929","7":"1015","8":"7","9":"21","_rn_":"281"},{"1":"282","2":"0","3":"1","4":"0.0006915629","5":"0.0000000","6":"0.0004896271","7":"9","8":"1","9":"2","_rn_":"282"},{"1":"283","2":"0","3":"1","4":"0.0006811989","5":"0.0000000","6":"0.0002222069","7":"3","8":"1","9":"2","_rn_":"283"},{"1":"284","2":"1","3":"1","4":"0.0009727626","5":"440.0000000","6":"0.0149082630","7":"584","8":"1","9":"3","_rn_":"284"},{"1":"285","2":"2","3":"4","4":"0.0007710100","5":"2.9681070","6":"0.0085748162","7":"152","8":"6","9":"9","_rn_":"285"},{"1":"286","2":"2","3":"0","4":"0.0007122507","5":"2.3448773","6":"0.0014145675","7":"19","8":"2","9":"3","_rn_":"286"},{"1":"287","2":"3","3":"1","4":"0.0007763975","5":"92.1185201","6":"0.0069937774","7":"119","8":"4","9":"5","_rn_":"287"},{"1":"288","2":"0","3":"2","4":"0.0007147963","5":"8.5459352","6":"0.0016207161","7":"32","8":"2","9":"3","_rn_":"288"},{"1":"289","2":"0","3":"3","4":"0.0007309942","5":"19.5988153","6":"0.0018396395","7":"45","8":"3","9":"4","_rn_":"289"},{"1":"290","2":"0","3":"1","4":"0.0007122507","5":"0.0000000","6":"0.0013748152","7":"27","8":"1","9":"2","_rn_":"290"},{"1":"291","2":"0","3":"1","4":"0.0007122507","5":"0.0000000","6":"0.0013748152","7":"27","8":"1","9":"2","_rn_":"291"},{"1":"292","2":"1","3":"1","4":"0.0009737098","5":"0.0000000","6":"0.0161677026","7":"605","8":"2","9":"4","_rn_":"292"},{"1":"293","2":"2","3":"1","4":"0.0008438819","5":"13.0353242","6":"0.0136698219","7":"399","8":"3","9":"5","_rn_":"293"},{"1":"294","2":"0","3":"1","4":"0.0006854010","5":"0.0000000","6":"0.0007045650","7":"8","8":"1","9":"2","_rn_":"294"},{"1":"295","2":"4","3":"1","4":"0.0009756098","5":"486.2369710","6":"0.0174085357","7":"615","8":"3","9":"9","_rn_":"295"},{"1":"296","2":"0","3":"1","4":"0.0006825939","5":"0.0000000","6":"0.0002594734","7":"6","8":"1","9":"2","_rn_":"296"},{"1":"297","2":"0","3":"2","4":"0.0009727626","5":"0.0000000","6":"0.0151644244","7":"588","8":"2","9":"4","_rn_":"297"},{"1":"298","2":"0","3":"3","4":"0.0010204082","5":"0.0000000","6":"0.0276800358","7":"955","8":"3","9":"8","_rn_":"298"},{"1":"299","2":"0","3":"2","4":"0.0009737098","5":"0.0000000","6":"0.0154644419","7":"593","8":"2","9":"4","_rn_":"299"},{"1":"300","2":"12","3":"3","4":"0.0010504202","5":"602.5685102","6":"0.0449483760","7":"1257","8":"10","9":"71","_rn_":"300"},{"1":"301","2":"11","3":"3","4":"0.0010351967","5":"86.4154966","6":"0.0580706106","7":"1060","8":"8","9":"38","_rn_":"301"},{"1":"302","2":"1","3":"4","4":"0.0009813543","5":"0.7878834","6":"0.0470691627","7":"638","8":"5","9":"6","_rn_":"302"},{"1":"303","2":"5","3":"2","4":"0.0010288066","5":"228.9672159","6":"0.0361830492","7":"1087","8":"5","9":"24","_rn_":"303"},{"1":"304","2":"0","3":"2","4":"0.0009718173","5":"0.0000000","6":"0.0154442576","7":"590","8":"2","9":"4","_rn_":"304"},{"1":"305","2":"1","3":"1","4":"0.0009775171","5":"0.0000000","6":"0.0172220389","7":"630","8":"2","9":"4","_rn_":"305"},{"1":"306","2":"4","3":"0","4":"0.0007451565","5":"39.8290067","6":"0.0020952991","7":"55","8":"3","9":"5","_rn_":"306"},{"1":"307","2":"3","3":"3","4":"0.0008417508","5":"50.7875402","6":"0.0319331044","7":"350","8":"4","9":"6","_rn_":"307"},{"1":"308","2":"8","3":"0","4":"0.0009960159","5":"87.8979307","6":"0.0290549708","7":"806","8":"8","9":"19","_rn_":"308"},{"1":"309","2":"0","3":"1","4":"0.0007336757","5":"0.0000000","6":"0.0022053608","7":"43","8":"1","9":"2","_rn_":"309"},{"1":"310","2":"29","3":"1","4":"0.0010298661","5":"2616.6729426","6":"0.0661528289","7":"933","8":"10","9":"56","_rn_":"310"},{"1":"311","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0009860047","7":"31","8":"1","9":"2","_rn_":"311"},{"1":"312","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0009860047","7":"31","8":"1","9":"2","_rn_":"312"},{"1":"313","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0009860047","7":"31","8":"1","9":"2","_rn_":"313"},{"1":"314","2":"0","3":"2","4":"0.0009813543","5":"0.0000000","6":"0.0158909557","7":"613","8":"2","9":"4","_rn_":"314"},{"1":"315","2":"0","3":"2","4":"0.0007122507","5":"1.3672267","6":"0.0018759113","7":"45","8":"2","9":"4","_rn_":"315"},{"1":"316","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0009860047","7":"31","8":"1","9":"2","_rn_":"316"},{"1":"317","2":"1","3":"5","4":"0.0010266940","5":"10.9665121","6":"0.0425214963","7":"958","8":"6","9":"10","_rn_":"317"},{"1":"318","2":"2","3":"2","4":"0.0010224949","5":"0.0000000","6":"0.0424459643","7":"959","8":"4","9":"8","_rn_":"318"},{"1":"319","2":"2","3":"2","4":"0.0008396306","5":"31.5933013","6":"0.0119802460","7":"358","8":"4","9":"6","_rn_":"319"},{"1":"320","2":"0","3":"3","4":"0.0010183299","5":"37.8212623","6":"0.0265007879","7":"930","8":"3","9":"8","_rn_":"320"},{"1":"321","2":"1","3":"1","4":"0.0007102273","5":"1.7393137","6":"0.0018206017","7":"29","8":"2","9":"3","_rn_":"321"},{"1":"322","2":"4","3":"1","4":"0.0009920635","5":"271.3732402","6":"0.0253975704","7":"743","8":"4","9":"11","_rn_":"322"},{"1":"323","2":"1","3":"2","4":"0.0008424600","5":"0.0000000","6":"0.0210793384","7":"342","8":"3","9":"4","_rn_":"323"},{"1":"324","2":"4","3":"0","4":"0.0009784736","5":"38.9226972","6":"0.0171458495","7":"631","8":"4","9":"8","_rn_":"324"},{"1":"325","2":"1","3":"1","4":"0.0009756098","5":"0.0000000","6":"0.0155749044","7":"598","8":"2","9":"4","_rn_":"325"},{"1":"326","2":"0","3":"3","4":"0.0010162602","5":"0.0000000","6":"0.0259971586","7":"931","8":"3","9":"8","_rn_":"326"},{"1":"327","2":"0","3":"1","4":"0.0007183908","5":"0.0000000","6":"0.0006699533","7":"16","8":"1","9":"2","_rn_":"327"},{"1":"328","2":"0","3":"1","4":"0.0007087172","5":"0.0000000","6":"0.0006876649","7":"9","8":"1","9":"2","_rn_":"328"},{"1":"329","2":"3","3":"6","4":"0.0010235415","5":"84.0661914","6":"0.0729963596","7":"984","8":"8","9":"18","_rn_":"329"},{"1":"330","2":"1","3":"2","4":"0.0009765625","5":"199.5382239","6":"0.0150082859","7":"592","8":"3","9":"4","_rn_":"330"},{"1":"331","2":"2","3":"4","4":"0.0009746589","5":"0.3333333","6":"0.0608536111","7":"610","8":"6","9":"6","_rn_":"331"},{"1":"332","2":"0","3":"1","4":"0.0006854010","5":"0.0000000","6":"0.0002347346","7":"4","8":"1","9":"2","_rn_":"332"},{"1":"333","2":"0","3":"2","4":"0.0009737098","5":"7.8757113","6":"0.0155463827","7":"604","8":"2","9":"3","_rn_":"333"},{"1":"334","2":"2","3":"0","4":"0.0007776050","5":"0.4935323","6":"0.0105097192","7":"184","8":"2","9":"4","_rn_":"334"},{"1":"335","2":"4","3":"2","4":"0.0009891197","5":"6.2401291","6":"0.0382938279","7":"730","8":"6","9":"10","_rn_":"335"},{"1":"336","2":"9","3":"3","4":"0.0010515247","5":"377.5951790","6":"0.0433613070","7":"1265","8":"9","9":"52","_rn_":"336"},{"1":"337","2":"3","3":"1","4":"0.0009784736","5":"187.5044542","6":"0.0153453721","7":"602","8":"3","9":"5","_rn_":"337"},{"1":"338","2":"1","3":"3","4":"0.0009775171","5":"0.0000000","6":"0.0457660740","7":"615","8":"4","9":"4","_rn_":"338"},{"1":"339","2":"1","3":"1","4":"0.0009794319","5":"14.2349766","6":"0.0171103119","7":"625","8":"2","9":"4","_rn_":"339"},{"1":"340","2":"1","3":"2","4":"0.0009737098","5":"197.1708453","6":"0.0152022422","7":"592","8":"2","9":"5","_rn_":"340"},{"1":"341","2":"9","3":"2","4":"0.0008802817","5":"86.0695399","6":"0.0331514384","7":"620","8":"9","9":"34","_rn_":"341"},{"1":"342","2":"3","3":"1","4":"0.0009756098","5":"328.6909821","6":"0.0164979346","7":"608","8":"3","9":"7","_rn_":"342"},{"1":"343","2":"1","3":"1","4":"0.0009727626","5":"0.0000000","6":"0.0151315397","7":"586","8":"2","9":"4","_rn_":"343"},{"1":"344","2":"1","3":"1","4":"0.0009727626","5":"97.7934609","6":"0.0149227508","7":"586","8":"2","9":"3","_rn_":"344"},{"1":"345","2":"2","3":"1","4":"0.0009784736","5":"32.3985660","6":"0.0181619796","7":"642","8":"3","9":"8","_rn_":"345"},{"1":"346","2":"1","3":"2","4":"0.0009737098","5":"0.0000000","6":"0.0298459920","7":"589","8":"3","9":"3","_rn_":"346"},{"1":"347","2":"2","3":"3","4":"0.0010341262","5":"198.4910607","6":"0.0335603555","7":"1052","8":"4","9":"13","_rn_":"347"},{"1":"348","2":"0","3":"1","4":"0.0007347539","5":"0.0000000","6":"0.0033413268","7":"54","8":"1","9":"2","_rn_":"348"},{"1":"349","2":"0","3":"1","4":"0.0007347539","5":"0.0000000","6":"0.0033413268","7":"54","8":"1","9":"2","_rn_":"349"},{"1":"350","2":"5","3":"1","4":"0.0009940358","5":"35.0007894","6":"0.0199309943","7":"704","8":"6","9":"12","_rn_":"350"},{"1":"351","2":"5","3":"1","4":"0.0008517888","5":"23.5528728","6":"0.0249139158","7":"411","8":"6","9":"7","_rn_":"351"},{"1":"352","2":"0","3":"1","4":"0.0007326007","5":"0.0000000","6":"0.0012767329","7":"31","8":"1","9":"2","_rn_":"352"},{"1":"353","2":"0","3":"2","4":"0.0007468260","5":"0.8040404","6":"0.0035635296","7":"60","8":"2","9":"3","_rn_":"353"},{"1":"354","2":"0","3":"3","4":"0.0010172940","5":"0.0000000","6":"0.0258264741","7":"928","8":"3","9":"8","_rn_":"354"},{"1":"355","2":"1","3":"3","4":"0.0009718173","5":"74.4734700","6":"0.0452013107","7":"590","8":"4","9":"4","_rn_":"355"},{"1":"356","2":"3","3":"2","4":"0.0009794319","5":"4.5595749","6":"0.0326373140","7":"629","8":"4","9":"10","_rn_":"356"},{"1":"357","2":"0","3":"2","4":"0.0009737098","5":"51.7845917","6":"0.0153809124","7":"589","8":"2","9":"3","_rn_":"357"},{"1":"358","2":"1","3":"1","4":"0.0009823183","5":"0.0000000","6":"0.0184348427","7":"648","8":"2","9":"4","_rn_":"358"},{"1":"359","2":"0","3":"1","4":"0.0006973501","5":"0.0000000","6":"0.0007765700","7":"10","8":"1","9":"2","_rn_":"359"},{"1":"360","2":"0","3":"4","4":"0.0009813543","5":"44.9757779","6":"0.0164609490","7":"631","8":"4","9":"7","_rn_":"360"},{"1":"361","2":"1","3":"20","4":"0.0008403361","5":"337.1514998","6":"0.0231037659","7":"445","8":"10","9":"35","_rn_":"361"},{"1":"362","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"362"},{"1":"363","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"363"},{"1":"364","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"364"},{"1":"365","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"365"},{"1":"366","2":"0","3":"2","4":"0.0009708738","5":"0.0000000","6":"0.0298099021","7":"583","8":"2","9":"2","_rn_":"366"},{"1":"367","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"367"},{"1":"368","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"368"},{"1":"369","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"369"},{"1":"370","2":"0","3":"1","4":"0.0007102273","5":"0.0000000","6":"0.0005180837","7":"11","8":"1","9":"2","_rn_":"370"},{"1":"371","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"371"},{"1":"372","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"372"},{"1":"373","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"373"},{"1":"374","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"374"},{"1":"375","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"375"},{"1":"376","2":"0","3":"3","4":"0.0010141988","5":"0.0000000","6":"0.0402321563","7":"916","8":"3","9":"4","_rn_":"376"},{"1":"377","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"377"},{"1":"378","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"378"},{"1":"379","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"379"},{"1":"380","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"380"},{"1":"381","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"381"},{"1":"382","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"382"},{"1":"383","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"383"},{"1":"384","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"384"},{"1":"385","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"385"},{"1":"386","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"386"},{"1":"387","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"387"},{"1":"388","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"388"},{"1":"389","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"389"},{"1":"390","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"390"},{"1":"391","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"391"},{"1":"392","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"392"},{"1":"393","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"393"},{"1":"394","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"394"},{"1":"395","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"395"},{"1":"396","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"396"},{"1":"397","2":"0","3":"3","4":"0.0010141988","5":"0.0000000","6":"0.0402321563","7":"916","8":"3","9":"4","_rn_":"397"},{"1":"398","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"398"},{"1":"399","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"399"},{"1":"400","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"400"},{"1":"401","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"401"},{"1":"402","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"402"},{"1":"403","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"403"},{"1":"404","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"404"},{"1":"405","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"405"},{"1":"406","2":"0","3":"3","4":"0.0010141988","5":"0.0000000","6":"0.0402321563","7":"916","8":"3","9":"4","_rn_":"406"},{"1":"407","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"407"},{"1":"408","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"408"},{"1":"409","2":"0","3":"2","4":"0.0010141988","5":"0.0000000","6":"0.0253272053","7":"915","8":"2","9":"4","_rn_":"409"},{"1":"410","2":"0","3":"3","4":"0.0010141988","5":"0.0000000","6":"0.0357494595","7":"916","8":"3","9":"4","_rn_":"410"},{"1":"411","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"411"},{"1":"412","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"412"},{"1":"413","2":"1","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0298099021","7":"583","8":"2","9":"2","_rn_":"413"},{"1":"414","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"414"},{"1":"415","2":"0","3":"2","4":"0.0009708738","5":"0.0000000","6":"0.0298099021","7":"583","8":"2","9":"2","_rn_":"415"},{"1":"416","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"416"},{"1":"417","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"417"},{"1":"418","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"418"},{"1":"419","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"419"},{"1":"420","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"420"},{"1":"421","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"421"},{"1":"422","2":"3","3":"1","4":"0.0007501875","5":"4.0905305","6":"0.0081089942","7":"92","8":"4","9":"5","_rn_":"422"},{"1":"423","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"423"},{"1":"424","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"424"},{"1":"425","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"425"},{"1":"426","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"426"},{"1":"427","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"427"},{"1":"428","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"428"},{"1":"429","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"429"},{"1":"430","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"430"},{"1":"431","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"431"},{"1":"432","2":"5","3":"38","4":"0.0009242144","5":"1192.2821069","6":"0.0846484230","7":"1119","8":"18","9":"239","_rn_":"432"},{"1":"433","2":"1","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0208445085","7":"334","8":"2","9":"2","_rn_":"433"},{"1":"434","2":"1","3":"2","4":"0.0008481764","5":"8.5677669","6":"0.0131751104","7":"395","8":"3","9":"6","_rn_":"434"},{"1":"435","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"435"},{"1":"436","2":"0","3":"1","4":"0.0008340284","5":"0.0000000","6":"0.0104222542","7":"333","8":"1","9":"2","_rn_":"436"},{"1":"437","2":"1","3":"1","4":"0.0007142857","5":"0.0000000","6":"0.0039416695","7":"36","8":"2","9":"2","_rn_":"437"},{"1":"438","2":"1","3":"1","4":"0.0008389262","5":"7.5253736","6":"0.0111396661","7":"353","8":"2","9":"3","_rn_":"438"},{"1":"439","2":"1","3":"1","4":"0.0007147963","5":"0.0000000","6":"0.0033095843","7":"33","8":"2","9":"2","_rn_":"439"},{"1":"440","2":"0","3":"1","4":"0.0009708738","5":"0.0000000","6":"0.0149049510","7":"582","8":"1","9":"2","_rn_":"440"},{"1":"441","2":"0","3":"0","4":"NA","5":"0.0000000","6":"0.0000000000","7":"0","8":"0","9":"1","_rn_":"441"},{"1":"442","2":"0","3":"0","4":"NA","5":"0.0000000","6":"0.0000000000","7":"0","8":"0","9":"1","_rn_":"442"},{"1":"443","2":"0","3":"0","4":"NA","5":"0.0000000","6":"0.0000000000","7":"0","8":"0","9":"1","_rn_":"443"},{"1":"444","2":"475","3":"106","4":"0.0016949153","5":"58205.5873579","6":"1.0000000000","7":"4342","8":"31","9":"2830","_rn_":"444"},{"1":"445","2":"276","3":"56","4":"0.0013175231","5":"16690.4261052","6":"0.6992478010","7":"3574","8":"31","9":"2218","_rn_":"445"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Additionally, a number of categorical variables pertaining to demographic characteristics are available for the same participants. Again, please refer to the data chapter of the book (Chapter 2; <span class="citation" data-cites="Lopez-Pernas2024-dat">[<a href="#ref-Lopez-Pernas2024-dat" role="doc-biblioref">59</a>]</span>) for more details on these variables. With an appropriate distance measure, namely the Gower distance measure, we will incorporate some of these variables in our applications of <span class="math inline">\(K\)</span>-Medoids and agglomerative hierarchical clustering (but <em>not</em> <span class="math inline">\(K\)</span>-Means) in addition to the continuous variables contained in <code>df</code>, largely for the purpose of demonstrating clustering methods which are capable of clustering variables of mixed type. We can download and preview the auxiliary categorical data set with the commands below. Note that we select only the following variables:</p>
<ul>
<li><code>experience</code> (coded as a level of experience, 1–3),</li>
<li><code>gender</code> (female or male),</li>
<li><code>region</code> (Midwest, Northeast, South, and West U.S.A., along with International),</li>
</ul>
<p>for the sake of simplifying the demonstration of mixed-type variables clustering and reducing the computational burden. We also extract the <code>UID</code> column, a user ID which corresponds to the <code>name</code> column in <code>df</code>, which will be required for later merging these two data sets. We also ensure that all but this leading <code>UID</code> column is formatted as a <code>factor</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="fu">paste0</span>(URL, <span class="st">"DLT1%20Nodes.csv"</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(UID, experience, gender, region) <span class="sc">|&gt;</span> </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_at</span>(<span class="fu">vars</span>(<span class="sc">-</span>UID), as.factor)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>demog</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["UID"],"name":[1],"type":["int"],"align":["right"]},{"label":["experience"],"name":[2],"type":["fct"],"align":["left"]},{"label":["gender"],"name":[3],"type":["fct"],"align":["left"]},{"label":["region"],"name":[4],"type":["fct"],"align":["left"]}],"data":[{"1":"1","2":"1","3":"female","4":"South","_rn_":"1"},{"1":"2","2":"1","3":"female","4":"South","_rn_":"2"},{"1":"3","2":"2","3":"female","4":"Northeast","_rn_":"3"},{"1":"4","2":"2","3":"female","4":"South","_rn_":"4"},{"1":"5","2":"3","3":"female","4":"South","_rn_":"5"},{"1":"6","2":"1","3":"female","4":"South","_rn_":"6"},{"1":"7","2":"2","3":"female","4":"Midwest","_rn_":"7"},{"1":"8","2":"1","3":"female","4":"International","_rn_":"8"},{"1":"9","2":"1","3":"female","4":"South","_rn_":"9"},{"1":"10","2":"2","3":"male","4":"South","_rn_":"10"},{"1":"11","2":"3","3":"female","4":"International","_rn_":"11"},{"1":"12","2":"3","3":"female","4":"South","_rn_":"12"},{"1":"13","2":"2","3":"female","4":"West","_rn_":"13"},{"1":"14","2":"1","3":"female","4":"South","_rn_":"14"},{"1":"15","2":"3","3":"female","4":"South","_rn_":"15"},{"1":"16","2":"1","3":"male","4":"International","_rn_":"16"},{"1":"17","2":"1","3":"female","4":"South","_rn_":"17"},{"1":"18","2":"1","3":"female","4":"South","_rn_":"18"},{"1":"19","2":"3","3":"male","4":"International","_rn_":"19"},{"1":"20","2":"1","3":"male","4":"Midwest","_rn_":"20"},{"1":"21","2":"1","3":"female","4":"South","_rn_":"21"},{"1":"22","2":"1","3":"male","4":"South","_rn_":"22"},{"1":"23","2":"3","3":"female","4":"South","_rn_":"23"},{"1":"24","2":"2","3":"female","4":"Midwest","_rn_":"24"},{"1":"25","2":"2","3":"female","4":"Midwest","_rn_":"25"},{"1":"26","2":"3","3":"female","4":"South","_rn_":"26"},{"1":"27","2":"1","3":"female","4":"Midwest","_rn_":"27"},{"1":"28","2":"3","3":"female","4":"Northeast","_rn_":"28"},{"1":"29","2":"2","3":"female","4":"South","_rn_":"29"},{"1":"30","2":"3","3":"female","4":"South","_rn_":"30"},{"1":"31","2":"1","3":"male","4":"West","_rn_":"31"},{"1":"32","2":"1","3":"female","4":"West","_rn_":"32"},{"1":"33","2":"3","3":"male","4":"West","_rn_":"33"},{"1":"34","2":"3","3":"female","4":"South","_rn_":"34"},{"1":"35","2":"2","3":"male","4":"Midwest","_rn_":"35"},{"1":"36","2":"2","3":"female","4":"West","_rn_":"36"},{"1":"37","2":"2","3":"male","4":"West","_rn_":"37"},{"1":"38","2":"1","3":"male","4":"Northeast","_rn_":"38"},{"1":"39","2":"2","3":"female","4":"South","_rn_":"39"},{"1":"40","2":"3","3":"female","4":"West","_rn_":"40"},{"1":"41","2":"1","3":"male","4":"Northeast","_rn_":"41"},{"1":"42","2":"3","3":"male","4":"Northeast","_rn_":"42"},{"1":"43","2":"3","3":"female","4":"Northeast","_rn_":"43"},{"1":"44","2":"2","3":"male","4":"South","_rn_":"44"},{"1":"45","2":"2","3":"female","4":"West","_rn_":"45"},{"1":"46","2":"2","3":"female","4":"Northeast","_rn_":"46"},{"1":"47","2":"3","3":"female","4":"Northeast","_rn_":"47"},{"1":"48","2":"3","3":"male","4":"Midwest","_rn_":"48"},{"1":"49","2":"3","3":"female","4":"Northeast","_rn_":"49"},{"1":"50","2":"3","3":"female","4":"South","_rn_":"50"},{"1":"51","2":"3","3":"male","4":"South","_rn_":"51"},{"1":"52","2":"3","3":"male","4":"Northeast","_rn_":"52"},{"1":"53","2":"3","3":"female","4":"Northeast","_rn_":"53"},{"1":"54","2":"2","3":"female","4":"Northeast","_rn_":"54"},{"1":"55","2":"1","3":"female","4":"South","_rn_":"55"},{"1":"56","2":"3","3":"female","4":"Northeast","_rn_":"56"},{"1":"57","2":"3","3":"female","4":"Midwest","_rn_":"57"},{"1":"58","2":"2","3":"female","4":"International","_rn_":"58"},{"1":"59","2":"1","3":"female","4":"Northeast","_rn_":"59"},{"1":"60","2":"2","3":"female","4":"South","_rn_":"60"},{"1":"61","2":"3","3":"female","4":"South","_rn_":"61"},{"1":"62","2":"1","3":"female","4":"Northeast","_rn_":"62"},{"1":"63","2":"1","3":"female","4":"South","_rn_":"63"},{"1":"64","2":"2","3":"female","4":"Northeast","_rn_":"64"},{"1":"65","2":"2","3":"female","4":"Northeast","_rn_":"65"},{"1":"66","2":"2","3":"female","4":"South","_rn_":"66"},{"1":"67","2":"3","3":"female","4":"South","_rn_":"67"},{"1":"68","2":"3","3":"female","4":"International","_rn_":"68"},{"1":"69","2":"3","3":"female","4":"Northeast","_rn_":"69"},{"1":"70","2":"1","3":"female","4":"South","_rn_":"70"},{"1":"71","2":"3","3":"female","4":"West","_rn_":"71"},{"1":"72","2":"2","3":"female","4":"South","_rn_":"72"},{"1":"73","2":"3","3":"female","4":"South","_rn_":"73"},{"1":"74","2":"3","3":"female","4":"Northeast","_rn_":"74"},{"1":"75","2":"3","3":"female","4":"Northeast","_rn_":"75"},{"1":"76","2":"3","3":"female","4":"Midwest","_rn_":"76"},{"1":"77","2":"2","3":"female","4":"South","_rn_":"77"},{"1":"78","2":"3","3":"female","4":"International","_rn_":"78"},{"1":"79","2":"1","3":"female","4":"Midwest","_rn_":"79"},{"1":"80","2":"2","3":"female","4":"South","_rn_":"80"},{"1":"81","2":"3","3":"female","4":"West","_rn_":"81"},{"1":"82","2":"1","3":"female","4":"South","_rn_":"82"},{"1":"83","2":"3","3":"female","4":"Northeast","_rn_":"83"},{"1":"84","2":"1","3":"female","4":"Northeast","_rn_":"84"},{"1":"85","2":"1","3":"female","4":"South","_rn_":"85"},{"1":"86","2":"3","3":"male","4":"Northeast","_rn_":"86"},{"1":"87","2":"2","3":"female","4":"Northeast","_rn_":"87"},{"1":"88","2":"1","3":"male","4":"Midwest","_rn_":"88"},{"1":"89","2":"3","3":"female","4":"Midwest","_rn_":"89"},{"1":"90","2":"3","3":"female","4":"Midwest","_rn_":"90"},{"1":"91","2":"2","3":"male","4":"Midwest","_rn_":"91"},{"1":"92","2":"3","3":"male","4":"Midwest","_rn_":"92"},{"1":"93","2":"1","3":"male","4":"South","_rn_":"93"},{"1":"94","2":"1","3":"male","4":"West","_rn_":"94"},{"1":"95","2":"1","3":"female","4":"Northeast","_rn_":"95"},{"1":"96","2":"2","3":"female","4":"South","_rn_":"96"},{"1":"97","2":"1","3":"female","4":"South","_rn_":"97"},{"1":"98","2":"2","3":"female","4":"South","_rn_":"98"},{"1":"99","2":"3","3":"male","4":"West","_rn_":"99"},{"1":"100","2":"2","3":"male","4":"South","_rn_":"100"},{"1":"101","2":"1","3":"female","4":"South","_rn_":"101"},{"1":"102","2":"3","3":"female","4":"Northeast","_rn_":"102"},{"1":"103","2":"2","3":"male","4":"Northeast","_rn_":"103"},{"1":"104","2":"3","3":"female","4":"Northeast","_rn_":"104"},{"1":"105","2":"2","3":"male","4":"South","_rn_":"105"},{"1":"106","2":"3","3":"male","4":"South","_rn_":"106"},{"1":"107","2":"2","3":"female","4":"South","_rn_":"107"},{"1":"108","2":"1","3":"female","4":"Northeast","_rn_":"108"},{"1":"109","2":"3","3":"female","4":"South","_rn_":"109"},{"1":"110","2":"1","3":"male","4":"South","_rn_":"110"},{"1":"111","2":"2","3":"female","4":"South","_rn_":"111"},{"1":"112","2":"2","3":"male","4":"Midwest","_rn_":"112"},{"1":"113","2":"2","3":"female","4":"West","_rn_":"113"},{"1":"114","2":"2","3":"male","4":"International","_rn_":"114"},{"1":"115","2":"2","3":"female","4":"Northeast","_rn_":"115"},{"1":"116","2":"3","3":"female","4":"Midwest","_rn_":"116"},{"1":"117","2":"1","3":"female","4":"South","_rn_":"117"},{"1":"118","2":"1","3":"male","4":"International","_rn_":"118"},{"1":"119","2":"2","3":"female","4":"South","_rn_":"119"},{"1":"120","2":"3","3":"female","4":"Midwest","_rn_":"120"},{"1":"121","2":"2","3":"male","4":"South","_rn_":"121"},{"1":"122","2":"3","3":"female","4":"South","_rn_":"122"},{"1":"123","2":"2","3":"female","4":"South","_rn_":"123"},{"1":"124","2":"2","3":"male","4":"South","_rn_":"124"},{"1":"125","2":"3","3":"female","4":"Northeast","_rn_":"125"},{"1":"126","2":"2","3":"male","4":"Northeast","_rn_":"126"},{"1":"127","2":"2","3":"male","4":"South","_rn_":"127"},{"1":"128","2":"1","3":"female","4":"International","_rn_":"128"},{"1":"129","2":"2","3":"female","4":"Midwest","_rn_":"129"},{"1":"130","2":"3","3":"female","4":"Northeast","_rn_":"130"},{"1":"131","2":"2","3":"female","4":"South","_rn_":"131"},{"1":"132","2":"1","3":"female","4":"Midwest","_rn_":"132"},{"1":"133","2":"3","3":"male","4":"South","_rn_":"133"},{"1":"134","2":"1","3":"male","4":"South","_rn_":"134"},{"1":"135","2":"1","3":"female","4":"South","_rn_":"135"},{"1":"136","2":"3","3":"male","4":"Northeast","_rn_":"136"},{"1":"137","2":"3","3":"male","4":"West","_rn_":"137"},{"1":"138","2":"2","3":"female","4":"Northeast","_rn_":"138"},{"1":"139","2":"2","3":"female","4":"South","_rn_":"139"},{"1":"140","2":"3","3":"female","4":"International","_rn_":"140"},{"1":"141","2":"3","3":"female","4":"West","_rn_":"141"},{"1":"142","2":"2","3":"female","4":"South","_rn_":"142"},{"1":"143","2":"2","3":"female","4":"Northeast","_rn_":"143"},{"1":"144","2":"2","3":"male","4":"South","_rn_":"144"},{"1":"145","2":"1","3":"female","4":"Midwest","_rn_":"145"},{"1":"146","2":"1","3":"female","4":"West","_rn_":"146"},{"1":"147","2":"2","3":"female","4":"Northeast","_rn_":"147"},{"1":"148","2":"2","3":"female","4":"South","_rn_":"148"},{"1":"149","2":"1","3":"male","4":"Midwest","_rn_":"149"},{"1":"150","2":"1","3":"male","4":"South","_rn_":"150"},{"1":"151","2":"2","3":"male","4":"Northeast","_rn_":"151"},{"1":"152","2":"2","3":"male","4":"Northeast","_rn_":"152"},{"1":"153","2":"1","3":"male","4":"West","_rn_":"153"},{"1":"154","2":"3","3":"female","4":"Midwest","_rn_":"154"},{"1":"155","2":"3","3":"female","4":"South","_rn_":"155"},{"1":"156","2":"3","3":"female","4":"Northeast","_rn_":"156"},{"1":"157","2":"1","3":"male","4":"South","_rn_":"157"},{"1":"158","2":"3","3":"female","4":"Midwest","_rn_":"158"},{"1":"159","2":"2","3":"male","4":"Northeast","_rn_":"159"},{"1":"160","2":"3","3":"female","4":"Midwest","_rn_":"160"},{"1":"161","2":"3","3":"female","4":"South","_rn_":"161"},{"1":"162","2":"3","3":"female","4":"South","_rn_":"162"},{"1":"163","2":"1","3":"male","4":"Northeast","_rn_":"163"},{"1":"164","2":"2","3":"male","4":"South","_rn_":"164"},{"1":"165","2":"3","3":"female","4":"Northeast","_rn_":"165"},{"1":"166","2":"1","3":"female","4":"Northeast","_rn_":"166"},{"1":"167","2":"3","3":"male","4":"South","_rn_":"167"},{"1":"168","2":"3","3":"female","4":"Midwest","_rn_":"168"},{"1":"169","2":"3","3":"female","4":"Northeast","_rn_":"169"},{"1":"170","2":"1","3":"male","4":"Midwest","_rn_":"170"},{"1":"171","2":"2","3":"male","4":"Midwest","_rn_":"171"},{"1":"172","2":"2","3":"female","4":"International","_rn_":"172"},{"1":"173","2":"2","3":"male","4":"West","_rn_":"173"},{"1":"174","2":"2","3":"male","4":"International","_rn_":"174"},{"1":"175","2":"3","3":"female","4":"Northeast","_rn_":"175"},{"1":"176","2":"2","3":"male","4":"Northeast","_rn_":"176"},{"1":"177","2":"2","3":"female","4":"Northeast","_rn_":"177"},{"1":"178","2":"2","3":"male","4":"Midwest","_rn_":"178"},{"1":"179","2":"2","3":"male","4":"South","_rn_":"179"},{"1":"180","2":"3","3":"male","4":"International","_rn_":"180"},{"1":"181","2":"1","3":"female","4":"West","_rn_":"181"},{"1":"182","2":"1","3":"female","4":"South","_rn_":"182"},{"1":"183","2":"2","3":"male","4":"South","_rn_":"183"},{"1":"184","2":"1","3":"male","4":"Northeast","_rn_":"184"},{"1":"185","2":"2","3":"female","4":"South","_rn_":"185"},{"1":"186","2":"1","3":"female","4":"Midwest","_rn_":"186"},{"1":"187","2":"2","3":"male","4":"West","_rn_":"187"},{"1":"188","2":"3","3":"male","4":"Northeast","_rn_":"188"},{"1":"189","2":"3","3":"female","4":"International","_rn_":"189"},{"1":"190","2":"1","3":"female","4":"International","_rn_":"190"},{"1":"191","2":"3","3":"female","4":"International","_rn_":"191"},{"1":"192","2":"3","3":"female","4":"Northeast","_rn_":"192"},{"1":"193","2":"2","3":"female","4":"Northeast","_rn_":"193"},{"1":"194","2":"3","3":"female","4":"South","_rn_":"194"},{"1":"195","2":"3","3":"female","4":"South","_rn_":"195"},{"1":"196","2":"1","3":"female","4":"South","_rn_":"196"},{"1":"197","2":"3","3":"female","4":"West","_rn_":"197"},{"1":"198","2":"2","3":"female","4":"South","_rn_":"198"},{"1":"199","2":"3","3":"female","4":"Northeast","_rn_":"199"},{"1":"200","2":"2","3":"female","4":"Midwest","_rn_":"200"},{"1":"201","2":"1","3":"female","4":"Northeast","_rn_":"201"},{"1":"202","2":"2","3":"female","4":"South","_rn_":"202"},{"1":"203","2":"3","3":"female","4":"Midwest","_rn_":"203"},{"1":"204","2":"2","3":"female","4":"West","_rn_":"204"},{"1":"205","2":"1","3":"female","4":"South","_rn_":"205"},{"1":"206","2":"2","3":"male","4":"West","_rn_":"206"},{"1":"207","2":"1","3":"male","4":"South","_rn_":"207"},{"1":"208","2":"2","3":"male","4":"Northeast","_rn_":"208"},{"1":"209","2":"3","3":"male","4":"Midwest","_rn_":"209"},{"1":"210","2":"2","3":"male","4":"Northeast","_rn_":"210"},{"1":"211","2":"2","3":"male","4":"South","_rn_":"211"},{"1":"212","2":"3","3":"male","4":"Northeast","_rn_":"212"},{"1":"213","2":"2","3":"female","4":"Northeast","_rn_":"213"},{"1":"214","2":"1","3":"female","4":"Northeast","_rn_":"214"},{"1":"215","2":"1","3":"female","4":"Northeast","_rn_":"215"},{"1":"216","2":"3","3":"female","4":"South","_rn_":"216"},{"1":"217","2":"3","3":"male","4":"West","_rn_":"217"},{"1":"218","2":"1","3":"male","4":"South","_rn_":"218"},{"1":"219","2":"3","3":"female","4":"South","_rn_":"219"},{"1":"220","2":"3","3":"female","4":"West","_rn_":"220"},{"1":"221","2":"1","3":"female","4":"Northeast","_rn_":"221"},{"1":"222","2":"3","3":"male","4":"Northeast","_rn_":"222"},{"1":"223","2":"2","3":"male","4":"West","_rn_":"223"},{"1":"224","2":"3","3":"female","4":"Midwest","_rn_":"224"},{"1":"225","2":"3","3":"female","4":"South","_rn_":"225"},{"1":"226","2":"3","3":"male","4":"Northeast","_rn_":"226"},{"1":"227","2":"2","3":"female","4":"Northeast","_rn_":"227"},{"1":"228","2":"1","3":"male","4":"South","_rn_":"228"},{"1":"229","2":"2","3":"male","4":"International","_rn_":"229"},{"1":"230","2":"3","3":"female","4":"South","_rn_":"230"},{"1":"231","2":"3","3":"female","4":"South","_rn_":"231"},{"1":"232","2":"3","3":"female","4":"South","_rn_":"232"},{"1":"233","2":"2","3":"female","4":"South","_rn_":"233"},{"1":"234","2":"3","3":"female","4":"Midwest","_rn_":"234"},{"1":"235","2":"2","3":"female","4":"South","_rn_":"235"},{"1":"236","2":"1","3":"female","4":"Midwest","_rn_":"236"},{"1":"237","2":"2","3":"male","4":"West","_rn_":"237"},{"1":"238","2":"1","3":"male","4":"Midwest","_rn_":"238"},{"1":"239","2":"1","3":"male","4":"West","_rn_":"239"},{"1":"240","2":"1","3":"female","4":"South","_rn_":"240"},{"1":"241","2":"1","3":"female","4":"International","_rn_":"241"},{"1":"242","2":"1","3":"female","4":"International","_rn_":"242"},{"1":"243","2":"1","3":"male","4":"Midwest","_rn_":"243"},{"1":"244","2":"3","3":"female","4":"South","_rn_":"244"},{"1":"245","2":"1","3":"female","4":"South","_rn_":"245"},{"1":"246","2":"3","3":"female","4":"Midwest","_rn_":"246"},{"1":"247","2":"2","3":"male","4":"Midwest","_rn_":"247"},{"1":"248","2":"1","3":"male","4":"Midwest","_rn_":"248"},{"1":"249","2":"2","3":"female","4":"Midwest","_rn_":"249"},{"1":"250","2":"2","3":"female","4":"South","_rn_":"250"},{"1":"251","2":"2","3":"male","4":"West","_rn_":"251"},{"1":"252","2":"2","3":"male","4":"South","_rn_":"252"},{"1":"253","2":"3","3":"female","4":"Northeast","_rn_":"253"},{"1":"254","2":"3","3":"female","4":"Midwest","_rn_":"254"},{"1":"255","2":"2","3":"female","4":"Northeast","_rn_":"255"},{"1":"256","2":"3","3":"female","4":"Northeast","_rn_":"256"},{"1":"257","2":"3","3":"male","4":"West","_rn_":"257"},{"1":"258","2":"3","3":"male","4":"South","_rn_":"258"},{"1":"259","2":"3","3":"female","4":"South","_rn_":"259"},{"1":"260","2":"3","3":"male","4":"Northeast","_rn_":"260"},{"1":"261","2":"2","3":"female","4":"South","_rn_":"261"},{"1":"262","2":"3","3":"female","4":"Northeast","_rn_":"262"},{"1":"263","2":"2","3":"female","4":"South","_rn_":"263"},{"1":"264","2":"2","3":"female","4":"West","_rn_":"264"},{"1":"265","2":"3","3":"male","4":"Midwest","_rn_":"265"},{"1":"266","2":"1","3":"female","4":"Midwest","_rn_":"266"},{"1":"267","2":"3","3":"male","4":"South","_rn_":"267"},{"1":"268","2":"1","3":"female","4":"South","_rn_":"268"},{"1":"269","2":"3","3":"female","4":"South","_rn_":"269"},{"1":"270","2":"1","3":"female","4":"Northeast","_rn_":"270"},{"1":"271","2":"2","3":"female","4":"South","_rn_":"271"},{"1":"272","2":"3","3":"male","4":"West","_rn_":"272"},{"1":"273","2":"3","3":"female","4":"South","_rn_":"273"},{"1":"274","2":"3","3":"female","4":"South","_rn_":"274"},{"1":"275","2":"1","3":"male","4":"Northeast","_rn_":"275"},{"1":"276","2":"3","3":"male","4":"South","_rn_":"276"},{"1":"277","2":"3","3":"female","4":"West","_rn_":"277"},{"1":"278","2":"2","3":"female","4":"West","_rn_":"278"},{"1":"279","2":"3","3":"female","4":"Northeast","_rn_":"279"},{"1":"280","2":"3","3":"male","4":"Midwest","_rn_":"280"},{"1":"281","2":"3","3":"male","4":"Northeast","_rn_":"281"},{"1":"282","2":"1","3":"female","4":"Midwest","_rn_":"282"},{"1":"283","2":"2","3":"female","4":"South","_rn_":"283"},{"1":"284","2":"1","3":"male","4":"South","_rn_":"284"},{"1":"285","2":"1","3":"male","4":"South","_rn_":"285"},{"1":"286","2":"3","3":"female","4":"Northeast","_rn_":"286"},{"1":"287","2":"1","3":"female","4":"Northeast","_rn_":"287"},{"1":"288","2":"1","3":"male","4":"International","_rn_":"288"},{"1":"289","2":"2","3":"female","4":"Midwest","_rn_":"289"},{"1":"290","2":"3","3":"female","4":"Midwest","_rn_":"290"},{"1":"291","2":"1","3":"female","4":"South","_rn_":"291"},{"1":"292","2":"1","3":"female","4":"Northeast","_rn_":"292"},{"1":"293","2":"3","3":"female","4":"South","_rn_":"293"},{"1":"294","2":"3","3":"male","4":"South","_rn_":"294"},{"1":"295","2":"1","3":"male","4":"International","_rn_":"295"},{"1":"296","2":"3","3":"male","4":"International","_rn_":"296"},{"1":"297","2":"3","3":"male","4":"South","_rn_":"297"},{"1":"298","2":"3","3":"female","4":"South","_rn_":"298"},{"1":"299","2":"3","3":"male","4":"Northeast","_rn_":"299"},{"1":"300","2":"2","3":"male","4":"Midwest","_rn_":"300"},{"1":"301","2":"2","3":"female","4":"Midwest","_rn_":"301"},{"1":"302","2":"3","3":"female","4":"South","_rn_":"302"},{"1":"303","2":"3","3":"female","4":"South","_rn_":"303"},{"1":"304","2":"1","3":"female","4":"Northeast","_rn_":"304"},{"1":"305","2":"3","3":"female","4":"Midwest","_rn_":"305"},{"1":"306","2":"1","3":"male","4":"South","_rn_":"306"},{"1":"307","2":"2","3":"female","4":"Midwest","_rn_":"307"},{"1":"308","2":"1","3":"female","4":"Northeast","_rn_":"308"},{"1":"309","2":"1","3":"female","4":"International","_rn_":"309"},{"1":"310","2":"2","3":"male","4":"International","_rn_":"310"},{"1":"311","2":"2","3":"male","4":"Northeast","_rn_":"311"},{"1":"312","2":"2","3":"female","4":"Northeast","_rn_":"312"},{"1":"313","2":"2","3":"female","4":"West","_rn_":"313"},{"1":"314","2":"2","3":"female","4":"West","_rn_":"314"},{"1":"315","2":"3","3":"female","4":"West","_rn_":"315"},{"1":"316","2":"3","3":"female","4":"International","_rn_":"316"},{"1":"317","2":"3","3":"male","4":"West","_rn_":"317"},{"1":"318","2":"2","3":"female","4":"Midwest","_rn_":"318"},{"1":"319","2":"2","3":"male","4":"Midwest","_rn_":"319"},{"1":"320","2":"3","3":"female","4":"South","_rn_":"320"},{"1":"321","2":"2","3":"male","4":"South","_rn_":"321"},{"1":"322","2":"1","3":"male","4":"South","_rn_":"322"},{"1":"323","2":"1","3":"female","4":"Northeast","_rn_":"323"},{"1":"324","2":"3","3":"female","4":"South","_rn_":"324"},{"1":"325","2":"2","3":"male","4":"South","_rn_":"325"},{"1":"326","2":"3","3":"female","4":"South","_rn_":"326"},{"1":"327","2":"1","3":"male","4":"Northeast","_rn_":"327"},{"1":"328","2":"3","3":"male","4":"Midwest","_rn_":"328"},{"1":"329","2":"2","3":"male","4":"Midwest","_rn_":"329"},{"1":"330","2":"3","3":"female","4":"West","_rn_":"330"},{"1":"331","2":"2","3":"male","4":"West","_rn_":"331"},{"1":"332","2":"1","3":"female","4":"South","_rn_":"332"},{"1":"333","2":"1","3":"female","4":"South","_rn_":"333"},{"1":"334","2":"3","3":"female","4":"South","_rn_":"334"},{"1":"335","2":"3","3":"female","4":"South","_rn_":"335"},{"1":"336","2":"2","3":"female","4":"South","_rn_":"336"},{"1":"337","2":"3","3":"male","4":"Northeast","_rn_":"337"},{"1":"338","2":"3","3":"female","4":"South","_rn_":"338"},{"1":"339","2":"3","3":"female","4":"South","_rn_":"339"},{"1":"340","2":"3","3":"female","4":"South","_rn_":"340"},{"1":"341","2":"3","3":"female","4":"Northeast","_rn_":"341"},{"1":"342","2":"3","3":"male","4":"International","_rn_":"342"},{"1":"343","2":"1","3":"female","4":"South","_rn_":"343"},{"1":"344","2":"3","3":"female","4":"Midwest","_rn_":"344"},{"1":"345","2":"3","3":"female","4":"South","_rn_":"345"},{"1":"346","2":"1","3":"male","4":"South","_rn_":"346"},{"1":"347","2":"2","3":"male","4":"South","_rn_":"347"},{"1":"348","2":"3","3":"female","4":"South","_rn_":"348"},{"1":"349","2":"1","3":"female","4":"Northeast","_rn_":"349"},{"1":"350","2":"2","3":"male","4":"Northeast","_rn_":"350"},{"1":"351","2":"2","3":"male","4":"West","_rn_":"351"},{"1":"352","2":"1","3":"female","4":"Midwest","_rn_":"352"},{"1":"353","2":"1","3":"female","4":"Northeast","_rn_":"353"},{"1":"354","2":"2","3":"female","4":"South","_rn_":"354"},{"1":"355","2":"2","3":"female","4":"Midwest","_rn_":"355"},{"1":"356","2":"2","3":"female","4":"Midwest","_rn_":"356"},{"1":"357","2":"3","3":"female","4":"South","_rn_":"357"},{"1":"358","2":"3","3":"female","4":"Northeast","_rn_":"358"},{"1":"359","2":"3","3":"female","4":"Northeast","_rn_":"359"},{"1":"360","2":"1","3":"female","4":"South","_rn_":"360"},{"1":"361","2":"3","3":"female","4":"South","_rn_":"361"},{"1":"362","2":"2","3":"female","4":"Midwest","_rn_":"362"},{"1":"363","2":"3","3":"male","4":"Midwest","_rn_":"363"},{"1":"364","2":"3","3":"male","4":"International","_rn_":"364"},{"1":"365","2":"2","3":"female","4":"South","_rn_":"365"},{"1":"366","2":"2","3":"female","4":"International","_rn_":"366"},{"1":"367","2":"1","3":"female","4":"Northeast","_rn_":"367"},{"1":"368","2":"2","3":"female","4":"Northeast","_rn_":"368"},{"1":"369","2":"3","3":"female","4":"Northeast","_rn_":"369"},{"1":"370","2":"3","3":"female","4":"South","_rn_":"370"},{"1":"371","2":"2","3":"female","4":"West","_rn_":"371"},{"1":"372","2":"3","3":"female","4":"South","_rn_":"372"},{"1":"373","2":"1","3":"female","4":"Northeast","_rn_":"373"},{"1":"374","2":"2","3":"female","4":"South","_rn_":"374"},{"1":"375","2":"3","3":"female","4":"South","_rn_":"375"},{"1":"376","2":"2","3":"female","4":"Northeast","_rn_":"376"},{"1":"377","2":"2","3":"male","4":"South","_rn_":"377"},{"1":"378","2":"1","3":"male","4":"South","_rn_":"378"},{"1":"379","2":"2","3":"female","4":"South","_rn_":"379"},{"1":"380","2":"3","3":"female","4":"Northeast","_rn_":"380"},{"1":"381","2":"2","3":"female","4":"Northeast","_rn_":"381"},{"1":"382","2":"2","3":"female","4":"South","_rn_":"382"},{"1":"383","2":"3","3":"female","4":"South","_rn_":"383"},{"1":"384","2":"2","3":"female","4":"South","_rn_":"384"},{"1":"385","2":"3","3":"female","4":"South","_rn_":"385"},{"1":"386","2":"2","3":"female","4":"International","_rn_":"386"},{"1":"387","2":"3","3":"female","4":"Midwest","_rn_":"387"},{"1":"388","2":"2","3":"male","4":"South","_rn_":"388"},{"1":"389","2":"3","3":"female","4":"Northeast","_rn_":"389"},{"1":"390","2":"1","3":"female","4":"Northeast","_rn_":"390"},{"1":"391","2":"3","3":"female","4":"South","_rn_":"391"},{"1":"392","2":"3","3":"male","4":"South","_rn_":"392"},{"1":"393","2":"2","3":"female","4":"West","_rn_":"393"},{"1":"394","2":"1","3":"male","4":"Midwest","_rn_":"394"},{"1":"395","2":"2","3":"female","4":"West","_rn_":"395"},{"1":"396","2":"2","3":"male","4":"Midwest","_rn_":"396"},{"1":"397","2":"1","3":"female","4":"Midwest","_rn_":"397"},{"1":"398","2":"1","3":"female","4":"South","_rn_":"398"},{"1":"399","2":"2","3":"female","4":"Northeast","_rn_":"399"},{"1":"400","2":"3","3":"male","4":"International","_rn_":"400"},{"1":"401","2":"2","3":"female","4":"International","_rn_":"401"},{"1":"402","2":"2","3":"male","4":"Midwest","_rn_":"402"},{"1":"403","2":"3","3":"female","4":"Midwest","_rn_":"403"},{"1":"404","2":"1","3":"female","4":"Midwest","_rn_":"404"},{"1":"405","2":"1","3":"male","4":"Northeast","_rn_":"405"},{"1":"406","2":"2","3":"female","4":"Midwest","_rn_":"406"},{"1":"407","2":"3","3":"female","4":"Northeast","_rn_":"407"},{"1":"408","2":"3","3":"male","4":"Midwest","_rn_":"408"},{"1":"409","2":"1","3":"female","4":"South","_rn_":"409"},{"1":"410","2":"1","3":"female","4":"West","_rn_":"410"},{"1":"411","2":"3","3":"female","4":"West","_rn_":"411"},{"1":"412","2":"1","3":"female","4":"South","_rn_":"412"},{"1":"413","2":"3","3":"female","4":"South","_rn_":"413"},{"1":"414","2":"2","3":"female","4":"West","_rn_":"414"},{"1":"415","2":"3","3":"male","4":"South","_rn_":"415"},{"1":"416","2":"2","3":"female","4":"West","_rn_":"416"},{"1":"417","2":"2","3":"female","4":"Northeast","_rn_":"417"},{"1":"418","2":"2","3":"female","4":"Northeast","_rn_":"418"},{"1":"419","2":"3","3":"female","4":"South","_rn_":"419"},{"1":"420","2":"2","3":"female","4":"Northeast","_rn_":"420"},{"1":"421","2":"2","3":"female","4":"Midwest","_rn_":"421"},{"1":"422","2":"2","3":"female","4":"South","_rn_":"422"},{"1":"423","2":"3","3":"male","4":"Northeast","_rn_":"423"},{"1":"424","2":"2","3":"female","4":"West","_rn_":"424"},{"1":"425","2":"3","3":"male","4":"Northeast","_rn_":"425"},{"1":"426","2":"1","3":"female","4":"Midwest","_rn_":"426"},{"1":"427","2":"3","3":"female","4":"Midwest","_rn_":"427"},{"1":"428","2":"3","3":"female","4":"Northeast","_rn_":"428"},{"1":"429","2":"1","3":"male","4":"West","_rn_":"429"},{"1":"430","2":"2","3":"female","4":"West","_rn_":"430"},{"1":"431","2":"2","3":"female","4":"Northeast","_rn_":"431"},{"1":"432","2":"1","3":"female","4":"South","_rn_":"432"},{"1":"433","2":"3","3":"female","4":"South","_rn_":"433"},{"1":"434","2":"1","3":"female","4":"Midwest","_rn_":"434"},{"1":"435","2":"1","3":"male","4":"Northeast","_rn_":"435"},{"1":"436","2":"2","3":"male","4":"South","_rn_":"436"},{"1":"437","2":"1","3":"female","4":"Midwest","_rn_":"437"},{"1":"438","2":"1","3":"male","4":"West","_rn_":"438"},{"1":"439","2":"1","3":"NULL","4":"NULL","_rn_":"439"},{"1":"440","2":"2","3":"female","4":"South","_rn_":"440"},{"1":"441","2":"3","3":"female","4":"South","_rn_":"441"},{"1":"442","2":"3","3":"female","4":"South","_rn_":"442"},{"1":"443","2":"2","3":"female","4":"Northeast","_rn_":"443"},{"1":"444","2":"3","3":"male","4":"South","_rn_":"444"},{"1":"445","2":"3","3":"female","4":"South","_rn_":"445"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<section id="sec-process" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="sec-process"><span class="header-section-number">4.1.1</span> Pre-processing the data</h4>
<p>In <code>df</code>, the first column (<code>name</code>) is the student identifier, and the remaining columns are each of the centrality measures calculated from students’ interactions in the MOOC forum. We will eventually discard the <code>name</code> column from future analyses; we retain it for the time being so that <code>df</code> and <code>demog</code> can be merged appropriately. The data also contain a small number of observations —only three rows of <code>df</code>— with missing values for the variable <code>Closeness_total</code>, as seen by</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> <span class="fu">is.na</span>() <span class="sc">|&gt;</span> <span class="fu">which</span>(<span class="at">arr.ind=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    row col
441 441   4
442 442   4
443 443   4</code></pre>
</div>
</div>
<p>Given that none of the clustering methods described in this chapter are capable of accommodating missing data, we remove these three observations for future analyses too. Furthermore, one of the rows in <code>demog</code> has <code>NULL</code> recorded for the <code>gender</code> variable. We remove all invalid rows from both <code>df</code> and <code>demog</code>. The function <code>complete.cases()</code> constructs a completely observed data set by extracting the rows which contain one or more missing values and we augment the index of fully observed rows with an index of non-<code>NULL</code> <code>gender</code> values. Finally, we drop the redundant <code>NULL</code> level from the <code>factor</code> variable <code>gender</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>obs_ind <span class="ot">&lt;-</span> <span class="fu">complete.cases</span>(df) <span class="sc">&amp;</span> demog<span class="sc">$</span>gender <span class="sc">!=</span> <span class="st">"NULL"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>name[<span class="sc">!</span>obs_ind] <span class="co"># indices of observations with missing values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 439 441 442 443</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">|&gt;</span> <span class="fu">filter</span>(obs_ind)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> <span class="fu">filter</span>(obs_ind) <span class="sc">|&gt;</span> <span class="fu">droplevels</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before proceeding any further, it would be prudent to explore the complete data visually, which we do via the matrix of pairwise scatter plots, excluding the <code>name</code> column, in <a href="#fig-pairs">Figure&nbsp;<span>8.1</span></a>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(df[,<span class="sc">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-pairs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-pairs-1.png" class="img-fluid figure-img" width="744"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Matrix of pairwise scatter plots for all variables in the MOOC centralities data set.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From the plots in <a href="#fig-pairs">Figure&nbsp;<span>8.1</span></a>, we can see that there are clearly two quite extreme outliers. Simple exploratory analyses (not shown) confirms that these are the final two rows of the complete data set. These observations are known to correspond to the two instructors who led the discussion. They have been marked using a red cross symbol in <a href="#fig-pairs">Figure&nbsp;<span>8.1</span></a>. Though we have argued that <span class="math inline">\(K\)</span>-Medoids is a more robust clustering method than <span class="math inline">\(K\)</span>-Means, for example, we also remove these observations in order to avoid their detrimental effects on the <span class="math inline">\(K\)</span>-Means output. The rows must be removed from both <code>df</code> and <code>demog</code> so that they can later be merged. With these observations included, <span class="math inline">\(K\)</span>-Means for instance would likely add one additional cluster containing just these two observations, about whom we already know their role differs substantially from the other observations, as they are quite far from the bulk of the data in terms of squared Euclidean distance. That is not to say, however, that there will not still be outliers in <code>df</code> after removing these two cases.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>keep_ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">nrow</span>(df) <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">|&gt;</span> <span class="fu">slice</span>(keep_ind)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>demog <span class="ot">&lt;-</span> demog <span class="sc">|&gt;</span> <span class="fu">slice</span>(keep_ind)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As is good practice when using dissimilarity-based clustering algorithms, we pre-process the purely continuous data by normalising each variable to have a mean of 0 and a variance of 1, by constructing the scaled data frame <code>sdf</code> using the function <code>scale()</code>, again excluding the <code>name</code> column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sdf <span class="ot">&lt;-</span> <span class="fu">scale</span>(df[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">center=</span><span class="cn">TRUE</span>, <span class="at">scale=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>sdf</code> can be used for all clustering methods described herein. To also accommodate the categorical demographic variables, we use <code>merge()</code> to combine both the scaled continuous data and the categorical data. This requires some manipulation of the <code>name</code> column, with which the two data sets are merged, but we ultimately discard the superfluous <code>name</code> column, which we do not want to contribute to any pairwise distance matrices or clustering solutions, from both <code>merged_df</code> and <code>sdf</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>merged_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">name=</span>df<span class="sc">$</span>name, sdf) <span class="sc">|&gt;</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">merge</span>(demog, <span class="at">by=</span><span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, before proceeding to apply various clustering methods to these data, we present summaries of the counts of each level of the categorical variables <code>experience</code>, <code>gender</code>, and <code>region</code>. These variables imply groupings of size three, two, and five, respectively, and it will be interesting to see if this is borne out in any of the mixed-type clustering applications.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>experience)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  1   2   3 
118 150 171 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>gender)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
female   male 
   299    140 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(merged_df<span class="sc">$</span>region)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
International       Midwest     Northeast         South          West 
           32            77           110           168            52 </code></pre>
</div>
</div>
</section>
</section>
<section id="sec-apps" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="sec-apps"><span class="header-section-number">4.2</span> Clustering applications</h3>
<p>We now show how each of the clustering methods described above can be implemented in R, using these data throughout and taking care to address all practical concerns previously raised. For each method —<span class="math inline">\(K\)</span>-Means in <a href="#sec-kmapp"><span>Section&nbsp;8.4.2.1</span></a>, <span class="math inline">\(K\)</span>-Medoids in <a href="#sec-pamapp"><span>Section&nbsp;8.4.2.2</span></a>, and agglomerative hierarchical clustering in <a href="#sec-hcapp"><span>Section&nbsp;8.4.2.3</span></a>— we show clustering results following the method-specific guidelines for choosing <span class="math inline">\(K\)</span>. However, we conclude by comparing results across different methods using the average silhouette width criterion to further guide the choice of <span class="math inline">\(K\)</span> in <a href="#sec-sil"><span>Section&nbsp;8.4.2.4</span></a>. We refrain from providing an interpretation of the uncovered clusters until <a href="#sec-optimal"><span>Section&nbsp;8.4.2.5</span></a>, after the optimal clustering solution is identified.</p>
<p>Before we proceed, we set the seed to ensure that results relying on random number generation are reproducible.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2024</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sec-kmapp" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="sec-kmapp"><span class="header-section-number">4.2.1</span> <span class="math inline">\(K\)</span>-Means application</h4>
<p>We begin by showing a naive use of the <code>kmeans()</code> function, supplying only the scaled data <code>sdf</code> and the pre-specified number of clusters <span class="math inline">\(K\)</span> via the <code>centers</code> argument. For now, we assume for no particular reason that there are <span class="math inline">\(K=3\)</span> clusters, just to demonstrate the use of the <code>kmeans()</code> function and its arguments. A number of aspects of the results are printed when we examine the resulting <code>km1</code> object.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>km1 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="dv">3</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>km1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>K-means clustering with 3 clusters of sizes 48, 129, 262

Cluster means:
    InDegree  OutDegree Closeness_total Betweenness       Eigen
1  2.2941694  1.9432559       1.0923551   1.9697769  2.00747791
2 -0.4088814 -0.4311073      -1.4086380  -0.3975783 -0.55451137
3 -0.2189864 -0.1437536       0.4934399  -0.1651209 -0.09475944
  Diffusion.degree   Coreness Cross_clique_connectivity
1        1.9078646  2.1923592                 2.0066265
2       -1.1038258 -0.5622732                -0.3094092
3        0.1939543 -0.1248092                -0.2152835

Clustering vector:
  [1] 1 2 2 3 1 1 1 1 3 1 1 3 1 1 1 2 1 3 1 2 2 1 2 1 3 1 1 2 1 1 2 3 3 1 1 1 2
 [38] 3 3 2 1 3 2 1 3 3 2 3 1 1 3 3 1 1 2 3 3 1 3 1 1 1 1 1 3 3 1 1 3 3 3 3 2 3
 [75] 3 3 3 3 2 2 3 3 3 2 3 2 3 1 2 3 3 1 2 3 3 3 2 1 3 1 3 3 3 3 3 2 3 2 3 3 2
[112] 3 3 3 1 1 2 2 2 3 3 2 2 3 2 3 3 3 3 2 3 2 3 3 2 3 1 3 3 2 3 3 3 3 2 3 3 3
[149] 2 3 2 3 2 3 3 2 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 3 3
[186] 3 3 3 3 3 2 3 3 3 3 3 2 1 3 3 3 3 3 3 3 3 3 2 2 2 3 3 2 3 2 2 3 3 1 2 3 2
[223] 1 2 2 3 2 2 2 2 2 3 2 1 3 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3
[260] 2 3 2 2 2 3 2 3 2 2 2 3 2 2 2 2 3 2 3 3 3 3 2 2 3 2 2 2 2 2 2 2 3 2 2 3 2
[297] 3 3 3 3 3 3 3 3 3 2 2 3 2 1 2 2 2 3 2 2 3 3 2 3 2 3 2 3 3 3 2 2 3 3 3 2 3
[334] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 2
[371] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
[408] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 3

Within cluster sum of squares by cluster:
[1] 858.90295  72.01655 414.30502
 (between_SS / total_SS =  61.6 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
</div>
<p>Among other things, this output shows the estimated centroid vectors <span class="math inline">\(\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_K\)</span> upon convergence of the algorithm (<code>$centers</code>), an indicator vector showing the assignment of each observation to one of the <span class="math inline">\(K=3\)</span> clusters (<code>$cluster</code>), the size of each cluster in terms of the number of allocated observations (<code>$size</code>), the within-cluster sum-of-squares <em>per cluster</em> (<code>$withinss</code>), and the ratio of the between-cluster sum-of-squares (<code>$betweenss</code>) to the total sum of squares (<code>$totss</code>). Ideally, this ratio should be large, if the total within-cluster sum-of-squares is minimised. We can access the TWCSS quantity by typing</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>km1<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1345.225</code></pre>
</div>
</div>
<p>However, these results were obtained using the default values of ten maximum iterations (<code>iter.max=10</code>, by default) and only one random set of initial centroid vectors (<code>nstart=1</code>, by default). To increase the likelihood of converging to the global minimum, it is prudent to increase <code>iter.max</code> and <code>nstart</code>, to avoid having the algorithm terminate prematurely and avoid converging to a local minimum, as discussed in <a href="#sec-kmpp"><span>Section&nbsp;8.3.1.2.2</span></a>. We use <code>nstart=50</code>, which is reasonably high but not so high as to incur too large a computational burden.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>km2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="dv">3</span>, <span class="at">nstart=</span><span class="dv">50</span>, <span class="at">iter.max=</span><span class="dv">100</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>km2<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1339.226</code></pre>
</div>
</div>
<p>We can see that the solution associated with the best random starting values achieves a lower TWCSS than the earlier naive attempt. Next, we see if an even lower value can be obtained using the <span class="math inline">\(K\)</span>-Means initialisation strategy, by invoking the <code>kmeans_pp()</code> function from <a href="#sec-kmpp"><span>Section&nbsp;8.3.1.2.2</span></a> with <code>K=3</code> centers and supplying these centroid vectors directly to <code>kmeans()</code>, rather than indicating the number of random <code>centers</code> to use.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>km3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">3</span>), <span class="at">iter.max=</span><span class="dv">100</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>km3<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1343.734</code></pre>
</div>
</div>
<p>In this case, using the <span class="math inline">\(K\)</span>-Means initialisation strategy did not further reduce the TWCSS; in fact it is worse than the solution obtained using <code>nstart=50</code>. However, recall that <span class="math inline">\(K\)</span>-Means is itself subject to randomness and should therefore also be run several times, though the number of <span class="math inline">\(K\)</span>-Means runs need not be so high as 50. In the code below, we use <code>replicate()</code> to invoke both <code>kmeans_pp()</code> and <code>kmeans()</code> itself ten times in order to obtain a better solution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>KMPP <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10</span>, <span class="fu">list</span>(<span class="fu">kmeans</span>(sdf, <span class="at">iter.max=</span><span class="dv">100</span>,</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">3</span>))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Among these ten solutions, five are identical and achieve the same minimum TWCSS value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>TWCSS <span class="ot">&lt;-</span> <span class="fu">sapply</span>(KMPP, <span class="st">"[["</span>, <span class="st">"tot.withinss"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>TWCSS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1345.225 1339.226 1339.226 1339.226 1340.457 1339.226 1345.225 1345.225
 [9] 1345.225 1339.226</code></pre>
</div>
</div>
<p>Thereafter, we can extract a solution which minimises the <code>tot.withinss</code> as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>km4 <span class="ot">&lt;-</span> KMPP[[<span class="fu">which.min</span>(TWCSS)]]</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>km4<span class="sc">$</span>tot.withinss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1339.226</code></pre>
</div>
</div>
<p>Finally, this approach resulted in an identical solution to <code>km2</code> being obtained — with just ten runs of <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Means rather than <code>nstart=50</code> runs of the <span class="math inline">\(K\)</span>-Means algorithm alone— which is indeed superior to the solution obtained with just one uninformed random start in <code>km1</code>. We will thus henceforth adopt this initialisation strategy always.</p>
<p>To date, the <span class="math inline">\(K\)</span>-Means algorithm has only been ran with the fixed number of clusters <span class="math inline">\(K=3\)</span>, which may be suboptimal. The following code iterates over a range of <span class="math inline">\(K\)</span> values, storing both the <code>kmeans()</code> output itself and the TWCSS value for each <span class="math inline">\(K\)</span>. The range <span class="math inline">\(K=1,\ldots,10\)</span> notably includes <span class="math inline">\(K=1\)</span>, corresponding to no group structure in the data. The reason for storing the <code>kmeans()</code> output itself is to avoid having to run <code>kmeans()</code> again after determining the optimal <span class="math inline">\(K\)</span> value; such a subsequent run may not converge to the same minimised TWCSS and having to run the algorithm again would be computationally wasteful.</p>
<div class="cell" data-hash="ch8-clus_cache/html/unnamed-chunk-22_c01f3c148c02144b39ff1e647d4224d9">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># set upper limit on range of K values</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>TWCSS <span class="ot">&lt;-</span> <span class="fu">numeric</span>(K) <span class="co"># allocate space for TWCSS estimates</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>KM <span class="ot">&lt;-</span> <span class="fu">list</span>() <span class="co"># allocate space for kmeans() output</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) { <span class="co"># loop over k=1,...,K</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Run K-means using K-Means++ initialisation: </span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use the current k value and do so ten times if k &gt; 1</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>  KMPP    <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="fu">ifelse</span>(k <span class="sc">&gt;</span> <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">1</span>), </span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">list</span>(<span class="fu">kmeans</span>(sdf, <span class="at">iter.max=</span><span class="dv">100</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span>k))))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract and store the solution which minimises the TWCSS</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>  KM[[k]] <span class="ot">&lt;-</span> KMPP[[<span class="fu">which.min</span>(<span class="fu">sapply</span>(KMPP, <span class="st">"[["</span>, <span class="st">"tot.withinss"</span>))]]</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract the TWCSS value for the current value of k</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>  TWCSS[k] <span class="ot">&lt;-</span> KM[[k]]<span class="sc">$</span>tot.withinss</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As previously stated in <a href="#sec-elbow"><span>Section&nbsp;8.3.1.2.1</span></a>, the so-called “elbow method” consists of plotting the range of <span class="math inline">\(K\)</span> values on the x-axis against the corresponding obtained TWCSS values on the y-axis and looking for an kink in the resulting curve.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">1</span><span class="sc">:</span>K, <span class="at">y=</span>TWCSS, <span class="at">type=</span><span class="st">"b"</span>,</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"K"</span>, <span class="at">ylab=</span><span class="st">"Total Within-Cluster</span><span class="sc">\n</span><span class="st"> Sum-of-Squares"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-elbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-elbow-1.png" class="img-fluid figure-img" width="528"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> Elbow plot showing a range of <span class="math inline">\(K\)</span> values against the corresponding obtained TWCSS for <span class="math inline">\(K\)</span>-Means applied to the MOOC centralities data set using <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K\)</span>-Means<sup>++</sup> initialisation.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Here, it appears that <span class="math inline">\(K=4\)</span> would produce the best results: beyond <span class="math inline">\(K=4\)</span>, the decrease in TWCSS is minimal, which suggests that an extra cluster is not required to model the data well; between <span class="math inline">\(K=3\)</span> and <span class="math inline">\(K=4\)</span>, there is a more substantial decrease in TWCSS, which suggests that the fourth cluster is necessary. This method is of course highly subjective, and we will further inform our choice of <span class="math inline">\(K\)</span> for <span class="math inline">\(K\)</span>-Means using silhouette widths and the ASW criterion in <a href="#sec-sil"><span>Section&nbsp;8.4.2.4</span></a>.</p>
<p>For now, we can interrogate the <span class="math inline">\(K\)</span>-Means solution with <span class="math inline">\(K=4\)</span> by examining the sizes of the clusters and their centroid vectors, using the corresponding fourth element of the list <code>KM</code> by setting <code>K &lt;- 4</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>KM[[K]]<span class="sc">$</span>size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 127  57   8 247</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>KM[[K]]<span class="sc">$</span>centers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    InDegree  OutDegree Closeness_total Betweenness      Eigen Diffusion.degree
1 -0.4086612 -0.4381057      -1.4220978  -0.3994123 -0.5596480       -1.1138889
2  1.5706416  1.1372352       0.9247804   1.3859592  1.0781087        1.4216179
3  4.1035975  5.0512503       1.5201941   3.4673658  5.4946785        3.1631065
4 -0.2852445 -0.2007813       0.4685522  -0.2267743 -0.1390054        0.1422138
    Coreness Cross_clique_connectivity
1 -0.5672245                -0.3102236
2  1.7423739                 0.9615041
3  3.4821417                 5.5033583
4 -0.2232184                -0.2406243</code></pre>
</div>
</div>
<p>However, these centroids correspond to the <em>scaled</em> version of the data created in <a href="../ch12-markov/ch12-markov.html#sec-process"><span>Section&nbsp;12.4.3</span></a>. Interpretation can be made more straightforward by undoing the scaling transformation on these centroids, so that they are on the same scale as the data <code>df</code> rather than the scaled data <code>sdf</code> which was used as input to <code>kmeans()</code>. The column-wise means and standard deviations used when <code>scale()</code> was invoked are available as the attributes <code>"scaled:center"</code> and <code>"scaled:scale"</code>, respectively and are used in the code below. We show these centroids rounded to four decimal places in <a href="#tbl-centroids">Table&nbsp;<span>8.1</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>rescaled_centroids <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(KM[[K]]<span class="sc">$</span>centers, <span class="dv">1</span>, <span class="cf">function</span>(r) {</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>                        r <span class="sc">*</span> <span class="fu">attr</span>(sdf, <span class="st">"scaled:scale"</span>) <span class="sc">+</span> <span class="fu">attr</span>(sdf, <span class="st">"scaled:center"</span>) </span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>                        } ))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(rescaled_centroids, <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-centroids" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;1<strong>.</strong> Centroids from the <span class="math inline">\(K=4\)</span> <span class="math inline">\(K\)</span>-Means solution on the original data scale.</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 15%">
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 16%">
<col style="width: 8%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1.1024</td>
<td style="text-align: right;">1.7480</td>
<td style="text-align: right;">0.0008</td>
<td style="text-align: right;">20.7010</td>
<td style="text-align: right;">0.0071</td>
<td style="text-align: right;">144.1732</td>
<td style="text-align: right;">2.6378</td>
<td style="text-align: right;">4.6850</td>
</tr>
<tr class="even">
<td style="text-align: right;">15.3684</td>
<td style="text-align: right;">14.8421</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">849.9328</td>
<td style="text-align: right;">0.0996</td>
<td style="text-align: right;">1370.1053</td>
<td style="text-align: right;">16.1053</td>
<td style="text-align: right;">157.5789</td>
</tr>
<tr class="odd">
<td style="text-align: right;">33.6250</td>
<td style="text-align: right;">47.3750</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">1816.6608</td>
<td style="text-align: right;">0.3492</td>
<td style="text-align: right;">2212.1250</td>
<td style="text-align: right;">26.2500</td>
<td style="text-align: right;">703.6250</td>
</tr>
<tr class="even">
<td style="text-align: right;">1.9919</td>
<td style="text-align: right;">3.7206</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">100.8842</td>
<td style="text-align: right;">0.0308</td>
<td style="text-align: right;">751.5061</td>
<td style="text-align: right;">4.6437</td>
<td style="text-align: right;">13.0526</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Now, we can more clearly see that the first cluster, with <span class="math inline">\(n_{k=1}=127\)</span> observations, has the lowest mean value for all <span class="math inline">\(d=8\)</span> centrality measures, while the last cluster, the largest with <span class="math inline">\(n_{k=4}=247\)</span> observations, has moderately larger values for all centrality measures. The two smaller clusters, cluster two with <span class="math inline">\(n_{k=2}=57\)</span> observations and cluster three with only <span class="math inline">\(n_{k=3}=8\)</span> observations have the second-largest and largest values for each measure, respectively. As previously stated, we defer a more-detailed interpretation of uncovered clusters to <a href="#sec-optimal"><span>Section&nbsp;8.4.2.5</span></a>, after the optimal clustering solution has been identified.</p>
</section>
<section id="sec-pamapp" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="sec-pamapp"><span class="header-section-number">4.2.2</span> <span class="math inline">\(K\)</span>-Medoids application</h4>
<p>The function <code>pam()</code> in the <code>cluster</code> library implements the PAM algorithm for <span class="math inline">\(K\)</span>-Medoids clustering. By default, this function requires only the arguments <code>x</code> (a pre-computed pairwise dissimilarity matrix, as can be created from the functions <code>dist()</code>, <code>daisy()</code>, and more) and <code>k</code>, the number of clusters. However, there are many options for many additional speed improvements and initialisation strategies <span class="citation" data-cites="Schubert2021">[<a href="#ref-Schubert2021" role="doc-biblioref">48</a>]</span>. Here, we invoke a faster variant of the PAM algorithm which necessitates specification of <code>nstart</code> as a number greater than one, to ensure the algorithm is evaluated with multiple random initial medoid vectors, in a similar fashion to <code>kmeans()</code>. Thus, we call <code>pam()</code> with <code>variant="faster"</code> and <code>nstart=50</code> throughout.</p>
<p>Firstly though, we need to construct the pairwise dissimilarity matrices to be used as input to the <code>pam()</code> function. Unlike <span class="math inline">\(K\)</span>-Means, we are not limited to <em>squared</em> Euclidean distances. It is prudent, therefore, to explore <span class="math inline">\(K\)</span>-Medoids solutions with several different dissimilarity measures and compare the different solutions obtained for different measures. Each distance measure will yield different results at the same <span class="math inline">\(K\)</span> value, thus the value of <span class="math inline">\(K\)</span> and the distance measure must be considered as a pair when determining the optimal solution.</p>
<p>We begin by calculating the Euclidean, Manhattan, and Minkowski (with <span class="math inline">\(p=3\)</span>) distances on the scaled continuous data in <code>sdf</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>dist_euclidean <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"euclidean"</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>dist_manhattan <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"manhattan"</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>dist_minkowski3 <span class="ot">&lt;-</span> <span class="fu">dist</span>(sdf, <span class="at">method=</span><span class="st">"minkowski"</span>, <span class="at">p=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Secondly, we avail of another principal advantage of <span class="math inline">\(K\)</span>-Medoids; namely, the ability to incorporate categorical variables in mixed-type data sets, by calculating pairwise Gower distances between each row of <code>merged_df</code>, using <code>daisy()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>dist_gower <span class="ot">&lt;-</span> <span class="fu">daisy</span>(merged_df, <span class="at">metric=</span><span class="st">"gower"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As per the <span class="math inline">\(K\)</span>-Means tutorial in <a href="#sec-kmapp"><span>Section&nbsp;8.4.2.1</span></a>, we can produce an elbow plot by running the algorithm over a range of <span class="math inline">\(K\)</span> values and extracting the minimised within-cluster total distance achieved upon convergence for each value of <span class="math inline">\(K\)</span>. We demonstrate this for the <code>dist_euclidean</code> input below.</p>
<div class="cell" data-hash="ch8-clus_cache/html/unnamed-chunk-30_487e99e4450bc202f415ef979458e7c3">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>WCTD_euclidean <span class="ot">&lt;-</span> <span class="fu">numeric</span>(K)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>pam_euclidean <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>  pam_euclidean[[k]] <span class="ot">&lt;-</span> <span class="fu">pam</span>(dist_euclidean, <span class="at">k=</span>k, <span class="at">variant=</span><span class="st">"faster"</span>, <span class="at">nstart=</span><span class="dv">50</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>  WCTD_euclidean[k] <span class="ot">&lt;-</span> pam_euclidean[[k]]<span class="sc">$</span>objective[<span class="dv">2</span>]</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Equivalent code chunks for the <code>dist_manhattan</code>, <code>dist_minkowski3</code>, and <code>dist_gower</code> inputs are almost identical, so we omit them here for the sake of brevity. Suffice to say, equivalent lists <code>pam_manhattan</code>, <code>pam_minkowski3</code>, and <code>pam_gower</code>, as well as equivalent vectors <code>WCTD_manhattan</code>, <code>WCTD_minkowski3</code>, and <code>WCTD_gower</code>, can all be obtained. Using these objects, corresponding elbow plots for all four dissimilarity measures are showcased in <a href="#fig-pamelbow">Figure&nbsp;<span>8.3</span></a>.</p>
<p>Some of the elbow plots in <a href="#fig-pamelbow">Figure&nbsp;<span>8.3</span></a> are more conclusive than others. As examples, there are reasonably clear elbows at <span class="math inline">\(K=3\)</span> for the Euclidean and Minkowski distances, arguably an elbow at <span class="math inline">\(K=4\)</span> for the Manhattan distance, and no clear, unambiguous elbow under the Gower distance. In any case, the elbow method only helps to identify the optimal <span class="math inline">\(K\)</span> value for a given dissimilarity measure; we defer a discussion of how to choose the overall best <span class="math inline">\(K\)</span>-Medoids clustering solution to later in this tutorial.</p>
<div class="cell" data-hash="ch8-clus_cache/html/fig-pamelbow_e3e6a4214631d2011521959ad5585399">
<div class="cell-output-display">
<div id="fig-pamelbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-pamelbow-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Elbow plots for <span class="math inline">\(K\)</span>-Medoids clustering evaluated with different dissimilarity measures over a range of <span class="math inline">\(K\)</span> values. In clockwise order, beginning with the top-left panel, these are the Euclidean distance, Manhattan distance, Minkowski distance and, for the merged data with additional categorical covariates, the Gower distance.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For now, let’s interrogate the <span class="math inline">\(K=3\)</span> solution obtained using the Euclidean distance. Recall that the results are already stored in the list <code>pam_euclidean</code>, so we merely need to access the third component of that list by setting <code>K &lt;- 3</code>. Firstly, we can examine the size of each cluster by tabulating the cluster-membership indicator vector as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pam_euclidean[[K]]<span class="sc">$</span>clustering)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  1   2   3 
 67 122 250 </code></pre>
</div>
</div>
<p>Examining the medoids which serve as prototypes of each cluster is rendered difficult by virtue of the input having been a distance matrix rather than the data set itself. Though <span class="math inline">\(K\)</span>-Medoids defines the medoids to be the rows in the data set from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised, the <code>medoids</code> component of the output instead gives the <em>indices</em> of the medoids within the data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>pam_euclidean[[K]]<span class="sc">$</span>medoids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  88 272  45</code></pre>
</div>
</div>
<p>Fortunately, these indices within <code>sdf</code> correspond to the same, unscaled observations within <code>df</code>. Allowing for the fact that observations with <code>name</code> greater than the largest index here were removed due to missingness, they are effectively the values of the <code>name</code> column corresponding to the medoids. Thus, we can easily examine the medoids on their original scale. In <a href="#tbl-medoids">Table&nbsp;<span>8.2</span></a>, they include the <code>name</code> column, which was <em>not</em> used when calculating the pairwise distance matrices, and are rounded to four decimal places.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">as.numeric</span>(pam_euclidean[[K]]<span class="sc">$</span>medoids)) <span class="sc">|&gt;</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-medoids" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;2<strong>.</strong> Medoids for the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solution obtained using the Euclidean distance on the original data scale, with the corresponding observation index in the column.</caption>
<colgroup>
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 6%">
<col style="width: 15%">
<col style="width: 8%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">name</th>
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">88</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">675.5726</td>
<td style="text-align: right;">0.1096</td>
<td style="text-align: right;">1415</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">157</td>
</tr>
<tr class="even">
<td style="text-align: right;">272</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.0007</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.0047</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">45</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">29.4645</td>
<td style="text-align: right;">0.0194</td>
<td style="text-align: right;">684</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Thus, we can see that there is a small cluster with <span class="math inline">\(n_{k=1}=67\)</span> observations which has the largest values for all <span class="math inline">\(d=8\)</span> centrality measures, a slightly larger cluster with <span class="math inline">\(n_{k=2}=122\)</span> observations and the lowest values for all variables, and the largest cluster with <span class="math inline">\(n_{k=3}=250\)</span> and intermediate values for all variables. The cluster sizes of the <span class="math inline">\(K\)</span>-Medoids solution being more evenly balanced than the earlier <span class="math inline">\(K\)</span>-Means solution is an artefact of <span class="math inline">\(K\)</span>-Medoids being less susceptible to outlying observations by virtue of not squaring the distances. We can explore this by cross-tabulating the clusters obtained by <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K=4\)</span> and <span class="math inline">\(K\)</span>-Medoids with <span class="math inline">\(K=3\)</span> and the Euclidean distance in <a href="#tbl-crosstab">Table&nbsp;<span>8.3</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pam_euclidean[[<span class="dv">3</span>]]<span class="sc">$</span>clustering,</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>      KM[[<span class="dv">4</span>]]<span class="sc">$</span>cluster)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-crosstab" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;3<strong>.</strong> Cross-tabulation of the clusters obtained by <span class="math inline">\(K\)</span>-Means with <span class="math inline">\(K=4\)</span> (along the columns) and <span class="math inline">\(K\)</span>-Medoids with <span class="math inline">\(K=3\)</span> and the Euclidean distance (along the rows).</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 8%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td>1</td>
<td></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td>2</td>
<td></td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>3</td>
<td></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">245</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>From the cross-tabulation in <a href="#tbl-crosstab">Table&nbsp;<span>8.3</span></a>, we can see that the <span class="math inline">\(n_k=8\)</span> observations in the smallest <span class="math inline">\(K\)</span>-Means cluster were absorbed into a larger cluster under the <span class="math inline">\(K\)</span>-Medoids solution, thereby demonstrating the robustness of <span class="math inline">\(K\)</span>-Medoids to outliers. Otherwise, both solutions are broadly in agreement.</p>
</section>
<section id="sec-hcapp" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="sec-hcapp"><span class="header-section-number">4.2.3</span> Agglomerative hierarchical clustering application</h4>
<p>Performing agglomerative hierarchical clustering is straightforward now that the distance matrices have already been created for the purposes of running <code>pam()</code>. All that is required is to specify the distance matrix and an appropriate linkage criterion as the <code>method</code> when calling <code>hclust()</code>. We demonstrate this below for a subset of all possible distance measure and linkage criterion combinations among those described in <a href="#sec-linkage"><span>Section&nbsp;8.3.2.1</span></a>. Recall that for the Ward criterion, the underlying distance measure is usually assumed to be squared Euclidean and that <code>"ward.D2"</code> is the correct <code>method</code> to use when the Euclidean distances are not already squared. For <code>method="centroid"</code>, we manually square the Euclidean distances.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>hc_minkowski3_complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_minkowski3, <span class="at">method=</span><span class="st">"complete"</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>hc_manhattan_single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_manhattan, <span class="at">method=</span><span class="st">"single"</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>hc_gower_average <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_gower, <span class="at">method=</span><span class="st">"average"</span>)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>hc_euclidean_ward <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidean, <span class="at">method=</span><span class="st">"ward.D2"</span>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>hc_euclidean2_centroid <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidean<span class="sc">^</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"ward.D2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plotting the resulting dendrograms is also straightforward. Simply calling <code>plot()</code> on any of the items above will produce a dendrogram visualisation. We do so here for four of the hierarchical clustering solutions constructed above —the undepicted <code>hc_euclidean2_centroid</code> dendrogram is virtually indistinguishable from that of <code>hc_euclidean_ward</code>— while suppressing the observation indices along the x-axis for clarity by specifying <code>labels=FALSE</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_minkowski3_complete, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Minkwoski Distance (p=3) with Complete Linkage"</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_manhattan_single, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Manhattan Distance with Single Linkage"</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_gower_average, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Gower Distance with Average Linkage"</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc_euclidean_ward, <span class="at">labels=</span><span class="cn">FALSE</span>, </span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Euclidean Distance with the Ward Criterion"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dendro" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-dendro-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> Dendrograms obtained by agglomerative hierarchical clustering with a selection of dissimilarity measures and linkage criteria.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As previously alluded to in <a href="#sec-cutree"><span>Section&nbsp;8.3.2.2</span></a>, some combinations of dissimilarity measure and linkage criterion are liable to produce undesirable results. The susceptibility to outliers of complete linkage clustering is visible in the top-left panel of <a href="#fig-dendro">Figure&nbsp;<span>8.4</span></a>, where just two observations form a cluster at a low height and are never again merged. The tendency of single linkage clustering to exhibit a “chaining” effect whereby all observations are successively merged into just one ever-larger cluster is evident in the top-right panel of <a href="#fig-dendro">Figure&nbsp;<span>8.4</span></a>, and similar behaviour is observed for the Gower distance with average linkage depicted in the bottom-left panel. The most reasonable results appear to arise from using the Ward criterion in conjunction with Euclidean distances.</p>
<p>Taking the set of candidate partitions in <code>hc_euclidean_ward</code>, for the reasons outlined above, all that remains is to cut this dendrogram at an appropriate height. Practitioners have the freedom to explore different levels of granularity in the final partition. <a href="#fig-cutree">Figure&nbsp;<span>8.5</span></a> shows the dendrogram from the bottom-right panel of <a href="#fig-dendro">Figure&nbsp;<span>8.4</span></a> cut horizontally at different heights, indicated by lines of different colours, as well as the corresponding implied <span class="math inline">\(K\)</span> values.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cutree" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-cutree-1.png" class="img-fluid figure-img" width="480"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;5<strong>.</strong> Dendrogram obtained using the Euclidean distance and Ward criterion cut at different heights with the corresponding implied <span class="math inline">\(K\)</span> indicated.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Thereafter, one simply extracts the resulting partition by invoking <code>cutree()</code> with the appropriate height <code>h</code>. For example, to extract the clustering with <span class="math inline">\(K=3\)</span>, which we choose here because of the wide range of heights at which a <span class="math inline">\(K=3\)</span> solution could be obtained:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>hc_ward2 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_euclidean_ward, <span class="at">h=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The object <code>hc_ward2</code> is now simply an vector indicating the cluster-membership of each observation in the data set. We show only the first few, for brevity, and then tabulate this vector to compute the cluster sizes. However, interpretation of these clusters is more difficult than in the case of <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, as there is no centroid or medoid prototype with which to characterise each cluster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(hc_ward2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 2 2 2 1 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(hc_ward2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>hc_ward2
  1   2 
 49 390 </code></pre>
</div>
</div>
<p>In this section, we have not presented an exhaustive evaluation of all possible pairs of dissimilarity measure and linkage criterion, but note that the code to do so is trivial. In any case, much like <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, we must turn to other cluster quality indices to guide the choice of the best overall solution, be that choosing the best distance and linkage settings for agglomerative hierarchical clustering, or choosing the best clustering method in general among several competing methods. We now turn to finding the optimal clustering solution among multiple competing methods, guided by silhouette widths and plots thereof.</p>
</section>
<section id="sec-sil" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="sec-sil"><span class="header-section-number">4.2.4</span> Identifying the optimal clustering solution</h4>
<p>In our application of <span class="math inline">\(K\)</span>-Means, the elbow method appeared to suggest that <span class="math inline">\(K=4\)</span> yielded the best solution. In our applications of <span class="math inline">\(K\)</span>-Medoids, the elbow method suggested different values of <span class="math inline">\(K\)</span> for different distance metrics. Finally, in our applications of hierarchical clustering, we noted that visualising the resulting dendrogram could be used to guide the choice of the height at which to cut to produce a single hard partition of <span class="math inline">\(K\)</span> clusters. Now, we must determine which method yields the overall best solution. Following <a href="#sec-chooseK"><span>Section&nbsp;8.3.3</span></a>, we employ silhouettes for this purpose.</p>
<p>For <span class="math inline">\(K\)</span>-Means and agglomerative hierarchical clustering, silhouettes can be computed using the <code>silhouette</code> function in the <code>cluster</code> library, in which case the function requires two arguments; the integer vector containing the cluster membership labels and an appropriate dissimilarity matrix. Thus, for instance, silhouettes could be obtained easily for the following two examples (using <code>kmeans()</code> and <code>hclust()</code>, respectively).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>kmeans_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">kmeans</span>(sdf, <span class="at">centers=</span><span class="fu">kmeans_pp</span>(sdf, <span class="at">K=</span><span class="dv">4</span>),</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>                                <span class="at">iter.max=</span><span class="dv">100</span>)<span class="sc">$</span>cluster, </span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>                         dist_euclidean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>hclust_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">cutree</span>(<span class="fu">hclust</span>(dist_euclidean, <span class="at">method=</span><span class="st">"ward.D2"</span>), <span class="at">k=</span><span class="dv">2</span>), </span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>                         dist_euclidean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For <span class="math inline">\(K\)</span>-Medoids, it suffices only to supply the output of <code>pam()</code> itself, from which the cluster membership labels and appropriate dissimilarity matrix will be extracted internally, e.g.,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>pam_sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(<span class="fu">pam</span>(dist_euclidean, <span class="at">k=</span><span class="dv">3</span>, <span class="at">variant=</span><span class="st">"faster"</span>, <span class="at">nstart=</span><span class="dv">50</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Thereafter, <code>plot()</code> can be called on <code>kmeans_sil</code>, <code>hclust_sil</code>, or <code>pam_sil</code> to produce a silhouette plot. For an example based on <code>hclust_sil</code>, see <a href="#fig-sil1">Figure&nbsp;<span>8.6</span></a>. Note that as <span class="math inline">\(K=2\)</span> here, <code>col=2:3</code> colours the silhouettes according to their cluster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hclust_sil, <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sil1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-sil1-1.png" class="img-fluid figure-img" width="6300"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;6<strong>.</strong> Silhouette plot for the <span class="math inline">\(K=2\)</span> hierarchical clustering solution obtained using the Ward criterion with Euclidean distances.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-sil1">Figure&nbsp;<span>8.6</span></a> shows that most silhouette widths are positive under this solution, indicating that most observations have been reasonably well-clustered. Cluster 2 appears to be the most cohesive, with a cluster-specific average silhouette width of <span class="math inline">\(0.70\)</span>, while cluster 1 appears to be the least cohesive, with a corresponding average of just <span class="math inline">\(0.24\)</span>. The overall ASW is <span class="math inline">\(0.65\)</span>, as indicated at the foot of the plot.</p>
<p>Given that we adopted a range of <span class="math inline">\(K=1,\ldots,10\)</span> when using <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids, and four different dissimilarity measures when using <span class="math inline">\(K\)</span>-Medoids, we have <span class="math inline">\(50\)</span> non-hierarchical candidate solutions to evaluate, of which some seem more plausible than others according to the respective elbow plots. For agglomerative hierarchical clustering, an exhaustive comparison over a range of <span class="math inline">\(K\)</span> values, for each dissimilarity measure and each linkage criterion, would be far too extensive for the present tutorial. Consequently, we limit our evaluation of silhouettes to the <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids solutions already present in the objects <code>KM</code>, <code>pam_euclidean</code>, <code>pam_manhattan</code>, <code>pam_minkowski3</code>, and <code>pam_gower</code>, and the hierarchical clustering solutions which employ the Ward criterion in conjunction with Euclidean distances (of which we consider a further <span class="math inline">\(10\)</span> solutions, again with <span class="math inline">\(K=1,\ldots,10\)</span>, by considering the <span class="math inline">\(10\)</span> possible associated heights at which the dendrogram can be cut). We limit the hierarchical clustering solutions to those based on the Ward criterion given that single linkage and complete linkage have been shown to be susceptible to chaining and sensitive to outliers, respectively. We use the corresponding pre-computed dissimilarity matrices <code>dist_euclidean</code>, <code>dist_manhattan</code>, <code>dist_minkowski3</code>, and <code>dist_gower</code>, where appropriate throughout.</p>
<p>Though the ASW associated with <code>hclust_sil</code> is given on the associated silhouette plot in <a href="#fig-sil1">Figure&nbsp;<span>8.6</span></a>, we can calculate ASW values for other solutions —which we must do to determine the best solution— without producing individiual silhouette plots. To show how this can be done, we examine the structure of the <code>hclust_sil</code> object, showing only its first few rows.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(hclust_sil)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     cluster neighbor sil_width
[1,]       1        2 0.4408007
[2,]       2        1 0.7416708
[3,]       2        1 0.7401549
[4,]       2        1 0.4696606
[5,]       1        2 0.2494557
[6,]       1        2 0.1915306</code></pre>
</div>
</div>
<p>The columns relate to the cluster to which object <span class="math inline">\(i\)</span> is assigned, the cluster for which the corresponding <span class="math inline">\(b(i)\)</span> was minimised, and the <span class="math inline">\(s(i)\)</span> score itself, respectively. Calculating <code>mean(hclust_sil[,3])</code> will return the ASW. Though the code is somewhat tedious, we calculate the ASW criterion values for all <span class="math inline">\(60\)</span> candidate solutions —that is, six methods evaluated over <span class="math inline">\(K=1,\ldots,10\)</span>— using a small helper function to calculate the ASW for the sake of tidying the code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>ASW <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">mean</span>(x[,<span class="dv">3</span>])</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>silhouettes <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">K=</span><span class="dv">2</span><span class="sc">:</span>K,</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmeans=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(KM[[k]]<span class="sc">$</span>cluster, dist_euclidean))),</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_euclidean=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_euclidean[[k]]))),</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_manhattan=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_manhattan[[k]]))),</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_minkowski3=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_minkowski3[[k]]))),</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmedoids_gower=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(pam_gower[[k]]))),</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">hc_euclidean_ward=</span><span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>K, <span class="cf">function</span>(k) <span class="fu">ASW</span>(<span class="fu">silhouette</span>(<span class="fu">cutree</span>(hc_euclidean_ward, </span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>                                                             k), dist_euclidean))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <a href="#fig-silall">Figure&nbsp;<span>8.7</span></a>, we plot these silhouettes against <span class="math inline">\(K\)</span> using <code>matplot()</code>, omitting the code to do so for brevity.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-silall" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-silall-1.png" class="img-fluid figure-img" width="528"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;7<strong>.</strong> ASW criterion values plotted against <span class="math inline">\(K\)</span> for <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-medoids (with the Euclidean, Manhattan, Minkowski (<span class="math inline">\(p=3\)</span>), and Gower distances), and agglomerative hierarchical clustering based on Euclidean distance and the Ward criterion.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>According to <a href="#fig-silall">Figure&nbsp;<span>8.7</span></a>, there is generally little support for <span class="math inline">\(K &gt; 5\)</span> across almost all methods considered, as most method’s ASW values begin to decline after this point. The ASW values also make clear that incorporating the additional categorical demographic variables in a mixed-type clustering using the Gower distance does not lead to reasonable partitions, for any number of clusters <span class="math inline">\(K\)</span>. Overall, the most promising solutions in terms of having the highest ASW are <span class="math inline">\(K\)</span>-Means, Ward hierarchical clustering, and <span class="math inline">\(K\)</span>-Medoids with the Manhattan distance, all with <span class="math inline">\(K=2\)</span>, and <span class="math inline">\(K\)</span>-Medoids with the Euclidean and Minkowski distances, each with <span class="math inline">\(K=3\)</span>. However, it would be wise to examine silhouette widths in more detail, rather than relying merely on the average. The silhouettes for this hierarchical clustering solution are already depicted in <a href="#fig-sil1">Figure&nbsp;<span>8.6</span></a>, so <a href="#fig-silplots">Figure&nbsp;<span>8.8</span></a> shows individual silhouette widths for the remaining solutions.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-silplots" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-silplots-1.png" class="img-fluid figure-img" width="1440"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;8<strong>.</strong> Silhouette plots showing silhouette widths for a numbering of promising solutions, coloured according to cluster membership.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It is notable that the silhouettes and ASW of the <span class="math inline">\(K=2\)</span> <span class="math inline">\(K\)</span>-Means solution (top-left panel of <a href="#fig-silplots">Figure&nbsp;<span>8.8</span></a>) and the Ward hierarchical clustering solution (<a href="#fig-sil1">Figure&nbsp;<span>8.6</span></a>) appear almost identical (if one accounts for the clusters being relabelled and associated colours being swapped). Indeed, according to a cross-tabulation of their partitions (not shown), their assignments differ for just <span class="math inline">\(4\)</span> out of <span class="math inline">\(n=439\)</span> observations. Despite having the highest ASW, we can justify dismissing these solutions given that <span class="math inline">\(K=2\)</span> was not well-supported by its corresponding elbow plot in <a href="#fig-elbow">Figure&nbsp;<span>8.2</span></a>. Similar logic suggests that the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Means solution and the <span class="math inline">\(K=2\)</span> <span class="math inline">\(K\)</span>-Medoids solution based on the Manhattan distance can also be disregarded. Although we stress again that an ideal analysis would more thoroughly determine an optimal solution with reference to additional cluster quality measures and note that various clustering solutions can be legitimate, for potentially different clustering aims, on the same data set <span class="citation" data-cites="Hennig2015 Hennig2016">[<a href="#ref-Hennig2015" role="doc-biblioref">2</a>, <a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>, we can judge that —among the two <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solutions— the one based on the Euclidean distance is arguably preferable, for two reasons. Firstly, its ASW is quite close to that of the Minkowski distance solution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>silhouettes <span class="sc">|&gt;</span> </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(K <span class="sc">==</span> <span class="dv">3</span>) <span class="sc">|&gt;</span> </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(kmedoids_euclidean, kmedoids_minkowski3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["kmedoids_euclidean"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["kmedoids_minkowski3"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.470844","2":"0.4795972"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Secondly, the first cluster has a higher cluster-specific average silhouette width under the solution based on the Euclidean distance. Indeed, this solution has fewer negative silhouette widths also:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">silhouette</span>(pam_euclidean[[<span class="dv">3</span>]])[,<span class="dv">3</span>] <span class="sc">&lt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 27</code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">silhouette</span>(pam_minkowski3[[<span class="dv">3</span>]])[,<span class="dv">3</span>] <span class="sc">&lt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 28</code></pre>
</div>
</div>
</section>
<section id="sec-optimal" class="level4" data-number="4.2.5">
<h4 data-number="4.2.5" class="anchored" data-anchor-id="sec-optimal"><span class="header-section-number">4.2.5</span> Interpreting the optimal clustering solution</h4>
<p>By now, we have identified that the <span class="math inline">\(K=3\)</span> solution obtained using <span class="math inline">\(K\)</span>-Medoids and the Euclidean distance is optimal. Although aspects of this solution were already discussed in <a href="#sec-pamapp"><span>Section&nbsp;8.4.2.2</span></a> —in particular, <a href="#tbl-medoids">Table&nbsp;<span>8.2</span></a> has already shown the <span class="math inline">\(K=3\)</span> medoid vectors obtained at convergence— we now turn to examining this output in greater detail, in order to provide a fuller interpretation of each of the uncovered clusters. We extract this solution for ease of manipulation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>final_pam <span class="ot">&lt;-</span> pam_euclidean[[<span class="dv">3</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Recall that this method yielded three clusters of sizes <span class="math inline">\(n_1=67\)</span>, <span class="math inline">\(n_2=122\)</span>, and <span class="math inline">\(n_3=250\)</span>. Although the categorical variables were not used by this clustering method, additional interpretative insight can be obtained by augmenting the respective medoids in <a href="#tbl-medoids">Table&nbsp;<span>8.2</span></a> with these cluster sizes and the <code>experience</code> values of the corresponding rows of the <code>merged_df</code> data set, which includes the additional demographic features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span> </span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>medoids)) <span class="sc">|&gt;</span> </span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">4</span>) <span class="sc">|&gt;</span> </span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">size=</span><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering)) <span class="sc">|&gt;</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(<span class="fu">slice</span>(demog <span class="sc">|&gt;</span> </span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">select</span>(UID, experience), </span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>medoids)), </span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">by=</span><span class="fu">join_by</span>(name <span class="sc">==</span> UID)) <span class="sc">|&gt;</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-medexp" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;4<strong>.</strong> Medoids for the <span class="math inline">\(K=3\)</span> <span class="math inline">\(K\)</span>-Medoids solution obtained using the Euclidean distance on the original data scale, augmented with the cluster sizes and the corresponding values of the variable (which was not directly used by the clustering algorithm).</caption>
<colgroup>
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 5%">
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 21%">
<col style="width: 4%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">InDegree</th>
<th style="text-align: right;">OutDegree</th>
<th style="text-align: right;">Closeness_total</th>
<th style="text-align: right;">Betweenness</th>
<th style="text-align: right;">Eigen</th>
<th style="text-align: right;">Diffusion.degree</th>
<th style="text-align: right;">Coreness</th>
<th style="text-align: right;">Cross_clique_connectivity</th>
<th style="text-align: right;">size</th>
<th style="text-align: right;">experience</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">16</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.0011</td>
<td style="text-align: right;">675.5726</td>
<td style="text-align: right;">0.1096</td>
<td style="text-align: right;">1415</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">157</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.0007</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.0047</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.0010</td>
<td style="text-align: right;">29.4645</td>
<td style="text-align: right;">0.0194</td>
<td style="text-align: right;">684</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Interpretation and labeling of the clustering results is the step that follows, with a focus only on the medoid values of the centrality scores (had the optimal solution been obtained by <code>kmeans()</code>, we could instead examine its centroids in its <code>$centers</code> component, i.e., mean vectors). We will follow the example papers that we used as a guide for choosing the centrality measures <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span> and <span class="citation" data-cites="Saqr2022b">[<a href="#ref-Saqr2022b" role="doc-biblioref">61</a>]</span>. Both papers have used traditional centrality measures (e.g., degree, closeness, and betweenness) as well as diffusion centralities (diffusion degree and coreness) to infer students’ roles. Furthermore, the second paper has an extended review of the roles and how they have been inferred from centrality measures, so readers are encouraged to read this review.</p>
<p>As the data shows, the first cluster has the highest degree centrality measures (<code>InDegree</code> and <code>OutDegree</code>), highest betweenness centrality, as well as the highest values of the diffusion centralities (<code>Diffusion_degree</code> and <code>Coreness</code>). These values are concordant with students who were actively engaged, received multiple replies, had their contributions discussed by others, and achieved significant diffusion. All of such criteria are concordant with the role of <em>leaders</em>. It stands to reason that the <em>leaders</em> cluster would be the smallest, with <span class="math inline">\(n_1=67\)</span>.</p>
<p>The third cluster has intermediate values for the degree centralities, high diffusion centrality values, as well as relatively high values of betweenness centrality. Such values are concordant with the role of an active participant who participates and coordinates the discussion. Therefore, we will use the label of <em>coordinators</em>.</p>
<p>Finally, the second cluster has the lowest values for all centrality measures (though its diffusion values are still fairly reasonable). Thus, this cluster could feasibly be labelled as an <em>isolates</em> cluster, gathering participants whose role in the discussions is peripheral at best. Overall, the interpretations of this <span class="math inline">\(K=3\)</span> solution are consistent with other findings in the existing literature, e.g., <span class="citation" data-cites="Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>]</span>.</p>
<p>We can now label the clusters accordingly to facilitate more informative cluster-specific summaries. Here, we also recall the size of each cluster with the new labels invoked, to demonstrate their usefulness.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>final_pam<span class="sc">$</span>clustering <span class="ot">&lt;-</span> <span class="fu">factor</span>(final_pam<span class="sc">$</span>clustering, </span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>                               <span class="at">labels=</span><span class="fu">c</span>(<span class="st">"leaders"</span>, <span class="st">"coordinators"</span>, <span class="st">"isolates"</span>))</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     leaders coordinators     isolates 
          67          122          250 </code></pre>
</div>
</div>
<p>As an example, we can use these labels to guide a study of the mean vectors of each cluster(bearing in mind that these are not centroid centroid vectors obtained by <span class="math inline">\(K\)</span>-Means, but rather mean vectors obtained calculated for each group defined by the <span class="math inline">\(K\)</span>-Medoids solution), for which the interpretation of the leader, coordinator, and isolate labels are still consistent with the conclusions drawn from the medoids in <a href="#tbl-medoids">Table&nbsp;<span>8.2</span></a>. Note that, for the sake of readability, the group-wise summary below is transposed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">|&gt;</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(<span class="at">clusters=</span>final_pam<span class="sc">$</span>clustering) <span class="sc">|&gt;</span> </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>name) <span class="sc">|&gt;</span> </span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise_all</span>(mean) <span class="sc">|&gt;</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.numeric, round, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols=</span><span class="sc">-</span>clusters, </span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to=</span><span class="st">"centrality"</span>) <span class="sc">|&gt;</span>   </span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from=</span>clusters, </span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>              <span class="at">values_from=</span>value) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["centrality"],"name":[1],"type":["chr"],"align":["left"]},{"label":["leaders"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["coordinators"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["isolates"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"InDegree","2":"17.12","3":"1.10","4":"1.98"},{"1":"OutDegree","2":"18.75","3":"1.68","4":"3.62"},{"1":"Closeness_total","2":"0.00","3":"0.00","4":"0.00"},{"1":"Betweenness","2":"942.90","3":"21.19","4":"99.07"},{"1":"Eigen","2":"0.13","3":"0.01","4":"0.03"},{"1":"Diffusion.degree","2":"1466.45","3":"131.82","4":"741.56"},{"1":"Coreness","2":"17.21","3":"2.56","4":"4.58"},{"1":"Cross_clique_connectivity","2":"219.75","3":"4.53","4":"12.62"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>From <a href="#tbl-medexp">Table&nbsp;<span>8.4</span></a>, we can also see that each observation which corresponds to a cluster medoid contains low, high, and medium levels of experience, respectively. However, one should be cautious not to therefore conclude that the clusters neatly map to experience levels, as the following cross-tabulation indicates little-to-no agreement between the groupings of experience levels in the data and the uncovered clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(final_pam<span class="sc">$</span>clustering, </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>      merged_df<span class="sc">$</span>experience,</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn=</span><span class="fu">c</span>(<span class="st">"Clusters"</span>, <span class="st">"Experience"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Experience
Clusters        1  2  3
  leaders      17 23 27
  coordinators 40 34 48
  isolates     61 93 96</code></pre>
</div>
</div>
<p>Finally, we can produce a visualisation of the uncovered clusters in order to better understand the solution. Visualising multivariate data with <span class="math inline">\(d&gt;2\)</span> is challenging and consequently such visualisations must resort to either plotting the first two principal components or mapping the pairwise dissimilarity matrix to a configuration of points in Cartesian space using multidimensional scaling. The latter is referred to as a “CLUSPLOT” <span class="citation" data-cites="Pison1999">[<a href="#ref-Pison1999" role="doc-biblioref">62</a>]</span> and is implemented in the <code>clusplot()</code> function in the same <code>cluster</code> library as <code>pam()</code> itself. This function uses classical (metric) multi-dimensional scaling <span class="citation" data-cites="Mead1992">[<a href="#ref-Mead1992" role="doc-biblioref">63</a>]</span> to create a bivariate plot displaying the partition of the data. Observations are represented by points in the scatter plot an ellipse spanning the smallest area containing all points in the given cluster is drawn around each cluster. In the code below, only <code>clusplot(final_pam)</code> is strictly necessary to produce such a plot for the optimal <span class="math inline">\(K=3\)</span> Euclidean distance <span class="math inline">\(K\)</span>-Medoids solution; all other arguments are purely for cosmetic purposes for the sake of the resulting <a href="#fig-clusplot">Figure&nbsp;<span>8.9</span></a> and are described in <code>?clusplot</code>.</p>
<p><a href="#fig-clusplot">Figure&nbsp;<span>8.9</span></a> shows that the <em>leaders</em> cluster —the smallest cluster with the highest value for all centrality measures— is quite diffuse. Conversely, the larger <em>coordinators</em> cluster, with the smallest values for all centrality measures, and the <em>isolates</em> cluster, the largest of all, with intermediate values for all centrality measures, are more compact. This is consistent with the cluster-specific average silhouette widths shown in the bottom-right panel of <a href="#fig-silplots">Figure&nbsp;<span>8.8</span></a>. That being said, the large span of the <em>leaders</em> cluster again affirms the relative robustness of <span class="math inline">\(K\)</span>-Medoids to outliers, of which some (all of which are leaders) still remain despite the earlier pre-processing steps.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clusplot</span>(final_pam,                                    <span class="co"># the pam() output</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">main=</span><span class="st">""</span>, <span class="at">sub=</span><span class="cn">NA</span>,            <span class="co"># remove the main title and subtitle</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">lines=</span><span class="dv">0</span>,                     <span class="co"># do not draw any lines on the plot</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">labels=</span><span class="dv">4</span>,                              <span class="co"># only label the ellipses</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">col.clus=</span><span class="st">"black"</span>,               <span class="co"># colour for ellipses and labels</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">col.p=</span><span class="fu">as.numeric</span>(final_pam<span class="sc">$</span>clustering) <span class="sc">+</span> <span class="dv">1</span>, <span class="co"># colours for points</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex.txt=</span><span class="fl">0.75</span>,                      <span class="co"># control size of text labels</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">17</span>)          <span class="co"># expand x-axis to avoid trimming labels</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>         )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-clusplot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch8-clus_files/figure-html/fig-clusplot-1.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;9<strong>.</strong> Two-dimensional clustering plot for the final <span class="math inline">\(K=3\)</span> Euclidean distance <span class="math inline">\(K\)</span>-Medoids solution obtained via classical multidimensional scaling. The ellipses around each cluster are given the associated labels of , , and and the points are coloured according to cluster membership.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-disc" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-disc"><span class="header-section-number">5</span> Discussion &amp; further readings</h2>
<p>The present analysis of the MOOC centralities data set incorporated three of the most commonly used dissimilarity-based clustering approaches; namely, <span class="math inline">\(K\)</span>-Means, <span class="math inline">\(K\)</span>-Medoids, and agglomerative hierarchical clustering. Throughout the applications, emphasis was placed on the sensitivity of the results to various choices regarding algorithmic inputs available to practitioners, be that the choice of how to choose initial centroids for <span class="math inline">\(K\)</span>-Means, the choice of dissimilarity measure for <span class="math inline">\(K\)</span>-Medoids, or the choice of linkage criterion for hierarchical clustering, as examples. Consequently, our analysis considered different values of <span class="math inline">\(K\)</span>, different distances, different linkage criteria, different combinations thereof, and indeed different clustering algorithms entirely, in an attempt to uncover the “best” clustering solution for the MOOC centralities data set. We showed how elbow plots and other graphical tools can guide the choice of <span class="math inline">\(K\)</span> for a given method but ultimately identified —via the average silhouette width criterion— an optimal solution with <span class="math inline">\(K=3\)</span>, using <span class="math inline">\(K\)</span>-Medoids in conjunction with the Euclidean distance measure. Although we note that there are a vast array of other cluster quality measures in the literature which target different definitions of what constitutes a “good” clustering <span class="citation" data-cites="Hennig2016">[<a href="#ref-Hennig2016" role="doc-biblioref">3</a>]</span>, the solution we obtained seems reasonable, in that the uncovered clusters which we labelled as <em>leaders</em>, <em>coordinators</em>, and <em>isolates</em> are consistent, from an interpretative point of view, with existing educational research. Indeed, several published studies have uncovered similar patterns of three groups which can be labelled in the same way <span class="citation" data-cites="KIM201962 Saqr2020-vr Saqr2021b">[<a href="#ref-Saqr2021b" role="doc-biblioref">28</a>, <a href="#ref-KIM201962" role="doc-biblioref">64</a>, <a href="#ref-Saqr2020-vr" role="doc-biblioref">65</a>]</span>.</p>
<p>Nonetheless, there are some limitations to our applications of dissimilarity-based clustering methods in this tutorial which are worth mentioning. Firstly, we note firstly that the decision to standardise the variables —in particular, to normalise them by subtracting their means and dividing by their standard deviations— was applied across the board to each clustering method we explored. Although the standardised data were only used as inputs to each algorithm (i.e., the output was always interpreted on the original scale), we note that different groups are liable to be uncovered with different standardisation schemes. In other words, not standardising the data, or using some other standardisation method (e.g., rescaling to the <span class="math inline">\([0,1]\)</span> range) may lead to different, possibly more or less meaningful clusters.</p>
<p>A second limitation is that all variables in the data were used as inputs (either directly in the case of <span class="math inline">\(K\)</span>-Means, or indirectly as inputs to the pairwise dissimilarity matrix calculations required for <span class="math inline">\(K\)</span>-Medoids and hierarchical clustering). As well as increasing the computational burden, using all variables can be potentially problematic in cases where the clustering structure is driven by some <span class="math inline">\(d^\star&lt; d\)</span> variables, i.e., if the data can be separated into homogeneous subgroups along fewer dimensions. In such instances where some of the variables are uninformative in terms of explaining the variability in the data, variable selection strategies tailored to the unsupervised paradigm may be of interest and again may lead to more meaningful clusters being found <span class="citation" data-cites="Witten2010 Hancer2020">[<a href="#ref-Witten2010" role="doc-biblioref">66</a>, <a href="#ref-Hancer2020" role="doc-biblioref">67</a>]</span>.</p>
<p>Although dissimilarity-based clustering encompasses a broad range of flexible methodologies which can be utilised in other, diverse settings —for example, dissimilarity-based clustering is applied in the context of longitudinal categorical data in the chapter on sequence analysis methods [Chapter 10; <span class="citation" data-cites="Saqr2024-tv">[<a href="#ref-Saqr2024-tv" role="doc-biblioref">18</a>]</span>]— there are other clustering paradigms which may be of interest in similar or alternative settings as the data used in the present tutorial, even if they are not yet widely adopted in educational research. We now briefly introduce some of these alternative clustering frameworks for readers interested in expanding their knowledge of the topic of clustering beyond the dissimilarity-based framework detailed herein.</p>
<p>A first alternative to dissimilarity-based clustering is the density-based clustering framework, most famously exemplified by the DBSCAN clustering algorithm <span class="citation" data-cites="Ester1996 Hahsler2019">[<a href="#ref-Ester1996" role="doc-biblioref">68</a>, <a href="#ref-Hahsler2019" role="doc-biblioref">69</a>]</span>. Density-based clustering broadly defines clusters as areas of higher density than the remainder of the data set, where objects are closely packed together with many nearby neighbours, while objects in the sparse areas which separate clusters are designated as outliers. This has been identified as one main advantage of DBSCAN by authors who applied it an education research context <span class="citation" data-cites="Du2021">[<a href="#ref-Du2021" role="doc-biblioref">70</a>]</span> —insofar as the capability to easily and effectively separate individual exceptionally poor or exceptionally outstanding students who require special attention from teachers is desirable— along with DBSCAN obviating the need to pre-specify the number of clusters <span class="math inline">\(K\)</span>.</p>
<p>Another alternative is given by the model-based clustering paradigm, which is further discussed in Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span>. Although we direct readers to that chapter for a full discussion of model-based clustering using finite Gaussian mixture models, the relationship between this approach and latent profile analysis, and a tutorial in R using the <code>mclust</code> package <span class="citation" data-cites="Scrucca2016">[<a href="#ref-Scrucca2016" role="doc-biblioref">71</a>]</span>, there are some aspects and advantages which are pertinent to discuss here. Firstly, as model-based clustering is based on an underlying generative probability model, rather than relying on dissimilarity-based heuristics, it admits the use of principled, likelihood-based model-selection criteria such as the Bayesian information criterion <span class="citation" data-cites="Schwarz1978">[<a href="#ref-Schwarz1978" role="doc-biblioref">72</a>]</span>, thereby eliminating the subjectivity of elbow plots and other graphical tools for guiding the choice of <span class="math inline">\(K\)</span>. Secondly, such models can be extended to allow covariates to guide the construction of the clusters <span class="citation" data-cites="Murphy2020">[<a href="#ref-Murphy2020" role="doc-biblioref">73</a>]</span>, thereby enabling, for example, incorporation of the categorical demographic variables associated with the MOOC centralities data set used in the present tutorial.</p>
<p>A third advantage of model-based clustering is that it returns a “soft” partition, whereas dissimilarity-based approaches yield either a single “hard” partition (with each observation placed in one group only), under partitional methods like <span class="math inline">\(K\)</span>-Means or <span class="math inline">\(K\)</span>-Medoids, or a set of nested hard partitions from which a single hard partition can be extracted, under hierarchical clustering. Specifically, model-based clustering assigns each observation a probability of belonging to each cluster, such that observations can have a non-negative association with more than one cluster and the uncertainty of the cluster memberships can be quantified. This has the effect of diminishing the effect of outliers or observations which lie on the boundary of two or more natural clusters, as they are not forced to wholly belong to one cluster. In light of these concerns, another clustering paradigm of potential interest is that of fuzzy clustering, which is notable for allowing for “soft” cluster-membership assignments while still being dissimilarity-based <span class="citation" data-cites="Kaufman1990-fanny Durso2016">[<a href="#ref-Kaufman1990-fanny" role="doc-biblioref">74</a>, <a href="#ref-Durso2016" role="doc-biblioref">75</a>]</span>. Indeed, there are fuzzy variants of the <span class="math inline">\(K\)</span>-Means and <span class="math inline">\(K\)</span>-Medoids algorithms which relax the assumption that the latent variable <span class="math inline">\(\mathbf{z}_i\)</span> encountered in <a href="#eq-kmeans_objective">Equation&nbsp;<span>8.1</span></a>, for example, is binary. They are implemented in the functions <code>FKM()</code> and <code>FKM.med()</code>, respectively, in the <code>fclust</code> R package <span class="citation" data-cites="fclust2019">[<a href="#ref-fclust2019" role="doc-biblioref">76</a>]</span>.</p>
<p>Another advantage of model-based clustering over the dissimilarity-based paradigm is the flexibility it affords in relation to the shapes, orientations, volumes, and sizes of the clusters it uncovers. At their most flexible, finite Gaussian mixture models can find clusters where all of these characteristics differ between each cluster, but intermediary configurations —whereby, as but one example, clusters can be constrained to have equal volume and orientation but varying shape and size— are permissible. This is achieved via different parsimonious parameterisations of the cluster-specific covariance matrices which control the geometric characteristics of the corresponding ellipsoids; see Chapter 9 <span class="citation" data-cites="Scrucca2024-mbc">[<a href="#ref-Scrucca2024-mbc" role="doc-biblioref">8</a>]</span> for details. By contrast, <span class="math inline">\(K\)</span>-Means is much more restrictive. The algorithm assumes that clusters are of similar size, even if the estimated clusters can vary in size upon convergence. Moreover, by relying on squared Euclidean distances to a mean vector, with no recourse to modelling covariances, <span class="math inline">\(K\)</span>-Means implicitly assumes that all clusters are spherical in shape, have equal volume, and radiate around their centroid. This can have the damaging consequence, in more challenging applications, that a a larger number of spherical clusters may be required to fit the data well, rather than a more parsimonious and easily interpretable mixture model with fewer non-spherical components. Finally, in light of these concerns, we highlight the spectral clustering approach <span class="citation" data-cites="Ng2001">[<a href="#ref-Ng2001" role="doc-biblioref">77</a>]</span>, implemented via the function <code>specc()</code> in the <code>kernlab</code> package in R, which is, like model-based clustering, capable of uncovering clusters with more flexible shapes, particularly when the data are not linearly separable, and shares some connections with kernel <span class="math inline">\(K\)</span>-Means <span class="citation" data-cites="Dhillon2004">[<a href="#ref-Dhillon2004" role="doc-biblioref">78</a>]</span>, another flexible generalisation of the standard <span class="math inline">\(K\)</span>-Means algorithm adopted in this tutorial.</p>
<p>Overall, we encourage readers to further explore the potential of dissimilarity-based clustering methods, bear in mind the limitations and practical concerns of each algorithm discussed in this tutorial, and remain cognisant of the implications thereof for results obtained in educational research applications. We believe that by paying particular attention to the guidelines presented for choosing an optimal partition among multiple competing methodologies, with different numbers of clusters and/or different dissimilarity measures and/or different linkage criteria, more meaningful and interpretable patterns of student behaviour can be found. Finally, we hope that the additional references provided to other clustering frameworks will inspire a broader interest in the topic of cluster analysis among practitioners in the field of education research.</p>


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Everitt2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Everitt BS, Landau S, Leese M, Stahl D (2011) <span>Cluster Analysis</span>, Fifth. John Wiley &amp; Sons, New York, NY, U.S.A.</div>
</div>
<div id="ref-Hennig2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Hennig C (2015) What are the true clusters? Pattern Recognition Letters 64:53–62</div>
</div>
<div id="ref-Hennig2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Hennig C (2016) Clustering strategy and method selection. In: Hennig C, Meila M, Murtagh F, Rocci R (eds) <span class="nocase">Handbook of Cluster Analysis</span>. Chapman; Hall/CRC Press, New York, N.Y., U.S.A., pp 703–730</div>
</div>
<div id="ref-MacQueen1967" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">MacQueen JB (1967) Some methods for classification and analysis of multivariate observations. In: Cam LML, Neyman J (eds) <span class="nocase">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</span>. University of California Press, June 21–July 18, 1965; December 27 1965–January 7, 1966, Statistical Laboratory of the University of California, Berkeley, CA, U.S.A., pp 281–297</div>
</div>
<div id="ref-Kaufman1990-pam" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Partitioning around medoids (program <span>PAM</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 68–125</div>
</div>
<div id="ref-Kaufman1990-agnes" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Agglomerative nesting (program <span>AGNES</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 199–252</div>
</div>
<div id="ref-R2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">R Core Team (2023) <a href="https://www.R-project.org/">R: A language and environment for statistical computing</a>. R Foundation for Statistical Computing, Vienna, Austria</div>
</div>
<div id="ref-Scrucca2024-mbc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Scrucca L, Saqr M, López-Pernas S, Murphy K (2024) An introduction and <span>R</span> tutorial to model-based clustering in education via latent profile analysis. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Bouveyron2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Bouveyron C, Celeux G, Murphy TB, Raftery AE (2019) <span class="nocase">Model-Based Clustering and Classification for Data Science: With Applications in R</span>. Cambridge University Press, Cambridge, UK</div>
</div>
<div id="ref-Rennenallhoff1983" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Rennen-Allhoff B, Allhoff P (1983) Clusteranalysen bei psychologisch-p<span>ä</span>dagogischen <span>F</span>ragestellungen. Psychologie in Erziehung und Unterricht 30:253–261</div>
</div>
<div id="ref-Hickendorff2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Hickendorff M, Edelsbrunner PA, McMullen J, Schneider M, Trezise K (2018) Informative tools for characterizing individual differences in learning: Latent class, latent profile, and latent transition analysis. Learning and Individual Differences 66:4–15</div>
</div>
<div id="ref-Saqr2021a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2021) The longitudinal trajectories of online engagement over a full program. Computers &amp; Education 175:104325</div>
</div>
<div id="ref-Cook2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Cook CR, Kilgus SP, Burns MK (2018) Advancing the science and practice of precision education to enhance student outcomes. Journal of School Psychology 66:4–10</div>
</div>
<div id="ref-Howard2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Howard MC, Hoffman ME (2018) Variable-centered, person-centered, and person-specific approaches: Where theory meets the method. Organizational Research Methods 21:846–876</div>
</div>
<div id="ref-Richters2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Richters JE (2021) Incredible utility: The lost causes and causal debris of psychological science. Basic and Applied Social Psychology 43:366–405</div>
</div>
<div id="ref-Saqr2023a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2023) The temporal dynamics of online problem-based learning: Why and when sequence matters. International Journal of Computer-Supported Collaborative Learning 18:11–37</div>
</div>
<div id="ref-Dutt2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Dutt A (2015) Clustering algorithms applied in educational data mining. International Journal of Information and Electronics Engineering 5:112–116</div>
</div>
<div id="ref-Saqr2024-tv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Helske S, Durand M, Murphy K, Studer M, Ritschard G (2024) Sequence analysis: Basic principles, technique, and tutorial. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Beder1990" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Beder HW, Valentine T (1990) Motivational profiles of adult basic education students. Adult Education Quarterly 40:78–94</div>
</div>
<div id="ref-Clement1994" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Clément R, Dörnyei Z, Noels KA (1994) Motivation, self‐confidence, and group cohesion in the foreign language classroom. Language Learning 44:417–448</div>
</div>
<div id="ref-Fernandez-Rio2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Fernandez-Rio J, Méndez-Giménez A, Cecchini Estrada JA (2014) A cluster analysis on students’ perceived motivational climate. Implications on psycho-social variables. The Spanish Journal of Psychology 17:E18</div>
</div>
<div id="ref-Cahapin2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Cahapin EL, Malabag BA, Santiago J Cereneo Sailog, Reyes JL, Legaspi GS, Adrales KL (2023) Clustering of students admission data using <span class="math inline">\(K\)</span>-means, hierarchical, and <span>DBSCAN</span> algorithms. Bulletin of Electrical Engineering and Informatics 12:3647–3656</div>
</div>
<div id="ref-Saqr2022a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Saqr M, Tuominen V, Valtonen T, Sointu E, Väisänen S, Hirsto L (2022) Teachers’ learning profiles in learning programming: The big picture! Frontiers in Education 7:1–10</div>
</div>
<div id="ref-Jovanovic2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Jovanović J, Gašević D, Dawson S, Pardo A, Mirriahi N (2017) Learning analytics to unveil learning strategies in a flipped classroom. The Internet and Higher Education 33:74–85</div>
</div>
<div id="ref-Lopez-Pernas2021a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">López-Pernas S, Saqr M (2021) Bringing synchrony and clarity to complex multi-channel data: A learning analytics study in programming education. IEEE Access 9:166531–166541</div>
</div>
<div id="ref-Lopez-Pernas2021b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Viberg O (2021) Putting it all together: Combining learning analytics methods and data sources to understand students’ approaches to learning programming. Sustainability 13:4825</div>
</div>
<div id="ref-Fan2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Fan Y, Tan Y, Raković M, Wang Y, Cai Z, Shaffer DW, Gašević D (2022) Dissecting learning tactics in <span>MOOC</span> using ordered network analysis. Journal of Computer Assisted Learning 39:154–166</div>
</div>
<div id="ref-Saqr2021b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2021) Modelling diffusion in computer-supported collaborative learning: A large scale learning analytics study. International Journal of Computer-Supported Collaborative Learning 16:441–483</div>
</div>
<div id="ref-Perera2009" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Perera D, Kay J, Koprinska I, Yacef K, Zaïane OR (2009) Clustering and sequential pattern mining of online collaborative learning data. IEEE Transactions on Knowledge and Data Engineering 21:759–772</div>
</div>
<div id="ref-Saqr2023b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Jovanović J, Gašević D (2023) Intense, turbulent, or wallowing in the mire: A longitudinal study of cross-course online tactics, strategies, and trajectories. The Internet and Higher Education 57:100902</div>
</div>
<div id="ref-Roque2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Vieira Roque F, Cechinel C, Merino E, Villarroel R, Lemos R, Munoz R (2018) Using multimodal data to find patterns in student presentations. In: <span class="nocase">2018 XIII Latin American Conference on Learning Technologies (LACLO)</span>. São Paulo, Brazil, pp 256–263</div>
</div>
<div id="ref-Lee2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Lee J-E, Chan JY-C, Botelho A, Ottmar E (2022) Does slow and steady win the race?: Clustering patterns of students’ behaviors in an interactive online mathematics game. Educational Technology Research and Development 70:1575–1599</div>
</div>
<div id="ref-Lopez-Pernas2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Gordillo A, Barra E (2022) A learning analytics perspective on educational escape rooms. Interactive Learning Environments 1–17</div>
</div>
<div id="ref-Rosa2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Rosa PJ, Morais D, Gamito P, Oliveira J, Saraiva T (2016) The immersive virtual reality experience: A typology of users revealed through multiple correspondence analysis combined with cluster analysis technique. Cyberpsychology, Behavior and Social Networking 19:209–216</div>
</div>
<div id="ref-Wang2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Wang X, Liu Q, Pang H, Tan SC, Lei J, Wallace MP, Li L (2023) What matters in <span class="nocase">AI-supported</span> learning: A study of <span class="nocase">human-AI</span> interactions in language learning using cluster analysis and epistemic network analysis. Computers &amp; Education 194:104703</div>
</div>
<div id="ref-cluster2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Maechler M, Rousseeuw P, Struyf A, Hubert M, Hornik K (2022) <a href="\url{https://CRAN.R-project.org/package=cluster}"><span class="nocase">cluster: cluster analysis basics and extensions</span></a></div>
</div>
<div id="ref-Lloyd1982" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Lloyd SP (1982) Least squares quantization in <span>PCM</span>. IEEE Transactions on Information Theory 28:129–137</div>
</div>
<div id="ref-Forgy1965" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline">Forgy EW (1965) Cluster analysis of multivariate data: Efficiency vs interpretability of classifications. Biometrics 21:768–769</div>
</div>
<div id="ref-Hartigan1979" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline">Hartigan JA, Wong MA (1979) Algorithm <span class="nocase">AS 136: a <span class="math inline">\(K\)</span>-M</span>eans clustering algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics) 28:100–108</div>
</div>
<div id="ref-Arthur2007" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline">Arthur D, Vassilvitskii S (2007) <span class="math inline">\(K\)</span>-means<sup>++</sup>: The advantages of careful seeding. In: <span class="nocase">SODA ’07: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</span>. Society for Industrial; Applied Mathematics, Philadelphia, PA, U.S.A., pp 1027–1035</div>
</div>
<div id="ref-Hamming1950" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline">Hamming RW (1950) <span>E</span>rror detecting and error correcting codes. The Bell System Technical Journal 29:147–160</div>
</div>
<div id="ref-Jaccard1901" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline">Jaccard P (1901) Distribution de la flore alpine dans le bassin des <span>D</span>ranses et dans quelqus r<span>é</span>gions voisines. Bulletin de la Soci<span>é</span>t<span>é</span> Vaudoise des Sciences Naturelles 37:241–272</div>
</div>
<div id="ref-Dice1945" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline">Dice LR (1945) Measures of the amount of ecologic association between species. Ecology 26:397–302</div>
</div>
<div id="ref-Sorensen1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline">Sørensen T (1948) A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on <span>D</span>anish commons. Kongelige Danske Videnskabernes Selskab 5:1–34</div>
</div>
<div id="ref-Gower1971" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline">Gower JC (1971) A general coefficient of similarity and some of its properties. Biometrics 27:857–871</div>
</div>
<div id="ref-Huang1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline">Huang Z (1998) Extensions to the k-means algorithm for clustering large data sets with categorical values. Data Mining and Knowledge Discovery 2:283–304</div>
</div>
<div id="ref-Huang1997" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline">Huang Z (1997) Clustering large data sets with mixed numeric and categorical values. In: Lu H, Motoda H, Luu H (eds) <span class="nocase">KDD: Techniques and Applications</span>. World Scientific, Singapore</div>
</div>
<div id="ref-Schubert2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline">Schubert E, Rousseeuw PJ (2021) Fast and eager <span><span class="math inline">\(K\)</span>-M</span>edoids clustering: <span><span class="math inline">\(\mathcal{O}(K)\)</span></span> runtime improvement of the <span class="nocase">PAM, CLARA, and CLARANS</span> algorithms. Information Systems 101:101804</div>
</div>
<div id="ref-Kaufman1990-diana" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Divisive analysis (program <span>DIANA</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 253–279</div>
</div>
<div id="ref-Gilpin2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline">Gilpin S, Qian B, Davidson I (2013) Efficient hierarchical clustering of large high dimensional datasets. In: <span class="nocase">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</span>. Association for Computing Machinery, New York, NY, U.S.A., pp 1371–1380</div>
</div>
<div id="ref-Bouguettaya2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline">Bouguettaya A, Yu Q, Liu X, Zhou X, Song A (2015) Efficient agglomerative hierarchical clustering. Expert Systems with Applications 42:2785–2797</div>
</div>
<div id="ref-Ward1963" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline">Ward, Jr. JH (1963) Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association 58:236–244</div>
</div>
<div id="ref-Murtagh2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline">Murtagh F, Legendre P (2014) Ward’s hierarchical agglomerative clustering method: Which algorithms implement <span>W</span>ard’s criterion? Journal of Classification 31:274–295</div>
</div>
<div id="ref-Rand1971" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline">Rand WM (1971) Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association 66:846–850</div>
</div>
<div id="ref-Hubert1985" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline">Hubert L, Arabie P (1985) Comparing partitions. Journal of Classification 2:193–218</div>
</div>
<div id="ref-Rousseeuw1987" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline">Rousseeuw PJ (1987) Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Computational and Applied Mathematics 20:53–65</div>
</div>
<div id="ref-tidyverse2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline">Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019) Welcome to the <span class="nocase">tidyverse</span>. Journal of Open Source Software 4:1686</div>
</div>
<div id="ref-rio2023" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline">Chan C, Leeper TJ, Becker J, Schoch D (2023) <a href="\url{https://cran.r-project.org/package=rio}"><span class="nocase">rio: a Swiss-army knife for data file </span></a></div>
</div>
<div id="ref-Lopez-Pernas2024-dat" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline">López-Pernas S, Saqr M, Conde J, Del-Río-Carazo L (2024) A broad collection of datasets for educational research training and application. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Saqr2024-sna" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Conde M Ángel, Hernández-García Ángel (2024) Social betwork analysis: A primer, a guide and a tutorial in <span>R</span>. In: Saqr M, López-Pernas S (eds) <span class="nocase">Learning Analytics Methods and Tutorials: A Practical Guide using R</span>. Springer</div>
</div>
<div id="ref-Saqr2022b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2022) How CSCL roles emerge, persist, transition, and evolve over time: A four-year longitudinal study. Computers &amp; Education 189:104581</div>
</div>
<div id="ref-Pison1999" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline">Pison G, Struyf A, Rousseeuq PJ (1999) Displaying a clustering with <span>CLUSPLOT</span>. Computational Statistics and Data Analysis 30:381–392</div>
</div>
<div id="ref-Mead1992" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline">Mead A (1992) Review of the development of multidimensional scaling methods. Journal of the Royal Statistical Society: Series D (The Statistician) 41:27–39</div>
</div>
<div id="ref-KIM201962" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline">Kim MK, Ketenci T (2019) Learner participation profiles in an asynchronous online collaboration context. The Internet and Higher Education 41:62–76</div>
</div>
<div id="ref-Saqr2020-vr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline">Saqr M, Viberg O (2020) Using diffusion network analytics to examine and support knowledge construction in <span>CSCL</span> settings. In: Alario-Hoyos C, Rodríguez-Triana MJ, Scheffel M, Arnedillo-Sánchez I, Dennerlein SM (eds) <span class="nocase">Addressing Global Challenges and Quality Education: Proceedings of the 15th European Conference on Technology Enhanced Learning, <span>EC-TEL 2020</span>, September 14–18, 2020</span>. Springer, Cham, Switzerland, pp 158–172</div>
</div>
<div id="ref-Witten2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline">Witten DM, Tibshirani R (2010) A framework for feature selection in clustering. Journal of the American Statistical Association 105:713–726</div>
</div>
<div id="ref-Hancer2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline">Hancer E, Xue B, Zhang M (2020) A survey on feature selection approaches for clustering. Artificial Intelligence Review 53:4519–4545</div>
</div>
<div id="ref-Ester1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline">Ester M, Kriegel H-P, Sander J, Xu X (1996) A density-based algorithm for discovering clusters in large spatial databases with noise. In: Simoudis E, Han Jiawei, Fayyad UM (eds) <span class="nocase">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</span>. AAAI Press, Portland, OR, U.S.A., pp 226–231</div>
</div>
<div id="ref-Hahsler2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline">Hahsler M, Piekenbrock M, Doran D (2019) <span class="nocase">dbscan</span>: Fast density-based clustering with <span>R</span>. Journal of Statistical Software 91:1–30</div>
</div>
<div id="ref-Du2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline">Du H, Chen S, Niu H, Li Y (2021) Application of <span>DBSCAN</span> clustering algorithm in evaluating students’ learning status. In: <span class="nocase">Proceedings of the 17th International Conference on Computational Intelligence and Security, November 19–22, 2021</span>. Chengdu, China, pp 372–376</div>
</div>
<div id="ref-Scrucca2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline">Scrucca L, Fop M, Murphy TB, Raftery AE (2016) <span class="nocase">mclust</span> 5: Clustering, classification and density estimation using <span>G</span>aussian finite mixture models. The <span>R</span> Journal 8:289–317</div>
</div>
<div id="ref-Schwarz1978" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline">Schwarz GE (1978) Estimating the dimension of a model. The Annals of Statistics 6:461–464</div>
</div>
<div id="ref-Murphy2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline">Murphy K, Murphy TB (2020) Gaussian parsimonious clustering models with covariates and a noise component. Advances in Data Analysis and Classification 14:293–325</div>
</div>
<div id="ref-Kaufman1990-fanny" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline">Kaufman L, Rousseeuw PJ (1990) Fuzzy analysis (program <span>FANNY</span>). In: Kaufman L, Rousseeuw PJ (eds) <span class="nocase">Finding Groups in Data: An Introduction to Cluster Analysis</span>. John Wiley &amp; Sons, New York, NY, U.S.A., pp 164–198</div>
</div>
<div id="ref-Durso2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline">D’Urso P (2016) Fuzzy clustering. In: Hennig C, Meila M, Murtagh F, Rocci R (eds) <span class="nocase">Handbook of Cluster Analysis</span>. Chapman; Hall/CRC Press, New York, NY, U.S.A., pp 245–575</div>
</div>
<div id="ref-fclust2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline">Ferraro MB, Giordani P, Serafini A (2019) <span class="nocase">fclust</span>: An <span>R</span> package for fuzzy clustering. The <span>R</span> Journal 11:198–210</div>
</div>
<div id="ref-Ng2001" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline">Ng AY, Jordan MI, Weiss Y (2001) On spectral clustering: Analysis and an algorithm. In: Dietterich T, Becker S, Ghahramani Z (eds) <span class="nocase">Advances in Neural Information Processing Systems</span>. MIT Press, Cambridge, MA, U.S.A., pp 849–856</div>
</div>
<div id="ref-Dhillon2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline">Dhillon IS, Guan Y, Kulis B (2004) Kernel <span><span class="math inline">\(K\)</span>-M</span>eans: Spectral clustering and normalized cuts. In: <span class="nocase">KDD ’04: Proceedings of the Tenth ACM SIGKDD International Conference of Knowledge Discovery and Data Mining, Seattle, WA, U.S.A.</span> Association for Computing Machinery, New York, NY, U.S.A., pp 551–556</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/ch07-prediction/ch7-pred.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Predictive modeling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/ch09-model-based-clustering/ch9-model.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model-based clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
  </div>
</footer>
<script>
  document.querySelector(".quarto-title").innerHTML =  /*'<div class="badge bs-warning bg-warning text-dark" style="float:right;">Pre-print</div>' + */ document.querySelector(".quarto-title").innerHTML
  var keywords = document.querySelector('meta[name="keywords"]')
  if (keywords && keywords.content) {
    document.getElementById("title-block-header").innerHTML = document.getElementById("title-block-header").innerHTML + 
      '<div class="abstract"><div class="abstract-title">Keywords</div><div class="quarto-title-meta-contents"><p>'+
      keywords.content +
      '</p></div></div>'
  }
  function insertAfter(referenceNode, newNode) {
      referenceNode.parentNode.insertBefore(newNode, referenceNode.nextSibling);
  }
  var authors = document.querySelectorAll('meta[name="author"]')
  var firstpage = document.querySelector('meta[name="citation_firstpage"]').content
  var lastpage = document.querySelector('meta[name="citation_lastpage"]').content
  var doi = document.querySelector('meta[name="citation_doi"]').content
  if (authors) {
    var authorlist = Array.from(authors).map(e=>e.content).reduce((accum, curr) =>  accum + curr + ", ", "","").replace(/\,\s$/,"")
    var citt = `<div class="card border-primary mb-3" style=;">
      <div class="card-header bg-primary">To cite this chapter</div>
      <div class="card-body small">
        <p class="card-text">${authorlist} (2024).
        <b>${document.getElementsByClassName("chapter-title")[0].innerText}</b>. 
        In M. Saqr & S. López-Pernas (Eds.), <i>Learning analytics methods and tutorials: A practical guide using R</i> &nbsp;
         (pp. ${firstpage}-${lastpage}).Springer, Cham. doi: <a href="https://doi.org/${doi}">${doi}</a></p>
      </div>
    </div>`;
    insertAfter(document.getElementsByTagName("HEADER")[1],new DOMParser().parseFromString(citt, 'text/html').body.childNodes[0])
  }
</script>



</body></html>